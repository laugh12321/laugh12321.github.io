{"pages":[{"title":"","text":"A simple, delicate, and modern theme for the static site generator Hexo. Preview | Documentation | Download :cd: InstallationDownload &amp; extract or git clone Icarus from GitHub to your blog’s theme folder, and that’s it! 1git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus Once started, Icarus will remind you of any missing dependencies and configuration files. :gift: FeaturesExtensive Plugin Support Icarus includes plentiful search, comment, sharing and other plugins out of the box. You can choose any of them to enrich yourblog experience, or build your own plugin easily referring to the existing Icarus plugins. Comment plugins Changyan Disqus Facebook Gitment Isso LiveRe Valine Search plugins Insight Search Google Custom Search Engine Baidu Site Search Share plugins AddThis AddToAny Baidu Share Share.js ShareThis Donation Buttons Alipay Wechat Paypal Patreon Other plugins Hexo Tag Plugin lightGallery &amp; Justified Gallery MathJax Site Analytics Rich Code Highlight Theme Choices Icarus directly import code highlight themes from the highlight.js package, and makes more than70 highlight themes available to you. Elastic Theme Configuration In addition to the minimalistic and easy-to-understand configuration design, Icarus allows you to set configurations on aper-page basis with the ability to merge and override partial configurations. _config.yml post.md menu: Archives: /archives Categories: /categories Tags: /tags About: /about title: A Simple Post menu: Go Home: /index.html --- # Here is some simple markdown. Responsive Layout No matter what modern browsering device your audiences are using, they can always get the best experience because Icarus’s responsivelayout across multiple viewpoints. :hammer: DevelopmentThis project is built with Hexo 3.7.1 Ejs Stylus Bulma 0.7.2 Please refer to the documentation for Icarus implementation details. :tada: ContributeIf you feel like to help us build a better Icarus, you can :electric_plug: Write a plugin |:black_nib: Submit a tutorial |:triangular_flag_on_post: Report a bug |:earth_asia: Add a translation :memo: LicenseThis project is licensed under the MIT License - see the LICENSE file for details.","link":"/about/index.html"}],"posts":[{"title":"深度学习环境搭建指南","text":"检查 GPU 是否支持如果你想搭建深度学习环境，那么首先得有一块 NVIDIA GPU。很遗憾，目前 AMD 系列 GPU 对深度学习并不友好。 有了 NVIDIA GPU 之后，首先需要检查其型号是否符合深度学习的最低配置，目前热门的深度学习框架对老旧型号的 NVIDIA GPU 并不支持。其中，判断的原则是 NVIDIA GPU 的计算性能指数（Compute Capability）大于或等于 3.0。 你可以访问 官方性能指数 页面查看。 更新 NVIDIA GPU 驱动接下来，你需要更新 NVIDIA GPU 驱动到最新版本。这一步骤非常简单，只需要到 官方驱动页面 找到对应型号下载安装即可。 安装 CUDA 架构CUDA（Compute Unified Device Architecture，统一计算架构）是由 NVIDIA 所推出的一种集成技术，深度学习需要 CUDA 并行计算架构的支持。 目前，CUDA 的版本是 9.x，这里推荐 9.0 版本即可，你可以访问 官方页面 下载。 目前，CUDA 支持 Windows，Linux 和 macOS 操作系统。 安装 cuDNN 深度神经网络库cuDNN 是 NVIDIA 为深度学习专门研发的神经网络加速库，并支持当前主流的 TensorFlow、PyTorch 等深度行学习框架。根据官方介绍，cuDNN 能更好地调度 GPU 资源，使得神经网络的训练过程更加高效。 安装 cuDNN 首先需要访问 NVIDIA Developer 网站 注册账号。该网站国内访问速度慢，可能需要耐心等待或采取其他手段。 登陆之后，需要根据之前安装的 CUDA 版本选择对应版本的 cuDNN 安装。 安装结果检查更新驱动 + 安装 CUDA + 安装 cuDNN 等三个步骤完成之后，我们需要检查深度学习框架是否能正常监测到 GPU 并调用。 这里推荐使用 TensorFlow，首先需要安装 GPU 版本的 TensorFlow。你可以阅读 官方安装指南。 安装完成之后，运行 TensorFlow 官方给出的 GPU 检测示例代码即可： 12345678# Creates a graph.a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')c = tf.matmul(a, b)# Creates a session with log_device_placement set to True.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))# Runs the op.print(sess.run(c)) 如果能在输出中看到 GPU 的字样，即代表安装成功。例如： 12345678Device mapping:/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K40c, pci busid: 0000:05:00.0b: /job:localhost/replica:0/task:0/device:GPU:0a: /job:localhost/replica:0/task:0/device:GPU:0MatMul: /job:localhost/replica:0/task:0/device:GPU:0[[ 22. 28.] [ 49. 64.]] 使用云主机搭建本地安装深度学习环境看起来很简单，但实际上因为系统环境等原因很容易碰到各种「坑」。很多时候，也非常推荐在云主机上完成深度学习训练。 当你选择在云主机上进行深度学习时，往往不需要自行配置环境，而是直接启动已配置好的镜像。无论是国内的 阿里云，还是国外的 AWS，都会提供最新的深度学习镜像。 例如，阿里云提供的 Ubuntu16.04（预装 NVIDIA GPU 驱动和 CUDA9.0） 镜像，已经为你配置好了驱动、CUDA、cuDNN。 你只需要在启动实例时加载即可，这样就省去了自行配置环境的麻烦。 除此之外，AWS 提供的 Amazon SageMaker 服务 可以让你一键启动 Jupyter Notebook，并在后端挂载相应的 GPU 实例，可以说再方便不过了。 在线 Notebook 环境推荐除了自行搭建环境，在此推荐 2 个免费的在线 Jupyter Notebook 环境。 Microsoft Azure NotebooksMicrosoft Azure Notebooks 是微软推出的免费 Jupyter Notebook 环境，非常方便。不过，Microsoft Azure Notebooks 仅提供 CPU 环境，无法完成深度学习模型训练。但是，平常的数据分析任务处理起来游刃有余了。 Google ColabGoogle Colab 是 Google 推出的线上 Notebook 环境。相对于 Microsoft Azure Notebooks 而言，Google Colab 的最大问题存在一些访问上的阻碍。不过，Google Colab 的最大优势在于提供了 Nvidia M40 系列 GPU，并免费开放使用。 使用时，只需要在 代码执行程序 → 更改运行类型 中选择 GPU，即可开启免费 GPU 环境。 Google Colab 提供的免费 GPU 并不是无限制使用，如果你请求频次过高或者连续运行时间太长，都有可能被强制中断。","link":"/2018/11/24/deep_learning_environment_building_guide/"},{"title":"【Python 排序算法】—— 基本排序算法","text":"​在讲解基础的排序算法之前，先来介绍排序算法经常会用到的 `swap` 函数——用来交换列表中的两项的位置 1234567def swap(lyst, i, j): \"\"\"Exchanges the items at positions i and j.\"\"\" # You could say lyst[i], lyst[j] = lyst[j], lyst[i] # but the following code shows what is really going on temp = lyst[i] lyst[i] = lyst[j] lyst[j] = temp 选择排序 | \\(selection \\, sort\\)​可能最简单的策略就是搜索整个列表，找到最小项的位置。如果该位置不是列表的第一个位置，算法就会交换这两个位置的项。然后，算法回到第 2 个位置并且重复这个过程，如果必要的话，将最小项和第 2 个位置的项交换。当算法到达整个过程的最后一个位置，列表就是排序好的了。这就是选择排序算法的基本思路。 ​下表展示了对于 5 个项的一个列表进行选择排序，在每一次搜索和交换之后的状态。因为每次经过主循环时，都会选择一个要移动的项，即在每一轮都只是交换两项，这两项的后面用 * 表示，并且表中已经排好序的部分用阴影表示。 如下是选择排序的 Python 描述： 123456789101112def selectionSort(lyst): i = 0 while i &lt; len(lyst) - 1: # Do n - 1 searches minIndex = i # for the smallest j = i + 1 while j &lt; len(lyst): # Start a search if lyst[j] &lt; lyst[minIndex]: minIndex = j j += 1 if minIndex != i: # Exchange if need swap(lyst, minIndex, i) i += 1 ​该算法的复杂度为 `O(n^2)`。由于数据项交换只是在外围循环中进行，所以在最坏情况和平均情况下，选择排序的这一额外开销都是线性的。 冒泡排序 | \\(bubble \\,sort\\)​冒泡排序法相对容易理解和编码。其策略是从列表的开头出开始，并且比较一对数据项，直到移动到列表的末尾。每当成对的两项之间顺序不正确的时候，算法就交换其位置。这个过程的效果就是将最大的项以冒泡的方式排到列表的末尾。然后，算法从列表头到倒数第 2 个列表项重复这一过程，依此类推，直到该算法从列表的最后一项开始执行。此时，列表是已经排序好的。 ​下表展示了对 5 个项的一个列表进行冒泡排序的过程。这个过程把嵌套的循环执行了 4 次，将最大的项冒泡的列表的末尾。再一次，只有交换的项用*标出，并且排好序的部分用阴影表示。 如下是冒泡排序的 Python 描述： 123456789def bubbleSort(lyst): n = len(lyst) while n &gt; 1: # Do n - 1 bubbles i = 1 # Start each bubble while i &lt; n: if lyst[i] &lt; lyst[i - 1]: # Exchange if need swap(lyst, i, i - 1) i += 1 n -= 1 ​和选择排序一样，冒泡排序的复杂度也是 `O(n^2)`。如果列表是已经排好的，冒泡排序不会执行任何交换。然而，在最坏的情况下，冒泡排序的交换超过线性方式。 ​可以对冒泡排序进行一个小的调整，将其在最好的情况下的性能提高到线性阶。如果在通过主循环的时候，没有发生交换，那么列表就是已经排序的。这种情况可能发生在任何一轮，但是，在最好的情况下，第 1 轮就会发生。可以使用一个布尔标记来记录交换动作的出现，并且当内部循环没有设置这个标记的时候，就从函数返回。如下是修改后的冒泡排序函数： 123456789101112def bubbleSortWithTweak(lyst): n = len(lyst) while n &gt; 1: swapped = False i = 1 while i &lt; n: if lyst[i] &lt; lyst[i - 1]: # Exchange if need swap(lyst, i, i - 1) swapped = True i += 1 if not swapped: return # Return if no swaps n -= 1 注意：这一修改只是改进了最好情况下的行为。在平均情况下，这个版本的复杂度仍为`O(n^2)`。 插入排序 | \\(insertion \\,sort\\)​修改过后的冒泡排序，对于已经排好的列表来说，其性能比选择排序要好。但是，如果列表中的项是没有顺序的，修改过后的冒泡排序的性能任然是很糟糕的。 插入排序法试图以一种不同的方式来对列表进行排列，其策略如下： 在第 i 轮通过列表的时候（\\( 1 \\leq i \\leq n-1 \\)）,第 i 个项应该插入到列表的前 i 个项之中的正确位置。 在第 i 轮之后，前 i 个项应该是排好序的。 这个过程类似于排列手中扑克牌的顺序。即，如果你按照顺序放好了前 i-1 张牌，抓取了第 i 张牌，并且将其与手中的牌进行比较，直到找到合适的位置。 和其他排序算法一样，插入排序包含两个循环。外围的循环遍历从 1 到 n-1 的位置。对于这个循环中的每一个位置 i， 我们都保存该项并且从位置 i-1 开始内部循环。对于这个循环中的每一个位置 j，我们都将项移动到位置 j+1，直到找到了给保存的项（第 i 项）的插入位置。 如下是 insertionSort 函数的代码： 12345678910111213def insertionSort(lyst): i = 1 while i &lt; len(lyst): itemToInsert = lyst[i] j = i - 1 while j &gt;= 0: if itemToInsert &lt; lyst[j]: lyst[j + 1] = lyst[j] j -= 1 else: break lyst[j + 1] = itemToInsert i += 1 ​下表展示了对 5 个项的一个列表进行插入排序，以及在每一次通过外围循环后的状态。在下一个轮次中插入的项用一个箭头标记出来，在将这个项插入之后，用*将其标记。 ​插入排序的最坏情况的复杂的为 `O(n^2)`。列表中排好序的项越多，插入排序的效果越好，在最好的情况下，列表本来就是有序的，那么，插入排序的复杂度是线性阶的。然而，在平均情况下，插入排序的复杂度仍然是二次方阶的。","link":"/2018/11/28/python-basic-sort/"},{"title":"【Python 排序算法】—— 更快的排序","text":"在上篇【Python 排序算法】—— 基本排序算法中，介绍的 3 种排序算法都拥有 `O(n^2)` 的运行时间。这些排序算法还有几种变体，其中的稍微快一些。但是，在最坏的情况和平均情况下，它们的性能还是 `O(n^2)`。然而，我们可以利用一些复杂度为 `O(nlogn)` 的更好的算法。这些更好的算法的秘诀就是，采用分而治之\\((divide-and-conquer)\\)的策略。也就是说，每一个算法都找了一种方法，将列表分解为更小的子列表。随后，这些子列表在递归地排序。理想情况下，如果这些子列表的复杂度为 `log(n)`，而重新排列每一个子列表中的数据所需的工作量为 `n`，那么，这样的排序算法总的复杂度就是 `O(nlogn)`。 这里将介绍两种排序算法，他们都突破了 `n^2` 复杂度的障碍，它们是快速排序和合并排序。 快速排序简介快速排序所使用的策略可以概括如下： 首先，从列表的中点位置选取一项。这一项叫做基准点\\((pivot)\\)。 将列表中的项分区，以便小于基准点的所有项都移动到基准点的左边，而剩下的项都移动到基准点的右边。根据相关的实际项，基准点自身的最终位置也是变化的。例如，如果基准点自身是最大的项，它会位于列表的最右边，如果基准点是最小值，它会位于最左边。但是，不管基准点最终位于何处，这个位置都是它在完全排序的列表中的最终位置。 分而治之。对于在基准点分割列表而形成的子列表，递归地重复应用该过程。一个子列表包含了基准点左边的所有的项（现在是最小的项），另一个子列表包含了基准点右边的所有的项（现在是较大的项）。 每次遇到少于2个项的一个子列表，就结束这个过程。 分割该算法最复杂的部分就是对子列表中的项进行分割的操作。有两种主要的方式用来进行分割。有一种方法较为容易，如何对任何子列表应用该方法的步骤如下： 将基准点和子列表的最后一项交换。 在已知小于基准点的项和剩余的项之间建立一个边界。一开始，这个边界就放在第 1 个项之前。 从子列表中的第 1 项开始，扫描整个子列表。每次遇到小于基准点的项，就将其与边界之后的第 1 项交换，且边界向后移动。 将基准点和边界之后的第 1 项交换，从而完成这个过程。 下图说明了对于数字12, 19, 17, 18, 14, 11, 15, 13 和 16 应用这些步骤的过程。在第 1 步中，建立了基准点并且将其与最后一项交换。在第 2 步中，在第 1 项之前建立了边界。在第 3 步到第 6 步，扫描了子列表以找到比基准点小的项，这些项将要和边界之后的第 1 项交换，并且边界向后移动。注意，边界左边的项总是小于基准点。最后，在第 7 步中，基准点和边界之后的第 1 项交换，子列表已经成功第分割好了。 在分割好一个子列表之后，对于左边和右边的子列表 （12，11，13 和 16，19，15，17，18）重复应用这个过程，直到子列表的长度最大为 1。 快速排序的实现快速排序使用递归算法更容易编码。如下脚本定义了一个顶层的quicksort 函数；一个递归的quicksortHelper函数，它隐藏了用与子列表终点的额外参数；还有一个partition函数。如下脚本实在20个随机排序的整数组成的一个列表上执行快速排序。 12345678910111213141516171819202122232425262728293031323334353637383940def quicksort(lyst): quicksortHelper(lyst, 0, len(lyst) - 1) def quicksortHelper(lyst, left, right): if left &lt; right: pivotLocation = partition(lyst, left, right) quicksortHelper(lyst, left, pivotLocation - 1) quicksortHelper(lyst, pivotLocation + 1, right) def partition(lyst, left, right): # Find the pivot and exchange it with the last item middle = (left + right) // 2 pivot = lyst[middle] lyst[middle] = lyst[right] lyst[right] = pivot # Set boundary point to first position boundary = left # Move items less than pivot to the left for index in range(left, right): if lyst[index] &lt; pivot: swap(lyst, index, boundary) boundary += 1 # Exchange the pivot item and the boundary item swap(lyst, right, boundary) return boundary# Earlier definition of the swap function goes hereimport randomdef main(size = 20, sort = quicksort): lyst = [] for count in range(size): lyst.append(random.randint(1, size + 1)) print(lyst) sort(lyst) print(lyst) if __name__ == \"__main__\": main()","link":"/2018/11/30/python-faster-sort/"},{"title":"一站式搭建 GitHub Pages 博客","text":"本文将详细讲解如何快速搭建 GitHub Pages 博客页面 关于博客主题，博客信息更改，上传文章等将会在 一站式搭建 GitHub Pages 博客 (二) 中进行详细讲解 准备阶段 注册 GitHub 账号 安装 GitHub Desktop 在 GitHub 建立名为 username.github.io 的仓库首先登陆 GitHub 账户，点击右上角的 ➕ ，选择New repository. 点击后，按照图示进行填写： [注意]Repository name 格式：username.github.io 将仓库变更为 GitHub Pages本地仓库建好后点击 Settings 将页面下拉到 GitHub Pages, 将 None 改为 master branch 并保存 如果发现更改不了，则点击 Change theme 随便选择一个主题就可以啦~ 这是更改后的效果图：上面的链接就是你的个人博客地址，到这里简易的个人博客就已经搭建完成了！ 在 一站式搭建 GitHub Pages 博客 (二) 中， 笔者将会讲解个人博客的详细配置。","link":"/2018/12/16/build_the_blog_00/"},{"title":"机器学习|线性回归三大评价指标实现『MAE, MSE, MAPE』（Python语言描述）","text":"对于回归预测结果，通常会有平均绝对误差、平均绝对百分比误差、均方误差等多个指标进行评价。这里，我们先介绍最常用的3个： 平均绝对误差（MAE）就是绝对误差的平均值，它的计算公式如下： MAE(y,\\hat{y}) = \\frac{1}{n}(\\sum_{i = 1}^{n}\\left | y - \\hat{y} \\right |)其中，`y_{i}` 表示真实值，`\\hat y_{i}` 表示预测值，`n` 则表示值的个数。MAE 的值越小，说明预测模型拥有更好的精确度。 我们可以尝试使用 Python 实现 MAE 计算函数： 123456789101112131415import numpy as npdef mae_value(y_true, y_pred): \"\"\" 参数: y_true -- 测试集目标真实值 y_pred -- 测试集目标预测值 返回: mae -- MAE 评价指标 \"\"\" n = len(y_true) mae = sum(np.abs(y_true - y_pred))/n return mae 均方误差（MSE） 它表示误差的平方的期望值，它的计算公式如下： {MSE}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^{2}其中，`y_{i}` 表示真实值，`\\hat y_{i}` 表示预测值，`n` 则表示值的个数。MSE 的值越小，说明预测模型拥有更好的精确度。 同样，我们可以尝试使用 Python 实现 MSE 计算函数： 123456789101112131415import numpy as npdef mse_value(y_true, y_pred): \"\"\" 参数: y_true -- 测试集目标真实值 y_pred -- 测试集目标预测值 返回: mse -- MSE 评价指标 \"\"\" n = len(y_true) mse = sum(np.square(y_true - y_pred))/n return mse 平均绝对百分比误差（MAPE）。 `MAPE` 是 `MAD` 的变形，它是一个百分比值，因此比其他统计量更容易理解。例如，如果 `MAPE` 为 `5`，则表示预测结果较真实结果平均偏离 `5%`。`MAPE` 的计算公式如下： {MAPE}(y, \\hat{y} ) = \\frac{\\sum_{i=1}^{n}{|\\frac{y_{i}-\\hat y_{i}}{y_{i}}|}}{n} \\times 100其中，`y_{i}` 表示真实值，`\\hat y_{i}` 表示预测值，`n` 则表示值的个数。`MAPE` 的值越小，说明预测模型拥有更好的精确度。 使用 Python 实现 MSE 计算函数： 123456789101112131415import numpy as npdef mape(y_true, y_pred): \"\"\" 参数: y_true -- 测试集目标真实值 y_pred -- 测试集目标预测值 返回: mape -- MAPE 评价指标 \"\"\" n = len(y_true) mape = sum(np.abs((y_true - y_pred)/y_true))/n*100 return mape 参考： 方差（variance）、标准差（Standard Deviation）、均方差、均方根值（RMS）、均方误差（MSE）、均方根误差（RMSE） Mean squared error-Wikipedia","link":"/2019/01/02/evaluation_index_with_Linear_Regression/"},{"title":"阿里云服务器ECS Ubuntu16.04 初次使用配置教程(图形界面安装)","text":"前一阵子购买了阿里云的云服务器ECS（学生优惠），折腾了一阵子后对有些东西不太满意，所以就重新初始化了磁盘，刚好要重新安装图形界面，于是就顺手写了这么一篇文章。 首次登陆第一次登陆服务器时，是这个样子的： 输入账号在 login: 后输入 root , 会出现 Password：， 然后输入你的实例密码注意：你输入的密码是不会显示出来的输入成功后效果如下： 输入指令然后依次输入下面的命令（期间需要手动确认三次）： 12345678# 更新软件库apt-get update# 升级软件apt-get upgrade# 安装桌面系统apt-get install ubuntu-desktop 输入apt-get update 后，效果如图： 然后输入 apt-get upgrade ，期间需要输入 y 进行确认，如图： 然后进行第二次确认，选择默认选项，如图： 软件升级完成后如图： 接下来就可以安装图形界面了，我们输入 apt-get install ubuntu-desktop 指令，输入后还要进行最后一次手动确认如图： 输入 y 即可，等到图形界面安装完成输入 reboot 指令进行重启，如图： 重启后可以发现我们是以访客身份登陆的，而且不能选择登陆用户并且不需要密码就可以登陆，登陆后还会出现警告信息。 桌面警告： 解决方法打开 usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf 文件并修改 修改前： 修改后： 代码如下： 1234[Seat:*]user-session=ubuntugreeter-show-manual-login=trueallow-guest=false 重启服务器后可以用 root 用户登录，但是登录还是有警告，这个需要修改 /root/.profile 文件 修改前： 修改后： 代码如下： 1234567# ~/.profile: executed by Bourne-compatible login shells.if [ \"$BASH\" ]; then if [ -f ~/.bashrc ]; then . ~/.bashrc fifitty -s &amp;&amp; mesg n || true 重启后只有root用户，登录后没有警告信息。 至此，服务器端安装桌面环境结束。 参考博客：阿里云服务器ECS Ubuntu16.04-64-bit学习之一：配置桌面","link":"/2019/01/23/Ubuntu_16.04_setting/"},{"title":"阿里云服务器ECS Ubuntu16.04 + Seafile 搭建私人网盘 （Seafile Pro）","text":"本文主要讲述 使用 Ubuntu 16.04 云服务器 通过脚本实现对 Seafile Pro 的安装，完成私人网盘的搭建 首先给出 Seafile 专业版的下载地址（Linux）: 👉 传送门 在本机下载好安装包后，通过 WinSCP 将安装包放在 /opt/ 目录下，并将专业版的安装包重命名为 seafile-pro-server_6.3.11_x86-64.tar.gz 的格式（方便安装）。这里使用的安装方式是使用官方给出的 Seafile 安装脚本 安装，优点是一步到位，坏处是安装失败需要还原到镜像。 使用步骤安装干净的 16.04 或 CentOS 7 系统，并做好镜像 (如果安装失败需要还原到镜像)。 切换成 root 账号 (sudo -i) 获取安装脚本Ubuntu 16.04（适用于 6.0.0 及以上版本）: 1wget https://raw.githubusercontent.com/haiwen/seafile-server-installer-cn/master/seafile-server-ubuntu-16-04-amd64-http 运行安装脚本并指定要安装的版本 (6.3.11)1bash seafile-server-ubuntu-16-04-amd64-http 6.3.11 输入 2 选择安装专业版 该脚本运行完后会在命令行中打印配置信息和管理员账号密码，请仔细阅读。(你也可以查看安装日志 /opt/seafile/aio_seafile-server.log )，MySQL 密码在 /root/.my.cnf 中。 通过 Web UI 对服务器进行配置安装完成后，需要通过 Web UI 服务器进行基本的配置，以便能正常的从网页端进行文件的上传和下载： 首先在浏览器中输入服务器的地址，并用管理员账号和初始密码登录 点击界面的右上角的头像按钮进入管理员界面 进入设置页面填写正确的服务器对外的 SERVICE_URL 和 FILE_SERVER_ROOT，比如 12SERVICE_URL: http://126.488.125.111：8000FILE_SERVER_ROOT: &apos;http://126.488.125.111/seafhttp&apos; 注意： 126.488.125.111 是你服务器的公网 ip 对了，还要在还要在 云服务器管理控制台 设置新的安全组规则（8082 和 80 端口），可以参考下图自行配置 现在可以退出管理员界面，并进行基本的测试。关于服务器的配置选项介绍和日常运维可以参考 http://manual-cn.seafile.com/config/index.html 在本地打开 Web UI因为使用一键安装脚本安装，默认使用了 nginx 做反向代理，并且开启了防火墙，所以你需要直接通过 80 端口访问，而不是 8000 端口。 注意：在本地输入的 ip 地址是你的云服务器的公网 IP 通过客户端登陆Windows 客户端登陆 Android 客户端登陆 至此，私人云盘已经搭建完毕 更多详细步骤请阅读： 👉 官方脚本说明","link":"/2019/01/24/Seafile_Pro_settings/"},{"title":"使用 Mini Batch K-Means 进行图像压缩 ","text":"针对一张成都著名景点：锦里的图片，通过 Mini Batch K-Means 的方法将相近的像素点聚合后用同一像素点代替，以达到图像压缩的效果。 图像导入1234567# 使用 Matplotlib 可视化示例图片%matplotlib inlineimport matplotlib.pyplot as plt import matplotlib.image as mpimg chengdu = mpimg.imread('chengdu.png') # 将图片加载为 ndarray 数组plt.imshow(chengdu) # 将数组还原成图像 1chengdu.shape (516, 819, 3) 在使用 mpimg.imread 函数读取图片后，实际上返回的是一个 numpy.array 类型的数组，该数组表示的是一个像素点的矩阵，包含长，宽，高三个要素。如成都锦里这张图片，总共包含了 `516` 行，`819` 列共 `516*819=422604` 个像素点，每一个像素点的高度对应着计算机颜色中的三原色 `R, G, B`（红，绿，蓝），共 3 个要素构成。 数据预处理为方便后期的数据处理，需要对数据进行降维。 123# 将形状为 (516, 819, 3) 的数据转换为 (422604, 3) 形状的数据。data = chengdu.reshape(516*819, 3)data.shape, data[10] ((422604, 3), array([0.12941177, 0.13333334, 0.14901961], dtype=float32)) 像素点种类个数计算尽管有 422604 个像素点，但其中仍然有许多相同的像素点。在此我们定义：`R, G, B` 值相同的点为一个种类，其中任意值不同的点为不同种类。 1234567891011121314151617\"\"\"计算像素点种类个数\"\"\"def get_variety(data): \"\"\" 参数: 预处理后像素点集合 返回: num_variety -- 像素点种类个数 思路：将数据转化为 list 类型，然后将每一个元素转换为 tuple 类型，最后利用 set() 和 len() 函数进行计算。 \"\"\" temp = data.tolist() num_variety=len(set([tuple(t) for t in temp])) return num_variety 1get_variety(data), data[20] (100109, array([0.24705882, 0.23529412, 0.2627451 ], dtype=float32)) Mini Batch K-Means 聚类像素点种类的数量是决定图片大小的主要因素之一，在此使用 Mini Batch K-Means 的方式将图片的像素点进行聚类，将相似的像素点用同一像素点值来代替，从而降低像素点种类的数量，以达到压缩图片的效果。 12345678from sklearn.cluster import MiniBatchKMeansmodel = MiniBatchKMeans(10) # 聚类簇数量设置为 10 类model.fit(data)predict=model.predict(data)new_colors = model.cluster_centers_[predict] 12# 调用前面实现计算像素点种类的函数，计算像素点更新后种类的个数get_variety(new_colors) 10 图像压缩前后展示123456# 将聚类后并替换为类别中心点值的像素点，变换为数据处理前的格式，并绘制出图片进行对比展示fig, ax = plt.subplots(1, 2, figsize=(16, 6))new_chengdu = new_colors.reshape(chengdu.shape)ax[0].imshow(chengdu)ax[1].imshow(new_chengdu) 通过图片对比，可以十分容易发现画质被压缩了。其实，因为使用了聚类，压缩后的图片颜色就变为了 10 种。 接下来，使用 mpimg.imsave() 函数将压缩好的文件进行存储，并对比压缩前后图像的体积变化。 1234# 运行对比mpimg.imsave(\"new_chengdu.png\",new_chengdu)!du -h new_chengdu.png!du -h chengdu.png 220K new_chengdu.png 1.1M chengdu.png 可以看到，使用 Mini Batch K-Means 聚类方法对图像压缩之后，体积明显缩小。","link":"/2019/02/09/Image_compression_using_Mini_Batch_k-means/"},{"title":"Jekyll + NexT + GitHub Pages 主题深度优化","text":"前言笔者在用 Jekyll 搭建个人博客时踩了很多的坑，最后发现了一款不错的主题 jekyll-theme-next，但网上关于 Jekyll 版的 Next 主题优化教程少之又少，于是就决定自己写一篇以供参考。 本文仅讲述 Next (Jekyll) 主题的深度优化操作，关于主题的基础配置请移步官方文档。 主题优化修改内容区域的宽度打开 _sass/_custom/custom.scss 文件，新增变量： 12345// 修改成你期望的宽度$content-desktop = 700px// 当视窗超过 1600px 后的宽度$content-desktop-large = 900px 此方法不适用于 Pisces Scheme 当你使用Pisces风格时可以用下面的方法： 编辑 Pisces Scheme 的 _sass/_schemes/Pisces/_layout.scss 文件，在最底部添加如下代码： 123header{ width: 90%; }.container .main-inner { width: 90%; }.content-wrap { width: calc(100% - 260px); } 对于有些浏览器或是移动设备，效果可能不是太好 编辑 Pisces Scheme 的 _sass/_schemes/Pisces/_layout.scss 文件，修改以下内容： 1234567891011121314// 将 .header 中的 width: $main-desktop;// 改为：width: 80%;// 将 .container .main-inner 中的：width: $main-desktop;// 改为：width: 80%;// 将 .content-wrap 中的：width: $content-desktop;// 改为：width: calc(100% - 260px); 还是不知道如何修改的话，可以直接赋值笔者改好的 👉 传送门 背景透明度打开 _sass/_custom/custom.scss 文件，新增变量： 1234567891011121314151617181920212223//文章内容背景改成了半透明.content-wrap { background: rgba(255, 255, 255, 0.8);}.sidebar { background: rgba(255, 255, 255, 0.1); box-shadow: 0 2px 6px #dbdbdb;}.site-nav{ box-shadow: 0px 0px 0px 0px rgba(0, 0, 0, 0.8);}.sidebar-inner { background: rgba(255, 255, 255, 0.8); box-shadow: 0 2px 6px #dbdbdb;}.header-inner { background: rgba(255, 255, 255, 0.8); box-shadow: 0 2px 6px #dbdbdb;}.footer { font-size: 14px; color: #434343;} 自定义背景图片打开 _sass/_custom/custom.scss 文件，新增变量： 1234567body{ background:url(https://images8.alphacoders.com/929/929202.jpg); background-size:cover; background-repeat:no-repeat; background-attachment:fixed; background-position:center;} url() 中可以时本地图片，也可以是图片链接 彩色时间轴打开 _sass/_custom/custom.scss 文件，新增变量： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 时间轴样式.posts-collapse { margin: 50px 0px;}@media (max-width: 1023px) { .posts-collapse { margin: 50px 20px; }}// 时间轴左边线条.posts-collapse::after { margin-left: -2px; background-image: linear-gradient(180deg,#f79533 0,#f37055 15%,#ef4e7b 30%,#a166ab 44%,#5073b8 58%,#1098ad 72%,#07b39b 86%,#6dba82 100%);}// 时间轴左边线条圆点颜色.posts-collapse .collection-title::before { background-color: rgb(255, 255, 255);}// 时间轴文章标题左边圆点颜色.posts-collapse .post-header:hover::before { background-color: rgb(161, 102, 171);}// 时间轴年份.posts-collapse .collection-title h1, .posts-collapse .collection-title h2 { color: rgb(102, 102, 102);}// 时间轴文章标题.posts-collapse .post-title a { color: rgb(80, 115, 184);}.posts-collapse .post-title a:hover { color: rgb(161, 102, 171);}// 时间轴文章标题底部虚线.posts-collapse .post-header:hover { border-bottom-color: rgb(161, 102, 171);}// archives页面顶部文字.page-archive .archive-page-counter { color: rgb(0, 0, 0);}// archives页面时间轴左边线条第一个圆点颜色.page-archive .posts-collapse .archive-move-on { top: 10px; opacity: 1; background-color: rgb(255, 255, 255); box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5);} 友链居中打开 _sass/_custom/custom.scss 文件，新增变量： 1234//友链居中.links-of-blogroll-title { text-align: center;} 修改友链文本颜色打开 _sass/_custom/custom.scss 文件，新增变量： 1234567891011//友链文本颜色//将链接文本设置为蓝色，鼠标划过时文字颜色加深，并显示下划线.post-body p a{ text-align: center; color: #434343; border-bottom: none; &amp;:hover { color: #5073b8; text-decoration: underline; }} 修改友链样式打开 _sass/_custom/custom.scss 文件，新增变量： 12345678910//修改友情链接样式.links-of-blogroll-item a{ text-align: center; color: #434343; border-bottom: none; &amp;:hover { color: #5073b8; text-decoration: underline; }} 自定义页脚的心样式打开 _sass/_custom/custom.scss 文件，新增变量： 12345678910111213// 自定义页脚的心样式@keyframes heartAnimate { 0%,100%{transform:scale(1);} 10%,30%{transform:scale(0.9);} 20%,40%,60%,80%{transform:scale(1.1);} 50%,70%{transform:scale(1.1);}}#heart { animation: heartAnimate 1.33s ease-in-out infinite;}.with-love { color: rgb(255, 113, 168);} 设置头像边框为圆形框打开 _sass/_common/components/sidebar/sidebar-author.scss 文件，新增变量： 12345678910111213.site-author-image { display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; 特效：鼠标放置头像上旋转打开 _sass/_common/components/sidebar/sidebar-author.scss 文件，新增变量： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;}img:hover { /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);}/* Z 轴旋转动画 */@-webkit-keyframes play { 0% { -webkit-transform: rotateZ(0deg); } 100% { -webkit-transform: rotateZ(-360deg); }}@-moz-keyframes play { 0% { -moz-transform: rotateZ(0deg); } 100% { -moz-transform: rotateZ(-360deg); }}@keyframes play { 0% { transform: rotateZ(0deg); } 100% { transform: rotateZ(-360deg); }} Bug 修复打赏文字抖动修复打开 _sass/_common/components/post/post-reward.scss 文件，然后注释其中的函数 wechat:hover 和 alipay:hover ，如下： 123456789101112/* #wechat:hover p{ animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear; } #alipay:hover p{ animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear; }*/ 修改文章底部的带#号标签打开 _includes/_macro/post.html 文件,搜索 rel=&quot;tag&quot;&gt;# ,将 # 换成 &lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 123456&lt;div class=\"post-tags\"&gt; {% for tag in post.tags %} {% assign tag_url_encode = tag | url_encode | replace: '+', '%20' %} &lt;a href=\"{{ '/tag/#/' | relative_url | append: tag_url_encode }}\" rel=\"tag\"&gt;&lt;i class=\"fa fa-tag\"&gt;&lt;/i&gt; {{ tag }}&lt;/a&gt; {% endfor %}&lt;/div&gt; 插件配置阅读次数统计（LeanCloud） 请查看 为NexT主题添加文章阅读量统计功能 打开 config.yml 文件，搜索 leancloud_visitors , 进行如下更改： 1234leancloud_visitors: enable: true app_id: &lt;app_id&gt; app_key: &lt;app_key&gt; app_id 和 app_key 分别是 你的LearnCloud 账号的 AppID 和 AppKey 阅读次数美化效果👉： 打开 _data/languages/zh-Hans.yml 文件，将 post 中的 visitors:阅读次数 改为：visitors: 热度。 打开 _includes/_macro/post.html 文件,搜索 leancloud-visitors-count ,在 &lt;span&gt;&lt;/span&gt; 之间添加 ℃ 1234567891011&lt;span id=\"{{ post.url | relative_url }}\" class=\"leancloud_visitors\" data-flag-title=\"{{ post.title }}\"&gt; &lt;span class=\"post-meta-divider\"&gt;|&lt;/span&gt; &lt;span class=\"post-meta-item-icon\"&gt; &lt;i class=\"fa fa-eye\"&gt;&lt;/i&gt; &lt;/span&gt; {% if site.post_meta.item_text %} &lt;span class=\"post-meta-item-text\"&gt;{{__.post.visitors}} &lt;/span&gt; {% endif %} &lt;span class=\"leancloud-visitors-count\"&gt;&lt;/span&gt; &lt;span&gt;℃&lt;/span&gt;&lt;/span&gt; 在网站底部加上访问量效果👉： 打开 _includes/_partials/footer.html 文件，在 &lt;div class=&quot;copyright&quot; &gt; 之前加入下面的代码： 1&lt;script async src=\"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\"&gt;&lt;/script&gt; 在 if site.copyright 之后加入下面的代码： 12345&lt;div class=\"powered-by\"&gt;&lt;i class=\"fa fa-user-md\"&gt;&lt;/i&gt;&lt;span id=\"busuanzi_container_site_uv\"&gt; 本站访客数:&lt;span id=\"busuanzi_value_site_uv\"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt; 部分样式可参考我的博客👉：Laugh’s Blog 参考文章： Hexo+Next主题优化 hexo的next主题个性化教程:打造炫酷网站 参考博客： DS Blog","link":"/2018/12/24/update_next_for_jekyll/"},{"title":"机器学习|线性回归算法详解 (Python 语言描述)","text":"线性回归线性回归是一种较为简单，但十分重要的机器学习方法。掌握线性的原理及求解方法，是深入了解线性回归的基本要求。除此之外，线性回归也是监督学习回归部分的基石。 线性回归介绍在了解线性回归之前，我们得先了解分类和回归问题的区别。 首先，回归问题和分类问题一样，训练数据都包含标签，这也是监督学习的特点。而不同之处在于，分类问题预测的是类别，回归问题预测的是连续值。 例如，回归问题往往解决： 股票价格预测 房价预测 洪水水位线 上面列举的问题，我们需要预测的目标都不是类别，而是实数连续值。 也就是说，回归问题旨在实现对连续值的预测，例如股票的价格、房价的趋势等。比如，下方展现了一个房屋面积和价格的对应关系图。 如上图所示，不同的房屋面积对应着不同的价格。现在，假设我手中有一套房屋想要出售，而出售时就需要预先对房屋进行估值。于是，我想通过上图，也就是其他房屋的售价来判断手中的房产价值是多少。应该怎么做呢？ 我采用的方法是这样的。如下图所示，首先画了一条红色的直线，让其大致验证橙色点分布的延伸趋势。然后，我将已知房屋的面积大小对应到红色直线上，也就是蓝色点所在位置。最后，再找到蓝色点对应于房屋的价格作为房屋最终的预估价值。 在上图呈现的这个过程中，通过找到一条直线去拟合数据点的分布趋势的过程，就是线性回归的过程。而线性回归中的「线性」代指线性关系，也就是图中所绘制的红色直线。 此时，你可能心中会有一个疑问。上图中的红色直线是怎么绘制出来的呢？为什么不可以像下图中另外两条绿色虚线，而偏偏要选择红色直线呢？ 上图中的绿色虚线的确也能反应数据点的分布趋势。所以，找到最适合的那一条红色直线，也是线性回归中需要解决的重要问题之一。 通过上面这个小例子，相信你对线性回归已经有一点点印象了，至少大致明白它能做什么。接下来的内容中，我们将了解线性回归背后的数学原理，以及使用 Python 代码对其实现。 线性回归原理及实现一元线性回归上面针对线性回归的介绍内容中，我们列举了一个房屋面积与房价变化的例子。其中，房屋面积为自变量，而房价则为因变量。另外，我们将只有 1 个自变量的线性拟合过程叫做一元线性回归。 下面，我们就生成一组房屋面积和房价变化的示例数据。x 为房屋面积，单位是平方米; y 为房价，单位是万元。 1234import numpy as npx = np.array([56, 72, 69, 88, 102, 86, 76, 79, 94, 74])y = np.array([92, 102, 86, 110, 130, 99, 96, 102, 105, 92]) 示例数据由 10 组房屋面积及价格对应组成。接下来，通过 Matplotlib 绘制数据点，x, y 分别对应着横坐标和纵坐标。 123456from matplotlib import pyplot as plt%matplotlib inlineplt.scatter(x, y)plt.xlabel(\"Area\")plt.ylabel(\"Price\") 正如上面所说，线性回归即通过线性方程（1 次函数）去拟合数据点。那么，我们令函数的表达式为： y(x, w) = w_0 + w_1x \\tag{1}公式（1）是典型的一元一次函数表达式，我们通过组合不同的 `w_0` 和 `w_1` 的值得到不同的拟合直线。我们对公式（1）进行代码实现： 123def f(x, w0, w1): y = w0 + w1 * x return y 那么，哪一条直线最能反应出数据的变化趋势呢？ 如下图所示，当我们使用 y(x, w) = w_0 + w_1x 对数据进行拟合时，我们能得到拟合的整体误差，即图中蓝色线段的长度总和。如果某一条直线对应的误差值最小，是不是就代表这条直线最能反映数据点的分布趋势呢？ 平方损失函数正如上面所说，如果一个数据点为 (`x_{i}`, `y_{i}`)，那么它对应的误差就为: y_{i}-(w_0 + w_1x_{i}) \\tag2上面的误差往往也称之为残差。但是在机器学习中，我们更喜欢称作「损失」，即真实值和预测值之间的偏离程度。那么，对应 n 个全部数据点而言，其对应的残差损失总和就为： \\sum\\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i})) \\tag3在线性回归中，我们更偏向于使用均方误差作为衡量损失的指标，而均方误差即为残差的平方和。公式如下： \\sum\\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2\\tag4对于公式（4）而言，机器学习中有一个专门的名词，那就是「平方损失函数」。而为了得到拟合参数 `w_0` 和 `w_1` 最优的数值，我们的目标就是让公式（4）对应的平方损失函数最小。 同样，我们可以对公式（4）进行代码实现： 123def square_loss(x, y, w0, w1): loss = sum(np.square(y - (w0 + w1*x))) return loss 最小二乘法及代数求解最小二乘法是用于求解线性回归拟合参数 `w` 的一种常用方法。最小二乘法中的「二乘」代表平方，最小二乘也就是最小平方。而这里的平方就是指代上面的平方损失函数。 简单来讲，最小二乘法也就是求解平方损失函数最小值的方法。那么，到底该怎样求解呢？这就需要使用到高等数学中的知识。推导如下： 首先，平方损失函数为： f = \\sum\\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 \\tag5我们的目标是求取平方损失函数 `min(f)` 最小时，对应的 `w`。首先求 `f` 的 1 阶偏导数： \\frac{\\partial f}{\\partial w_{0}}=-2(\\sum_{i=1}^{n}{y_i}-nw_{0}-w_{1}\\sum_{i=1}^{n}{x_i})\\\\ \\frac{\\partial f}{\\partial w_{1}}=-2(\\sum_{i=1}^{n}{x_iy_i}-w_{0}\\sum_{i=1}^{n}{x_i}-w_{1}\\sum_{i=1}^{n}{x_i}^2) \\tag6然后，我们令 \\(\\frac{\\partial f}{\\partial w_{0}}=0\\) 以及 \\(\\frac{\\partial f}{\\partial w_{1}}=0\\)，解得： w_{1}=\\frac {n\\sum_{}^{}{x_iy_i}-\\sum_{}^{}{x_i}\\sum_{}^{}{y_i}} {n\\sum_{}^{}{x_i}^2-(\\sum_{}^{}{x_i})^2}\\\\ w_{0}=\\frac {\\sum_{}^{}{x_i}^2\\sum_{}^{}{y_i}-\\sum_{}^{}{x_i}\\sum_{}^{}{x_iy_i}} {n\\sum_{}^{}{x_i}^2-(\\sum_{}^{}{x_i})^2}\\tag7到目前为止，已经求出了平方损失函数最小时对应的 `w` 参数值，这也就是最佳拟合直线。 线性回归 Python 实现我们将公式（7）求解得到 `w` 的过程进行代码实现： 12345def w_calculator(x, y): n = len(x) w1 = (n*sum(x*y) - sum(x)*sum(y))/(n*sum(x*x) - sum(x)*sum(x)) w0 = (sum(x*x)*sum(y) - sum(x)*sum(x*y))/(n*sum(x*x)-sum(x)*sum(x)) return w0, w1 于是，可以向函数 w_calculator(x, y) 中传入 x 和 y 得到 `w_0` 和 `w_1` 的值。 1w_calculator(x, y) 1(41.33509168550616, 0.7545842753077117) 当然，我们也可以求得此时对应的平方损失的值： 1234w0 = w_calculator(x, y)[0]w1 = w_calculator(x, y)[1]square_loss(x, y, w0, w1) 1447.69153479025357 接下来，我们尝试将拟合得到的直线绘制到原图中： 1234x_temp = np.linspace(50,120,100) # 绘制直线生成的临时点plt.scatter(x, y)plt.plot(x_temp, x_temp*w1 + w0, 'r') 从上图可以看出，拟合的效果还是不错的。那么，如果你手中有一套 150 平米的房产想售卖，获得预估报价就只需要带入方程即可： 1f(150, w0, w1) 1154.5227329816629 这里得到的预估售价约为 154 万元。 线性回归 scikit-learn 实现上面的内容中，我们学习了什么是最小二乘法，以及使用 Python 对最小二乘线性回归进行了完整实现。那么，我们如何利用机器学习开源模块 scikit-learn 实现最小二乘线性回归方法呢？ 使用 scikit-learn 实现线性回归的过程会简单很多，这里要用到 LinearRegression() 类。看一下其中的参数： 1sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1) 其中： fit_intercept: 默认为 True，计算截距项。 normalize: 默认为 False，不针对数据进行标准化处理。 copy_X: 默认为 True，即使用数据的副本进行操作，防止影响原数据。 n_jobs: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。 1234567891011\"\"\"scikit-learn 线性回归拟合\"\"\"from sklearn.linear_model import LinearRegression# 定义线性回归模型model = LinearRegression()model.fit(x.reshape(len(x),1), y) # 训练, reshape 操作把数据处理成 fit 能接受的形状# 得到模型拟合参数model.intercept_, model.coef_ 1(41.33509168550615, array([0.75458428])) 我们通过 model.intercept_ 得到拟合的截距项，即上面的 `w_{0}`，通过 model.coef_ 得到 `x` 的系数，即上面的 `w_{1}`。对比发现，结果是完全一致的。 同样，我们可以预测 150 平米房产的价格： 1model.predict([[150]]) 1array([154.52273298]) 可以看到，这里得出的结果和自行实现计算结果一致。 最小二乘法的矩阵推导及实现学习完上面的内容，相信你已经了解了什么是最小二乘法，以及如何使用最小二乘法进行线性回归拟合。上面，实验采用了求偏导数的方法，并通过代数求解找到了最佳拟合参数 w 的值。 这里，我们尝试另外一种方法，即通过矩阵的变换来计算参数 w 。推导如下： 首先，一元线性函数的表达式为 `y(x, w) = w_0 + w_1x`，表达成矩阵形式为： 即：\\(y(x, w) = XW \\tag{8b}\\) （8）式中，`W` 为，而 `X` 则是矩阵。然后，平方损失函数为： f = \\sum\\limits_{i = 1}^n (y_{i}-(w_0 + w_1x_{i}))^2 =(y-XW)^T(y-XW)\\tag{9}此时，对矩阵求偏导数（超纲）得到：当矩阵 `X^TX` 满秩（不满秩后面的实验中会讨论）时，`(X^TX)^{-1}X^TX=E`，且 `EW=W`。所以，`(X^TX)^{-1}X^TXW=(X^TX)^{-1}X^Ty`。最终得到： W=(X^TX)^{-1}X^Ty \\tag{11}我们可以针对公式（11）进行代码实现： 123def w_matrix(x, y): w = (x.T * x).I * x.T * y return w 我们针对原 x 数据添加截距项系数 1。 1234x = np.matrix([[1,56],[1,72],[1,69],[1,88],[1,102],[1,86],[1,76],[1,79],[1,94],[1,74]])y = np.matrix([92, 102, 86, 110, 130, 99, 96, 102, 105, 92])w_matrix(x, y.reshape(10,1)) 12matrix([[41.33509169], [ 0.75458428]]) 可以看到，矩阵计算结果和前面的代数计算结果一致。你可能会有疑问，那就是为什么要采用矩阵变换的方式计算？一开始学习的代数计算方法不好吗？ 其实，并不是说代数计算方式不好，在小数据集下二者运算效率接近。但是，当我们面对十万或百万规模的数据时，矩阵计算的效率就会高很多，这就是为什么要学习矩阵计算的原因。 参考： 最小二乘法-维基百科 线性回归-维基百科 知乎问答-最小二乘法的本质是什么？","link":"/2019/01/01/Linear_Regression/"},{"title":"机器学习|多项式回归算法详解 (Python 语言描述)","text":"多项式回归介绍在线性回归中，我们通过建立自变量 x 的一次方程来拟合数据。而非线性回归中，则需要建立因变量和自变量之间的非线性关系。从直观上讲，也就是拟合的直线变成了「曲线」。 如下图所示，是某地区人口数量的变化数据。如果我们使用线性方差去拟合数据，那么就会存在「肉眼可见」的误差。而对于这样的数据，使用一条曲线去拟合则更符合数据的发展趋势。 对于非线性回归问题而言，最简单也是最常见的方法就是本次实验要讲解的「多项式回归」。多项式是中学时期就会接触到的概念，这里引用 维基百科 的定义如下： 多项式（Polynomial）是代数学中的基础概念，是由称为未知数的变量和称为系数的常量通过有限次加法、加减法、乘法以及自然数幂次的乘方运算得到的代数表达式。多项式是整式的一种。未知数只有一个的多项式称为一元多项式；例如 `x^2-3x+4` 就是一个一元多项式。未知数不止一个的多项式称为多元多项式，例如 `x^3-2xyz^2+2yz+1` 就是一个三元多项式。 多项式回归基础首先，我们通过一组示例数据来认识多项式回归 123# 加载示例数据x = [4, 8, 12, 25, 32, 43, 58, 63, 69, 79]y = [20, 33, 50, 56, 42, 31, 33, 46, 65, 75] 示例数据一共有 10 组，分别对应着横坐标和纵坐标。接下来，通过 Matplotlib 绘制数据，查看其变化趋势。 1234%matplotlib inlinefrom matplotlib import pyplot as pltplt.scatter(x, y) 实现 2 次多项式拟合接下来，通过多项式来拟合上面的散点数据。首先，一个标准的一元高阶多项式函数如下所示： y(x, w) = w_0 + w_1x + w_2x^2 +...+w_mx^m = \\sum\\limits_{j=0}^{m}w_jx^j \\tag{1}其中，`m` 表示多项式的阶数，`x^j` 表示 `x` 的 `j` 次幂，`w` 则代表该多项式的系数。 当我们使用上面的多项式去拟合散点时，需要确定两个要素，分别是：多项式系数 `w` 以及多项式阶数 `m`，这也是多项式的两个基本要素。 如果通过手动指定多项式阶数 `m` 的大小，那么就只需要确定多项式系数 `w` 的值是多少。例如，这里首先指定 `m=2`，多项式就变成了： y(x, w) = w_0 + w_1x + w_2x^2= \\sum\\limits_{j=0}^{2}w_jx^j \\tag{2}当我们确定 `w` 的值的大小时，就回到了前面线性回归中学习到的内容。 首先，我们构造两个函数，分别是用于拟合的多项式函数，以及误差函数。 1234567891011121314\"\"\"实现 2 次多项式函数及误差函数\"\"\"def func(p, x): \"\"\"根据公式，定义 2 次多项式函数 \"\"\" w0, w1, w2 = p f = w0 + w1*x + w2*x*x return fdef err_func(p, x, y): \"\"\"残差函数（观测值与拟合值之间的差距） \"\"\" ret = func(p, x) - y return ret 接下来，使用 NumPy 提供的随机数方法初始化 3 个 `w` 参数 12345import numpy as npp_init = np.random.randn(3) # 生成 3 个随机数p_init 1array([ 0.60995017, 1.32614407, -1.22657863]) 接下来，就是使用最小二乘法求解最优参数的过程。这里为了方便，我们直接使用 Scipy 提供的最小二乘法类，得到最佳拟合参数。当然，你完全可以按照线性回归实验中最小二乘法公式自行求解参数。不过，实际工作中为了快速实现，往往会使用像 Scipy 这样现成的函数，这里也是为了给大家多介绍一种方法。 1234567\"\"\"使用 Scipy 提供的最小二乘法函数得到最佳拟合参数\"\"\"from scipy.optimize import leastsqparameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))print('Fitting Parameters: ', parameters[0]) 关于 scipy.optimize.leastsq() 的具体使用介绍，可以阅读 官方文档。 1Fitting Parameters: [ 3.76893126e+01 -2.60474221e-01 8.00078171e-03] 我们这里得到的最佳拟合参数 `w_0`, `w_1`, `w_2` 依次为 3.76893117e+01, -2.60474147e-01 和 8.00078082e-03。也就是说，我们拟合后的函数（保留两位有效数字）为： y(x) = 37 - 0.26x + 0.0080x^2 \\tag{3}然后，我们尝试绘制出拟合后的图像。 12345678910\"\"\"绘制 2 次多项式拟合图像\"\"\"# 绘制拟合图像时需要的临时点x_temp = np.linspace(0, 80, 10000)# 绘制拟合函数曲线plt.plot(x_temp, func(parameters[0], x_temp), 'r')# 绘制原数据点plt.scatter(x, y) 实现 N 次多项式拟合你会发现，上面采用 2 次多项式拟合的结果也不能恰当地反映散点的变化趋势。此时，我们可以尝试 3 次及更高次多项式拟合。接下来的代码中，我们将针对上面 2 次多项式拟合的代码稍作修改，实现一个 N 次多项式拟合的方法。 1234567891011121314151617181920\"\"\"实现 n 次多项式拟合\"\"\"def fit_func(p, x): \"\"\"根据公式，定义 n 次多项式函数 \"\"\" f = np.poly1d(p) return f(x)def err_func(p, x, y): \"\"\"残差函数（观测值与拟合值之间的差距） \"\"\" ret = fit_func(p, x) - y return retdef n_poly(n): \"\"\"n 次多项式拟合 \"\"\" p_init = np.random.randn(n) # 生成 n 个随机数 parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y))) return parameters[0] 可以使用 n=3 验证一下上面的代码是否可用。 1n_poly(3) 1array([ 8.00077828e-03, -2.60473932e-01, 3.76893089e+01]) 此时得到的参数结果和公式（3）的结果一致，只是顺序有出入。这是因为 NumPy 中的多项式函数 np.poly1d(3) 默认的样式是： y(x) = 0.0080x^2 - 0.26x + 37\\tag{4}接下来，我们绘制出 4，5，6，7, 8, 9 次多项式的拟合结果。 1234567891011121314151617181920212223242526272829303132\"\"\"绘制出 4，5，6，7, 8, 9 次多项式的拟合图像\"\"\"# 绘制拟合图像时需要的临时点x_temp = np.linspace(0, 80, 10000)# 绘制子图fig, axes = plt.subplots(2, 3, figsize=(15,10))axes[0,0].plot(x_temp, fit_func(n_poly(4), x_temp), 'r')axes[0,0].scatter(x, y)axes[0,0].set_title(\"m = 4\")axes[0,1].plot(x_temp, fit_func(n_poly(5), x_temp), 'r')axes[0,1].scatter(x, y)axes[0,1].set_title(\"m = 5\")axes[0,2].plot(x_temp, fit_func(n_poly(6), x_temp), 'r')axes[0,2].scatter(x, y)axes[0,2].set_title(\"m = 6\")axes[1,0].plot(x_temp, fit_func(n_poly(7), x_temp), 'r')axes[1,0].scatter(x, y)axes[1,0].set_title(\"m = 7\")axes[1,1].plot(x_temp, fit_func(n_poly(8), x_temp), 'r')axes[1,1].scatter(x, y)axes[1,1].set_title(\"m = 8\")axes[1,2].plot(x_temp, fit_func(n_poly(9), x_temp), 'r')axes[1,2].scatter(x, y)axes[1,2].set_title(\"m = 9\") 从上面的 6 张图可以看出，当 m=4（4 次多项式） 时，图像拟合的效果已经明显优于 m=3 的结果。但是随着 m 次数的增加，当 m=8 时，曲线呈现出明显的震荡，这也就是线性回归实验中所讲到的过拟和（Overfitting）现象。 使用 scikit-learn 进行多项式拟合除了像上面我们自己去定义多项式及实现多项式回归拟合过程，也可以使用 scikit-learn 提供的多项式回归方法来完成。 这里，我们会用到sklearn.preprocessing.PolynomialFeatures() 这个类。PolynomialFeatures() 主要的作用是产生多项式特征矩阵。 如果你第一次接触这个概念，可能需要仔细理解下面的内容。 对于一个二次多项式而言，我们知道它的标准形式为：`y(x, w) = w_0 + w_1x + w_2x^2`。但是，多项式回归却相当于线性回归的特殊形式。例如，我们这里令 `x = x_1`, `x^2 = x_2` ，那么原方程就转换为：`y(x, w) = w_0 + w_1x_1 + w_2x_2`，这也就变成了多元线性回归。这就完成了一元高次多项式到多元一次多项式之间的转换。 举例说明，对于自变量向量 `X` 和因变量 `y`，如果 `X`： 我们可以通过 `y = w_1 x + w_0` 线性回归模型进行拟合。同样，如果对于一元二次多项式 `y(x, w) = w_0 + w_1x + w_2x^2`，如果能得到由 `x = x_1`, `x^2 = x_2` 构成的特征矩阵，即： 那么也就可以通过线性回归进行拟合了。 你可以手动计算上面的结果，但是当多项式为一元高次或者多元高次时，特征矩阵的表达和计算过程就变得比较复杂了。例如，下面是二元二次多项式的特征矩阵表达式。 \\mathbf{X} = \\left [ X_{1}, X_{2}, X_{1}^2, X_{1}X_{2}, X_{2}^2 \\right ] \\tag{5c}还好，在 scikit-learn 中，我们可以通过 PolynomialFeatures() 类自动产生多项式特征矩阵，PolynomialFeatures() 类的默认参数及常用参数定义如下： 1sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True) degree: 多项式次数，默认为 2 次多项式 interaction_only: 默认为 False，如果为 True 则产生相互影响的特征集。 include_bias: 默认为 True，包含多项式中的截距项。 对应上面的特征向量，我们使用 PolynomialFeatures() 的主要作用是产生 2 次多项式对应的特征矩阵，如下所示： 1234567\"\"\"使用 PolynomialFeatures 自动生成特征矩阵\"\"\"from sklearn.preprocessing import PolynomialFeaturesX=[2, -1, 3]X_reshape = np.array(X).reshape(len(X), 1) # 转换为列向量PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_reshape) 123array([[ 2., 4.], [-1., 1.], [ 3., 9.]]) 对于上方单元格中的矩阵，第 1 列为 `X^1`，第 2 列为 `X^2`。我们就可以通过多元线性方程 `y(x, w) = w_0 + w_1x_1 + w_2x_2` 对数据进行拟合。 注意：本篇文章中，你会看到大量的 reshape 操作，它们的目的都是为了满足某些类传参的数组形状。这些操作在本实验中是必须的，因为数据原始形状（如上面的一维数组）可能无法直接传入某些特定类中。但在实际工作中并不是必须的，因为你手中的原始数据集形状可能支持直接传入。所以，不必为这些 reshape 操作感到疑惑，也不要死记硬背。 回到 2.1 小节中的示例数据，其自变量应该是 `x`，而因变量是 `y`。如果我们使用 2 次多项式拟合，那么首先使用 PolynomialFeatures() 得到特征矩阵。 123456789101112\"\"\"使用 sklearn 得到 2 次多项式回归特征矩阵\"\"\"from sklearn.preprocessing import PolynomialFeaturesx = np.array(x).reshape(len(x), 1) # 转换为列向量y = np.array(y).reshape(len(y), 1)poly_features = PolynomialFeatures(degree=2, include_bias=False)poly_x = poly_features.fit_transform(x)poly_x 12345678910array([[4.000e+00, 1.600e+01], [8.000e+00, 6.400e+01], [1.200e+01, 1.440e+02], [2.500e+01, 6.250e+02], [3.200e+01, 1.024e+03], [4.300e+01, 1.849e+03], [5.800e+01, 3.364e+03], [6.300e+01, 3.969e+03], [6.900e+01, 4.761e+03], [7.900e+01, 6.241e+03]]) 可以看到，输出结果正好对应一元二次多项式特征矩阵公式：`[ X, X^2]` 然后，我们使用 scikit-learn 训练线性回归模型。这里将会使用到 LinearRegression() 类，LinearRegression() 类的默认参数及常用参数定义如下： 1sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1) fit_intercept: 默认为 True，计算截距项。 normalize: 默认为 False，不针对数据进行标准化处理。 copy_X: 默认为 True，即使用数据的副本进行操作，防止影响原数据。 n_jobs: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。 12345678910\"\"\"转换为线性回归预测\"\"\"from sklearn.linear_model import LinearRegression# 定义线性回归模型model = LinearRegression()model.fit(poly_x, y) # 训练# 得到模型拟合参数model.intercept_, model.coef_ 1(array([2.13162821e-14]), array([[1.00000000e+00, 4.35999447e-18]])) 你会发现，这里得到的参数值和公式 (3), (4) 一致。为了更加直观，这里同样绘制出拟合后的图像。 1234567\"\"\"绘制拟合图像\"\"\"x_temp = np.array(x_temp).reshape(len(x_temp),1)poly_x_temp = poly_features.fit_transform(x_temp)plt.plot(x_temp, model.predict(poly_x_temp), 'r')plt.scatter(x, y) 你会发现，上图似曾相识。它和公式（3）下方的图其实是一致的。","link":"/2019/01/04/PolynomialRegression/"},{"title":"机器学习| K-近邻算法详解 (Python 语言描述)","text":"最近邻算法介绍 K-近邻算法之前，首先说一说最近邻算法。最近邻算法（Nearest Neighbor，简称：NN），其针对未知类别数据 `x`，在训练集中找到与 `x` 最相似的训练样本 `y`，用 `y` 的样本对应的类别作为未知类别数据 `x` 的类别，从而达到分类的效果。 如上图所示，通过计算数据 `X_{u}`（未知样本）和已知类别 `{\\omega_{1},\\omega_{2},\\omega_{3}}`（已知样本）之间的距离，判断 `X_{u}` 与不同训练集的相似度，最终判断 `X_{u}` 的类别。显然，这里将绿色未知样本类别判定与红色已知样本类别相同较为合适。 K-近邻算法K-近邻（K-Nearest Neighbors，简称：KNN）算法是最近邻（NN）算法的一个推广，也是机器学习分类算法中最简单的方法之一。KNN 算法的核心思想和最近邻算法思想相似，都是通过寻找和未知样本相似的类别进行分类。但 NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好。 如上图所示，对于未知测试样本(图中 ？所示)采用 KNN 算法进行分类，首先计算未知样本和训练样本之间的相似度，找出最近 K 个相邻样本（在图中 K 值为 3，圈定距离 ？最近的 3 个样本），再根据最近的 K 个样本最终判断未知样本的类别。 K-近邻算法实现KNN 算法在理论上已经非常成熟，其简单、易于理解的思想以及良好的分类准确度使得 KNN 算法应用非常广泛。算法的具体流程主要是以下的 4 个步骤： 数据准备：通过数据清洗，数据处理，将每条数据整理成向量。 计算距离：计算测试数据与训练数据之间的距离。 寻找邻居：找到与测试数据距离最近的 K 个训练数据样本。 决策分类：根据决策规则，从 K 个邻居得到测试数据的类别。 数据生成下面，我们尝试完成一个 KNN 分类流程。首先，生成一组示例数据，共包含 2 个类别（A和B），其中每一条数据包含两个特征（x和y）。 1234567891011\"\"\"生成示例数据\"\"\"import numpy as npdef create_data(): features = np.array( [[2.88, 3.05], [3.1, 2.45], [3.05, 2.8], [2.9, 2.7], [2.75, 3.4], [3.23, 2.9], [3.2, 3.75], [3.5, 2.9], [3.65, 3.6], [3.35, 3.3]]) labels = ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'] return features, labels 然后，我们尝试加载并打印这些数据。 12345\"\"\"打印示例数据\"\"\"features, labels = create_data()print('features: \\n', features)print('labels: \\n', labels) 12345678910111213features: [[2.88 3.05] [3.1 2.45] [3.05 2.8 ] [2.9 2.7 ] [2.75 3.4 ] [3.23 2.9 ] [3.2 2.75] [3.5 2.9 ] [3.65 3.6 ] [3.35 3.3 ]]labels: ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'] 为了更直观地理解数据，接下来用 Matplotlib 下的 pyplot 包来对数据集进行可视化。为了代码的简洁，我们使用了 map 函数和 lamda 表达式对数据进行处理。 12345678910111213\"\"\"示例数据绘图\"\"\"from matplotlib import pyplot as plt%matplotlib inlineplt.figure(figsize=(5, 5))plt.xlim((2.4, 3.8))plt.ylim((2.4, 3.8))x_feature = list(map(lambda x: x[0], features)) # 返回每个数据的x特征值y_feature = list(map(lambda y: y[1], features))plt.scatter(x_feature[:5], y_feature[:5], c=\"b\") # 在画布上绘画出\"A\"类标签的数据点plt.scatter(x_feature[5:], y_feature[5:], c=\"g\")plt.scatter([3.18], [3.15], c=\"r\", marker=\"x\") # 待测试点的坐标为 [3.1，3.2] 由上图所示，标签为 A（蓝色圆点）的数据在画布的左下角位置，而标签为 B（绿色圆点）的数据在画布的右上角位置，通过图像可以清楚看出不同标签数据的分布情况。其中红色 x 点即表示需预测类别的测试数据。 距离度量在计算两个样本间的相似度时，可以通过计算样本之间特征值的距离进行表示。若两个样本距离值越大（相距越远），则表示两个样本相似度低，相反，若两个样本值越小（相距越近），则表示两个样本相似度越高。 计算距离的方法有很多，本实验介绍两个最为常用的距离公式：曼哈顿距离和欧式距离。这两个距离的计算图示如下： 曼哈顿距离曼哈顿距离又称马氏距离，出租车距离，是计算距离最简单的方式之一。公式如下： d_{man}=\\sum_{i=1}^{N}\\left | X_{i}-Y_{i} \\right |其中： `X`,`Y`：两个数据点 `N`：每个数据中有 `N` 个特征值 `X_{i}` ：数据 `X` 的第 `i` 个特征值 公式表示为将两个数据 `X` 和 `Y` 中每一个对应特征值之间差值的绝对值，再求和，便得到曼哈顿距离。 12345678910111213141516\"\"\"曼哈顿距离计算\"\"\"import numpy as npdef d_man(x, y): d = np.sum(np.abs(x - y)) return dx = np.array([3.1, 3.2])print(\"x:\", x)y = np.array([2.5, 2.8])print(\"y:\", y)d_man = d_man(x, y)print(d_man) 123x: [3.1 3.2]y: [2.5 2.8]1.0000000000000004 欧式距离欧式距离源自 `N` 维欧氏空间中两点之间的距离公式。表达式如下: d_{euc}= \\sqrt{\\sum_{i=1}^{N}(X_{i}-Y_{i})^{2}}其中： `X`, `Y` ：两个数据点 `N`：每个数据中有 `N` 个特征值 `X_{i}` ：数据 `X` 的第 `i` 个特征值 公式表示为将两个数据 X 和 Y 中的每一个对应特征值之间差值的平方，再求和，最后开平方，便是欧式距离。 12345678910111213141516\"\"\"欧氏距离的计算\"\"\"import numpy as npdef d_euc(x, y): d = np.sqrt(np.sum(np.square(x - y))) return dx = np.random.random(10) # 随机生成10个数的数组作为x特征的值print(\"x:\", x)y = np.random.random(10)print(\"y:\", y)distance_euc = d_euc(x, y)print(distance_euc) 12345x: [0.10725148 0.78394185 0.85568109 0.5774587 0.96974919 0.79467734 0.26009361 0.93204 0.08424034 0.16970618]y: [0.88013554 0.5943479 0.31357311 0.20830397 0.20686205 0.9475627 0.61453761 0.27882129 0.61228018 0.75968914]1.6876178018976438 决策规则在得到测试样本和训练样本之间的相似度后，通过相似度的排名，可以得到每一个测试样本的 K 个相邻的训练样本，那如何通过 K 个邻居来判断测试样本的最终类别呢？可以根据数据特征对决策规则进行选取，不同的决策规则会产生不同的预测结果，最常用的决策规则是： 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。 这里推荐使用多数表决法，这种方法更加简单。 12345678910111213\"\"\"多数表决法\"\"\"import operatordef majority_voting(class_count): sorted_class_count = sorted( class_count.items(), key=operator.itemgetter(1), reverse=True) return sorted_class_countarr = {'A': 3, 'B': 2, \"C\": 6, \"D\": 5}majority_voting(arr) 1[('C', 6), ('D', 5), ('A', 3), ('B', 2)] 在多数表决法的定义中，我们导入了 operater 计算模块，目的是对字典类型结构排序。可以从结果中看出函数返回的结果为票数最多的 C，得票为 6 次。 KNN 算法实现在学习完以上的各个步骤之后，KNN 算法也逐渐被勾勒出来。以下就是对 KNN 算法的完整实现，本次的距离计算采用欧式距离，分类的决策规则为多数表决法，定义函数 knn_classify()，其中函数的参数包括： test_data：用于分类的输入向量。 train_data：输入的训练样本集。 labels：样本数据的类标签向量。 k：用于选择最近邻居的数目。 12345678910111213141516171819202122\"\"\"KNN 方法完整实现\"\"\"def knn_classify(test_data, train_data, labels, k): distances = np.array([]) # 创建一个空的数组用于存放距离 for each_data in train_data: # 使用欧式距离计算数据相似度 d = d_euc(test_data, each_data) distances = np.append(distances, d) sorted_distance_index = distances.argsort() # 获取按距离大小排序后的索引 sorted_distance = np.sort(distances) r = (sorted_distance[k]+sorted_distance[k-1])/2 # 计算 class_count = {} for i in range(k): # 多数表决 vote_label = labels[sorted_distance_index[i]] class_count[vote_label] = class_count.get(vote_label, 0) + 1 final_label = majority_voting(class_count) return final_label, r 分类预测在实现 KNN 算法之后，接下来就可以对我们未知数据[3.18,3.15]开始分类,假定我们 K 值初始设定为 5，让我们看看分类的效果。 123test_data = np.array([3.18, 3.15])final_label, r = knn_classify(test_data, features, labels, 5)final_label 1[('B', 3), ('A', 2)] 可视化展示在对数据 [3.18,3.15] 实现分类之后，接下来我们同样用画图的方式形象化展示 KNN 算法决策方式。 123456789101112131415161718def circle(r, a, b): # 为了画出圆，这里采用极坐标的方式对圆进行表示 ：x=r*cosθ，y=r*sinθ。 theta = np.arange(0, 2*np.pi, 0.01) x = a+r * np.cos(theta) y = b+r * np.sin(theta) return x, yk_circle_x, k_circle_y = circle(r, 3.18, 3.15)plt.figure(figsize=(5, 5))plt.xlim((2.4, 3.8))plt.ylim((2.4, 3.8))x_feature = list(map(lambda x: x[0], features)) # 返回每个数据的x特征值y_feature = list(map(lambda y: y[1], features))plt.scatter(x_feature[:5], y_feature[:5], c=\"b\") # 在画布上绘画出\"A\"类标签的数据点plt.scatter(x_feature[5:], y_feature[5:], c=\"g\")plt.scatter([3.18], [3.15], c=\"r\", marker=\"x\") # 待测试点的坐标为 [3.1，3.2]plt.plot(k_circle_x, k_circle_y) 如图所示，当我们 K 值为 5 时，与测试样本距离最近的 5 个训练数据（如蓝色圆圈所示）中属于 B 类的有 3 个，属于 A 类的有 2 个，根据多数表决法决策出测试样本的数据为 B 类。 通过尝试不同的 K 值我们会发现，不同的 K 值预测出不同的结果。","link":"/2019/01/07/K-Nearest Neighbors/"},{"title":"翻译|Gradient Descent in Python","text":"123import numpy as npimport matplotlib.pyplot as plt%matplotlib inline 1plt.style.use(['ggplot']) 当你初次涉足机器学习时，你学习的第一个基本算法就是 梯度下降 (Gradient Descent), 可以说梯度下降法是机器学习算法的支柱。 在这篇文章中，我尝试使用 `python` 解释梯度下降法的基本原理。一旦掌握了梯度下降法，很多问题就会变得容易理解，并且利于理解不同的算法。 如果你想尝试自己实现梯度下降法， 你需要加载基本的 \\(python\\) \\(packages\\) —— \\(numpy\\) and \\(matplotlib\\)首先， 我们将创建包含着噪声的线性数据 123# 随机创建一些噪声X = 2 * np.random.rand(100, 1)y = 4 + 3 * X + np.random.randn(100, 1) 接下来通过 matplotlib 可视化数据 12345# 可视化数据plt.plot(X, y, 'b.')plt.xlabel(\"$x$\", fontsize=18)plt.ylabel(\"$y$\", rotation=0, fontsize=18)plt.axis([0, 2, 0, 15]) 显然， `y` 与 `x` 具有良好的线性关系，这个数据非常简单，只有一个自变量 `x`. 我们可以将其表示为简单的线性关系： y = b + mx并求出 `b` , `m`。 这种被称为解方程的分析方法并没有什么不妥，但机器学习是涉及矩阵计算的，因此我们使用矩阵法（向量法）进行分析。 我们将 `y` 替换成 \\(J(\\theta)\\)， `b` 替换成 \\(\\theta_0\\)， `m` 替换成 \\(\\theta_1\\)。得到如下表达式： J(\\theta) = \\theta_0 + \\theta_1 x注意： 本例中 \\(\\theta_0 = 4\\)， \\(\\theta_1 = 3\\) 求解 \\(\\theta_0\\) 和 \\(\\theta_1\\) 的分析方法，代码如下： 123X_b = np.c_[np.ones((100, 1)), X] # 为X添加了一个偏置单位，对于X中的每个向量都是1theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)theta_best array([[3.86687149], [3.12408839]]) 不难发现这个值接近真实的 \\(\\theta_0\\)，\\(\\theta_1\\)，由于我在数据中引入了噪声，所以存在误差。 1234X_new = np.array([[0], [2]])X_new_b = np.c_[np.ones((2, 1)), X_new]y_predict = X_new_b.dot(theta_best)y_predict array([[ 3.86687149], [10.11504826]]) 梯度下降法 （Gradient Descent）Cost Function &amp; Gradients计算代价函数和梯度的公式如下所示。 注意：代价函数用于线性回归，对于其他算法，代价函数是不同的，梯度必须从代价函数中推导出来。 Cost \\begin{equation} J(\\theta) = 1/2m \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 \\end{equation}Gradient \\begin{equation} \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_j^{(i)} \\end{equation}Gradients \\begin{equation}\\theta_0: = \\theta_0 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)})\\end{equation} \\begin{equation}\\theta_1: = \\theta_1 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_1^{(i)})\\end{equation} \\begin{equation}\\theta_2: = \\theta_2 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_2^{(i)})\\end{equation} \\begin{equation}\\theta_j: = \\theta_j -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)})\\end{equation} 1234567891011121314151617def cal_cost(theta, X, y): ''' Calculates the cost for given X and Y. The following shows and example of a single dimensional X theta = Vector of thetas X = Row of X's np.zeros((2,j)) y = Actual y's np.zeros((2,1)) where: j is the no of features ''' m = len(y) predictions = X.dot(theta) cost = (1/2*m) * np.sum(np.square(predictions - y)) return cost 12345678910111213141516171819202122232425def gradient_descent(X, y, theta, learning_rate = 0.01, iterations = 100): ''' X = Matrix of X with added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len(y) # learning_rate = 0.01 # iterations = 100 cost_history = np.zeros(iterations) theta_history = np.zeros((iterations, 2)) for i in range(iterations): prediction = np.dot(X, theta) theta = theta - (1/m) * learning_rate * (X.T.dot((prediction - y))) theta_history[i, :] = theta.T cost_history[i] = cal_cost(theta, X, y) return theta, cost_history, theta_history 123456789# 从1000次迭代开始，学习率为0.01。从高斯分布的θ开始lr =0.01n_iter = 1000theta = np.random.randn(2, 1)X_b = np.c_[np.ones((len(X), 1)), X]theta, cost_history, theta_history = gradient_descent(X_b, y, theta, lr, n_iter)print('Theta0: {:0.3f},\\nTheta1: {:0.3f}'.format(theta[0][0],theta[1][0]))print('Final cost/MSE: {:0.3f}'.format(cost_history[-1])) Theta0: 3.867, Theta1: 3.124 Final cost/MSE: 5457.747 123456# 绘制迭代的成本图fig, ax = plt.subplots(figsize=(12,8))ax.set_ylabel('J(Theta)')ax.set_xlabel('Iterations')ax.plot(range(1000), cost_history, 'b.') 在大约 150 次迭代之后代价函数趋于稳定，因此放大到迭代200，看看曲线 12fig, ax = plt.subplots(figsize=(10,8))ax.plot(range(200), cost_history[:200], 'b.') 值得注意的是，最初成本下降得更快，然后成本降低的收益就不那么多了。 我们可以尝试使用不同的学习速率和迭代组合，并得到不同学习率和迭代的效果会如何。 让我们建立一个函数，它可以显示效果，也可以显示梯度下降实际上是如何工作的。 123456789101112131415161718192021222324252627def plot_GD(n_iter, lr, ax, ax1=None): ''' n_iter = no of iterations lr = Learning Rate ax = Axis to plot the Gradient Descent ax1 = Axis to plot cost_history vs Iterations plot ''' ax.plot(X, y, 'b.') theta = np.random.randn(2, 1) tr = 0.1 cost_history = np.zeros(n_iter) for i in range(n_iter): pred_prev = X_b.dot(theta) theta, h, _ = gradient_descent(X_b, y, theta, lr, 1) pred = X_b.dot(theta) cost_history[i] = h[0] if ((i % 25 == 0)): ax.plot(X, pred, 'r-', alpha=tr) if tr &lt; 0.8: tr += 0.2 if not ax1 == None: ax1.plot(range(n_iter), cost_history, 'b.') 1234567891011121314151617# 绘制不同迭代和学习率组合的图fig = plt.figure(figsize=(30,25), dpi=200)fig.subplots_adjust(hspace=0.4, wspace=0.4)it_lr = [(2000, 0.001), (500, 0.01), (200, 0.05), (100, 0.1)]count = 0for n_iter, lr in it_lr: count += 1 ax = fig.add_subplot(4, 2, count) count += 1 ax1 = fig.add_subplot(4, 2, count) ax.set_title(\"lr:{}\" .format(lr)) ax1.set_title(\"Iterations:{}\" .format(n_iter)) plot_GD(n_iter, lr, ax, ax1) 通过观察发现，以较小的学习速率收集解决方案需要很长时间，而学习速度越大，学习速度越快。 12_, ax = plt.subplots(figsize=(14, 10))plot_GD(100, 0.1, ax) 随机梯度下降法（Stochastic Gradient Descent）随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的 `m` 个样本的数据，而是仅仅选取一个样本 `j` 来求梯度。对应的更新公式是： \\theta_i = \\theta_i - \\alpha (h_\\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)}123456789101112131415161718192021222324252627def stocashtic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=10): ''' X = Matrix of X with added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len(y) cost_history = np.zeros(iterations) for it in range(iterations): cost = 0.0 for i in range(m): rand_ind = np.random.randint(0, m) X_i = X[rand_ind, :].reshape(1, X.shape[1]) y_i = y[rand_ind, :].reshape(1, 1) prediction = np.dot(X_i, theta) theta -= (1/m) * learning_rate * (X_i.T.dot((prediction - y_i))) cost += cal_cost(theta, X_i, y_i) cost_history[it] = cost return theta, cost_history 12345678lr = 0.5n_iter = 50theta = np.random.randn(2,1)X_b = np.c_[np.ones((len(X),1)), X]theta, cost_history = stocashtic_gradient_descent(X_b, y, theta, lr, n_iter)print('Theta0: {:0.3f},\\nTheta1: {:0.3f}' .format(theta[0][0],theta[1][0]))print('Final cost/MSE: {:0.3f}' .format(cost_history[-1])) Theta0: 3.762, Theta1: 3.159 Final cost/MSE: 46.964 1234567fig, ax = plt.subplots(figsize=(10,8))ax.set_ylabel('$J(\\Theta)$' ,rotation=0)ax.set_xlabel('$Iterations$')theta = np.random.randn(2,1)ax.plot(range(n_iter), cost_history, 'b.') 小批量梯度下降法（Mini-batch Gradient Descent）小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于 `m` 个样本，我们采用 `x` 个样子来迭代，\\(1&lt;x&lt;m\\)。一般可以取 `x=10`，当然根据样本的数据，可以调整这个 `x` 的值。对应的更新公式是： \\theta_i = \\theta_i - \\alpha \\sum\\limits_{j=t}^{t+x-1}(h_\\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)}1234567891011121314151617181920212223242526272829303132def minibatch_gradient_descent(X, y, theta, learning_rate=0.01, iterations=10, batch_size=20): ''' X = Matrix of X without added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len(y) cost_history = np.zeros(iterations) n_batches = int(m / batch_size) for it in range(iterations): cost = 0.0 indices = np.random.permutation(m) X = X[indices] y = y[indices] for i in range(0, m, batch_size): X_i = X[i: i+batch_size] y_i = y[i: i+batch_size] X_i = np.c_[np.ones(len(X_i)), X_i] prediction = np.dot(X_i, theta) theta -= (1/m) * learning_rate * (X_i.T.dot((prediction - y_i))) cost += cal_cost(theta, X_i, y_i) cost_history[it] = cost return theta, cost_history 1234567lr = 0.1n_iter = 200theta = np.random.randn(2, 1)theta, cost_history = minibatch_gradient_descent(X, y, theta, lr, n_iter)print('Theta0: {:0.3f},\\nTheta1: {:0.3f}' .format(theta[0][0], theta[1][0]))print('Final cost/MSE: {:0.3f}' .format(cost_history[-1])) Theta0: 3.842, Theta1: 3.146 Final cost/MSE: 1090.518 1234567fig, ax = plt.subplots(figsize=(10,8))ax.set_ylabel('$J(\\Theta)$', rotation=0)ax.set_xlabel('$Iterations$')theta = np.random.randn(2, 1)ax.plot(range(n_iter), cost_history, 'b.') 参考： Gradient Descent in Python 梯度下降（Gradient Descent）小结","link":"/2019/03/09/Gradient_Descent/"},{"title":"推荐系统|矩阵分解概述","text":"&emsp;推荐系统的研究从上世纪90年代初发展至今，目前有三大主流算法作为几乎全部的推荐算法的基石，它们就是基于内容的过滤算法（content-based filtering，简称CBF）、邻域算法（neighborhood methods）、隐语义模型（latent factor models，简称LFM），其中后两者统称为协同过滤算法（collaborative filtering，简CF）。&emsp;CBF通过给用户、物品定义显式的属性（通常会找所涉及的推荐领域的人类专家来定义）来描述他们的本质，然后为用户推荐与他们本质“门当户对”的物品；CF则是通过发动“群体的力量”，从其他用户、物品中学习到宝贵的信息，无需显式地定义属性：CF下的邻域算法着重于学习用户与用户、物品与物品之间的关系，为目标用户推荐与目标用户相似的用户所选择的物品（user-based）或者与目标用户所选择的物品相似的物品（item-based）；CF下的隐语义模型则是通过学习用户与用户、物品与物品之间的关系来自动获得用户、物品的隐属性（这里的“隐”指的是学习到的属性是不可解释的），相当于把用户-评分矩阵分解成用户隐属性矩阵和物品隐属性矩阵，然后通过用户隐属性向量u与物品隐属性向量i作点乘来获取到该用户对该物品的评分，以此为依据进行推荐。 矩阵分解的主要思想&emsp;如上图所示，矩阵分解是构建隐语义模型的主要方法，即通过把整理、提取好的“用户—物品”评分矩阵进行分解，来得到一个用户隐向量矩阵和一个物品隐向量矩阵。假设现在有一个 \\(M*N\\) 的矩阵，\\(M\\) 代表用户数，\\(N\\) 代表物品数，想将用户、物品分别训练出两个隐属性，即每个用户、每个物品都对应着一个二维向量，即得到了一个 \\(M*2\\) 的用户隐向量矩阵和一个 \\(N*2\\) 的矩阵，分解示意图如下所示： &emsp;在得到用户隐向量和物品隐向量（均是2维向量）之后，我们可以将每个用户、物品对应的二维隐向量看作是一个坐标，将其画在坐标轴上。虽然我们得到的是不可解释的隐向量，但是可以为其赋予一定的意义来帮助我们理解这个分解结果。比如我们把用户、物品的2维的隐向量赋予严肃文学（Serious）vs.消遣文学（Escapist）、针对男性（Geared towards males）vs.针对女性（Geared towards females），那么可以形成如下图的可视化图片： &emsp;从上图我们可以看到，用户对于与其处于同一象限的物品的喜爱度/评分是会很高的，因为他们相比于其它的组合更加“门当户对”一些。而实例人物Dave对应的坐标恰好处于坐标系的中央，这说明他对图中所有物品的喜爱程度差不多，没有特别喜欢的也没有特别讨厌的。 矩阵分解中的显式反馈与隐式反馈&emsp;推荐系统、推荐算法的设计依赖于多种输入。在上部分中，我们针对用户—物品评分矩阵来对其进行分解，得到了两个隐向量矩阵。这里我们用到的输入就是这个评分矩阵。在推荐系统中，用户的评分信息输入最重要的输入信息，也是一种显式反馈。不过，显式反馈的信息往往是很稀有的，也就是说我们要分解的评分矩阵往往是一个很稀疏的矩阵，可能里面 70% 以上的元素都是 0，只有 30% 的部分是稀稀落落的评分，所以单纯地依赖显式反馈信息在如今会得到正确率较低的推荐结果。不过矩阵分解的好处在于，它可以融入多种额外的信息。用户的购买、浏览、点击行为虽然不如评分那样有着很大的信息量，但是也是一种隐式反馈的信息，利用好它们，我们可以组成一个很稠密的矩阵，以此来改良推荐结果。 矩阵分解过程完整代码地址 👉 https://github.com/DL-Metaphysics/APR/blob/master/MF/MF.ipynb&emsp;先把之后需要用到的全部的数学符号或者缩略语都统一列到下表中： &emsp;如前面所介绍的，矩阵分解可以融入多种额外信息，不断地对待分解矩阵进行升级、改良，整体的矩阵分解框架如下图所示： 我们首先看一下没有融合额外信息的时候，是如何来预测评分的： \\hat{r}_{u i}=q_{i}^{T} * p_{u}&emsp;可以看出，其实很简单的。当我们要预测 `u` 对 `i` 评分的时候，就直接把对应的用户隐向量、物品隐向量直接点乘起来就好了，就得到预测的评分了。但是这两个矩阵是怎么来的呢？当然是训练出来的啦，我们要通过优化下面这个目标函数来训练出这两个矩阵，这就是标准的“机器学习版”的矩阵分解算法，也是推荐系统领域非常重要的算法： \\begin{equation}\\min _{q^{*}, p^{*}} \\sum_{(u, i) \\in K}\\left(r_{u i}-q_{i}^{T} * p_{u}\\right)^{2}+\\lambda *\\left(\\left|q_{i}\\right|^{2}+\\left|p_{u}\\right|^{2}\\right)\\end{equation} 我们采用随机梯度下降（SGD）算法来训练两个隐向量矩阵： \\begin{aligned} q_{i} &amp;=q_{i}+\\gamma *\\left(e_{u i} * p_{u}-\\lambda * q_{i}\\right)\\end{aligned} \\begin{aligned}p_{u}&amp;=p_{u}+\\gamma *\\left(e_{u i} * q_{i}-\\lambda * p_{u}\\right)\\end{aligned} 整个SGD的函数代码如下所示： 12345678910def sgd(data_matrix, user, item, alpha, lam, iter_num): for j in range(iter_num): for u in range(data_matrix.shape[0]): for i in range(data_matrix.shape[1]): if data_matrix[u][i] != 0: e_ui = data_matrix[u][i] - sum(user[u,:] * item[i,:]) user[u,:] += alpha * (e_ui * item[i,:] - lam * user[u,:]) item[i,:] += alpha * (e_ui * user[u,:] - lam * item[i,:]) return user, item &emsp;这样我们就训练出了两个隐向量矩阵，将他们相乘便得到了预测版的评分矩阵，可以和真实评分矩阵对比一下，如果训练的次数足够，训练步长不大的话其实预测的评分已经比较准了。 &emsp;但是，有一个问题我们需要考虑一下。如果有的用户比较苛刻，对他来说，烂片最多打1分，好的片子也就得3-4分，有的用户比较宽容，他认为人家拍个电影挺不容易的，烂片也给了3分，好的片子一律给5分，那对于这两种用户，我们要采取同样的对待方式进行预测吗？如果有的电影拍的真心好，普遍评分都很高，有的电影烂出了新高度，基本上上3分那都是绝对的高分了，那这两种电影真的都处于0-5分这一分段吗？ &emsp;在这种情况下，我们其实需要为每个用户和每个物品加入一些偏置元素 bu 和 bi，代表了他们自带的与其他事物无关的属性，融入了这些元素，才能区别且正确地对待每一个用户和每一个物品，才能在预测中显得更加个性化。所以，预测评分的计算公式就变成了这样： \\hat{r}_{u i}=b_{u i}+q_{i}^{T} * p_{u}&emsp;我们要优化、训练参数的目标公式也就变成了下图所示，要训练的参数除了用户、物品隐向量还要加上用户、物品偏置值，训练的方法同样是采用随机梯度下降法： \\begin{equation}\\min _{q^{*}, p^{*}} \\sum_{(u, i) \\in K}\\left(r_{u i}-b_{u i}-q_{i}^{T} * p_{u}\\right)^{2}+\\lambda *\\left(\\left|q_{i}\\right|^{2}+\\left|p_{u}\\right|^{2}+b_{u}^{2}+b_{i}^{2}\\right)\\end{equation} 整个SGD_bias的函数的代码如下所示： 123456789101112131415def sgd_bias(data_matrix, user, item, alpha, lam, iter_num, miu): b_u = [1] * rating_matrix.shape[0] b_i = [1] * rating_matrix.shape[1] for j in range(iter_num): for u in range(data_matrix.shape[0]): for i in range(data_matrix.shape[1]): if data_matrix[u][i] != 0: b_ui = b_u[u] + b_i[i] + miu e_ui = data_matrix[u][i] - b_ui - sum(user[u,:] * item[i,:]) user[u,:] += alpha * (e_ui * item[i,:] - lam * user[u,:]) item[i,:] += alpha * (e_ui * user[u,:] - lam * item[i,:]) b_u[u] += alpha * (e_ui - lam * b_u[u]) b_i[i] += alpha * (e_ui - lam * b_i[i]) return user, item, b_u, b_i &emsp;在加入偏置后，预测的就更加准确一些了。在编程实验中，笔者采用了推荐系统算法常见的评测标准——MSE来进行两种分解算法的评测，两者的结果如下所示（注：训练步长0.001，正则化系数0.1，训练次数：1000次）： 原文链接 - 论文篇：Matrix Factorization Techniques for RS","link":"/2019/04/09/Matrix_Factorization_Techniques_for_RS/"},{"title":"机器学习术语表","text":"本术语表中列出了一般的机器学习术语和 TensorFlow 专用术语的定义。 提示：你可以通过中文名称拼音首字母快速检索。 C超参数｜Hyperparameter在机器学习中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给模型选择一组最优超参数，以提高学习的性能和效果。 超平面｜Hyperplane将一个空间划分为两个子空间的边界。例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。在机器学习中更典型的是：超平面是分隔高维度空间的边界。核支持向量机利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。 参数｜Parameter机器学习系统自行训练的模型的变量。 例如，权重就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。参数的概念与超参数相对应。 测试集｜Test Set数据集的子集，用于在模型经过验证集验证之后测试模型。当然，有时候我们不设置验证集（主要用于模型调参），直接使用训练数据训练模型后就进行测试。 D 独热编码｜One-Hot Encoding一种稀疏向量，其中： 一个元素设为 1。 所有其他元素均设为 0。 One-Hot 编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为 One-Hot 向量，向量的大小为 15000。 独立同分布独立就是每次抽样之间是没有关系的,不会相互影响。 同分布，意味着随机变量 `X_1` 和 `X_2` 具有相同的分布形状和相同的分布参数，对离散随机变量具有相同的分布律，对连续随机变量具有相同的概率密度函数，有着相同的分布函数，相同的期望、方差。 例如，某个网页的访问者在短时间内的分布可能为独立同分布，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。 迭代｜Iteration模型的权重在训练期间的一次更新，迭代包含计算参数在单个批量数据的梯度损失。 F 泛化｜Generalization指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。 反向传播算法｜Backpropagation在神经网络上执行梯度下降法的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数。 G 过拟合｜Overfitting创建的模型与训练数据过于匹配，以致于模型无法根据新数据做出正确的预测。 如图，绿线代表过拟合模型，黑线代表正则化模型。虽然绿线完美的匹配训练数据，但太过依赖，并且与黑线相比，对于新的测试数据上具有更高的错误率。 ©️ 图片来源 H 混淆矩阵｜Confusion Matrix一种 NxN 表格，用于总结分类模型的预测成效；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在二元分类问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例： 上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个真正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个真负例），归类错误的有 6 个（6 个假正例）。 多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。混淆矩阵包含计算各种效果指标（包括精确率和召回率）所需的充足信息。 J 集成学习｜Ensemble多个模型的预测结果的并集。 通俗来讲，集成学习把大大小小的多种算法融合在一起，共同协作来解决一个问题。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等。 你可以通过以下一项或多项来创建集成学习： 不同的初始化 不同的超参数 不同的整体结构 决策边界｜Decision Boundary在二元分类或多类别分类问题中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线： 精确率｜Precision一种分类模型指标。精确率指模型正确预测正类别的频率。 召回率 = \\frac { 真正例数 } { 真正例数 + 假负例数 }交叉熵｜Cross-Entropy对数损失函数向多类别分类问题进行的一种泛化。交叉熵可以量化两种概率分布之间的差异。 H(p,q) = \\sum_{i=1}^{n} p(x) \\cdot log(\\frac{1}{q(x)})激活函数｜Activation Function一种函数（例如 ReLU 或 S 型函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。 结构风险最小化｜Structural Risk Minimization一种算法，用于平衡以下两个目标： 期望构建最具预测性的模型（例如损失最低）。 期望使模型尽可能简单（例如强大的正则化）。 例如，旨在将基于训练集的损失和正则化降至最低的模型函数就是一种结构风险最小化算法。 L 离群点｜Outlier与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。 绝对值很高的权重。 与实际值相差很大的预测值。 值比平均值高大约 3 个标准偏差的输入数据。 离群值常常会导致模型训练出现问题。 类别｜Class为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的二元分类模型中，两种类别分别是「垃圾邮件」和「非垃圾邮件」。在识别狗品种的多类别分类模型中，类别可以是「贵宾犬」、「小猎犬」、「哈巴犬」等。 离散特征｜Discrete Feature一种特征，包含有限个可能值。例如，某个值只能是「动物」、「蔬菜」或「矿物」的特征便是一个离散特征（或分类特征）。与连续特征相对。 M 密集层｜Dense Layer是全连接层的同义词。 P 批次｜Batch模型训练的一次迭代（即一次梯度更新）中使用样本簇。 偏差｜Bias距离原点的截距或偏移。偏差（也称为偏差项）在机器学习模型中以 `b` 或 `w_0` 表示。例如，在下面的公式中，偏差为 `b`： y' = b + w_1x_1 + w_2x_2 + … w_nx_n请勿与「预测偏差」混淆。 批次规模｜Batch Size模型迭代一次，使用的样本集的大小。 例如训练集有 6400 个样本，batch_size=128，那么训练完整个样本集需要 50 次迭代。Batch Size 的大小一般设置为 16 及 16 的倍数。 R ROC 曲线下面积一种会考虑所有可能分类阀值的评价指标。 ROC 曲线下面积的数值意义为：对于随机选择的正类别样本确实为正类别，以及随机选择的负类样本为正类别，分类器更确信前者的概率。 S 输入层｜Input Layer神经网络中的第一层（接受输入数据的层） 输出层｜Output Layer神经网络最后一层。 损失｜Loss一种衡量指标，用于衡量模型的预测偏离其标签的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将均方误差用于损失函数，而逻辑回归模型则使用对数损失函数。 收敛｜Convergence通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练损失和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。 缩放｜Scaling特征工程中的一种常用做法，是对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。 随机梯度下降法｜SGDSGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。 Softmax 函数一种函数，可提供多类别分类模型中每个可能类别的概率。这些概率的总和正好为 1.0。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为完整 softmax。） T 推断｜Inference在机器学习中，推断通常指以下过程：通过将训练过的模型应用于无标签样本来做出预测。在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。 梯度｜Gradient偏导数相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。 梯度下降法｜Gradient Descent一种通过计算并且减小梯度将损失降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到权重和偏差的最佳组合，从而将损失降至最低。 特征｜Feature在进行预测时使用的输入变量。 特征组合｜Feature Cross通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的合成特征。特征组合有助于表示非线性关系。 特征工程｜Feature Engineering指以下过程：确定哪些特征可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。 凸函数｜Convex Function一种函数，函数图像以上的区域为凸集。典型凸函数的形状类似于字母 U。例如，以下都是凸函数： 严格凸函数只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。 很多常见的损失函数（包括下列函数）都是凸函数： L2 损失函数 对数损失函数 L1 正则化 L2 正则化 梯度下降法的很多变体都一定能找到一个接近严格凸函数最小值的点。同样，随机梯度下降法的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。 两个凸函数的和（例如 L2 损失函数 + L1 正则化）也是凸函数。 深度模型绝不会是凸函数。值得注意的是，专门针对凸优化设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。 凸优化｜Convex Optimization使用数学方法（例如梯度下降法）寻找凸函数最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。 X 学习率｜Learning Rate在训练模型时用于梯度下降的一个变量。在每次迭代期间，梯度下降法都会将学习速率与梯度相乘。得出的乘积称为梯度步长。 学习速率是一个重要的超参数。 稀疏特征｜Sparse Feature一种特征向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。 与密集特征相对。 协同过滤｜Collabroative Filtering根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。 Y 预训练模型已经过训练的模型或模型组件（例如嵌套）。有时，您需要将预训练的嵌套馈送到神经网络。在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。 Z 准确率｜Accuracy分类模型的正确预测所占的比例。在多类别分类中，准确率的定义如下： 准确率 = \\frac { 正确预测数 } { 样本总数 }在二元分类中，准确率的定义如下： 准确率 = \\frac { 真正例数 + 真负例数 } { 样本总数 }真负例被模型正确地预测为负类别的样本。例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。 真正例被模型正确地预测为正类别的样本。例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。 召回率一种分类模型指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即： 召回率 = \\frac { 真正例数 } { 真正例数 + 假正例数}张量｜TensorTensorFlow 程序中的主要数据结构。张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。 迁移学习｜Transfer Learning将信息从一个机器学习任务转移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的深度模型。迁移学习可能涉及将知识从较简单任务的解决方案转移到较复杂的任务，或者将知识从数据较多的任务转移到数据较少的任务。 大多数机器学习系统都只能完成一项任务。迁移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。 L1 正则化｜L1 Regularization一种正则化，根据权重的绝对值的总和来惩罚权重。在依赖稀疏特征的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 L2 正则化相对。 L2 正则化｜L2 Regularization一种正则化，根据权重的平方和来惩罚权重。L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 L1 正则化相对。）在线性模型中，L2 正则化始终可以改进泛化。 周期｜Epoch在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（N/批次规模）次训练迭代，其中 N 是样本总数。 ©️ 部分内容参考自 [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/)","link":"/2018/11/13/machine_learning_glossary/"},{"title":"机器学习| 朴素贝叶斯详解 (Python 语言描述)","text":"在分类预测中，以概率论作为基础的算法比较少，而朴素贝叶斯就是其中之一。朴素贝叶斯算法实现简单，且预测分类的效率很高，是一种十分常用的算法。 朴素贝叶斯基础基本概念朴素贝叶斯的数学理论基础源于概率论。所以，在学习朴素贝叶斯算法之前，首先对其中涉及到的概率论知识做简要讲解。 条件概率条件概率就是指事件 \\(A\\) 在另外一个事件 \\(B\\) 已经发生条件下的概率。如图所示 ： 其中： \\(P(A)\\) 表示 \\(A\\) 事件发生的概率。 \\(P(B)\\) 表示 \\(B\\) 事件发生的概率。 \\(P(AB)\\) 表示 \\(A, B\\) 事件同时发生的概率。 而最终计算得到的 \\(P(A \\mid B)\\) 便是条件概率，表示在 \\(B\\) 事件发生的情况下 \\(A\\) 事件发生的概率。 贝叶斯定理上面提到了条件概率的基本概念，那么当知道事件 \\(B\\) 发生的情况下事件 \\(A\\) 发生的概率 \\(P(A \\mid B)\\)，如何求 \\(P(B \\mid A)\\) 呢？贝叶斯定理应运而生。根据条件概率公式可以得到: P(B \\mid A)=\\frac{P(AB)}{P(A)} \\tag1而同样通过条件概率公式可以得到： P(AB)=P(A \\mid B)*P(B) \\tag2将 (2) 式带入 (1) 式便可得到完整的贝叶斯定理： P(B \\mid A)=\\frac{P(AB)}{P(A)}=\\frac{P(A \\mid B)*P(B)}{P(A)} \\tag{3}以下，通过一张图来完整且形象的展示条件概率和贝叶斯定理的原理。 先验概率先验概率（Prior Probability）指的是根据以往经验和分析得到的概率。例如以上公式中的 \\(P(A), P(B)\\),又例如：\\(X\\) 表示投一枚质地均匀的硬币，正面朝上的概率，显然在我们根据以往的经验下，我们会认为 \\(X\\) 的概率 \\(P(X) = 0.5\\) 。其中 \\(P(X) = 0.5\\) 就是先验概率。 后验概率后验概率（Posterior Probability）是事件发生后求的反向条件概率；即基于先验概率通过贝叶斯公式求得的反向条件概率。例如公式中的 \\(P(B \\mid A)\\) 就是通过先验概率 \\(P(A)\\)和\\(P(B)\\) 得到的后验概率，其通俗的讲就是「执果寻因」中的「因」。 什么是朴素贝叶斯朴素贝叶斯（Naive Bayes）就是将贝叶斯原理以及条件独立结合而成的算法，其思想非常的简单，根据贝叶斯公式： P(B \\mid A)=\\frac{P(A \\mid B)*P(B)}{P(A)} \\tag{4}变形表达式为： P(类别 \\mid 特征)=\\frac{P(特征 \\mid 类别) * P(类别)}{P(特征)} \\tag{5}公式（5）利用先验概率，即特征和类别的概率；再利用不同类别中各个特征的概率分布，最后计算得到后验概率，即各个特征分布下的预测不同的类别。 利用贝叶斯原理求解固然是一个很好的方法，但实际生活中数据的特征之间是有相互联系的，在计算 \\(P(特征\\mid类别)\\) 时，考虑特征之间的联系会比较麻烦，而朴素贝叶斯则人为的将各个特征割裂开，认定特征之间相互独立。 朴素贝叶斯中的「朴素」，即条件独立，表示其假设预测的各个属性都是相互独立的,每个属性独立地对分类结果产生影响，条件独立在数学上的表示为：\\(P(AB)=P(A)*P(B)\\)。这样，使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。对于预测数据，求解在该预测数据的属性出现时各个类别的出现概率，将概率值大的类别作为预测数据的类别。 朴素贝叶斯算法实现前面主要介绍了朴素贝叶斯算法中几个重要的概率论知识，接下来我们对其进行具体的实现，算法流程如下： 第 1 步：设 `X = { a_{1},a_{2},a_{3},…,a_{n}}` 为预测数据，其中 \\(a_{i}\\) 是预测数据的特征值。 第 2 步：设 `Y = {y_{1},y_{2},y_{3},…,y_{m}}` 为类别集合。 第 3 步：计算 \\(P(y_{1}\\mid x)\\), \\(P(y_{2}\\mid x)\\), \\(P(y_{3}\\mid x)\\), \\(…\\), \\(P(y_{m}\\mid x)\\)。 第 4 步：寻找 \\(P(y_{1}\\mid x), P(y_{2}\\mid x), P(y_{3}\\mid x), …, P(y_{m}\\mid x)\\) 中最大的概率 \\(P(y_{k}\\mid x)\\) ，则 \\(x\\) 属于类别 \\(y_{k}\\)。 生成示例数据下面我们利用 python 完成一个朴素贝叶斯算法的分类。首先生成一组示例数据：由 A 和 B两个类别组成，每个类别包含 x,y两个特征值，其中 x 特征包含r,g,b（红，绿，蓝）三个类别，y特征包含s,m,l（小，中，大）三个类别，如同数据 \\(X = [g,l]\\)。 1234567891011\"\"\"生成示例数据\"\"\"import pandas as pddef create_data(): data = {\"x\": ['r', 'g', 'r', 'b', 'g', 'g', 'r', 'r', 'b', 'g', 'g', 'r', 'b', 'b', 'g'], \"y\": ['m', 's', 'l', 's', 'm', 's', 'm', 's', 'm', 'l', 'l', 's', 'm', 'm', 'l'], \"labels\": ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B']} data = pd.DataFrame(data, columns=[\"labels\", \"x\", \"y\"]) return data 在创建好数据后，接下来进行加载数据，并进行预览。 1234\"\"\"加载并预览数据\"\"\"data = create_data()data 参数估计根据朴素贝叶斯的原理，最终分类的决策因素是比较 \\({ P(类别 1 \\mid 特征),P(类别 2 \\mid 特征),…,P(类别 m \\mid 特征) }\\) 各个概率的大小，根据贝叶斯公式得知每一个概率计算的分母 \\(P(特征)\\) 都是相同的，只需要比较分子 \\(P(类别)\\) 和 \\(P(特征 \\mid 类别)\\) 乘积的大小。 那么如何得到 \\(P(类别)\\),以及 \\(P(特征\\mid 类别)\\)呢？在概率论中，可以应用极大似然估计法以及贝叶斯估计法来估计相应的概率。 极大似然估计什么是极大似然？下面通过一个简单的例子让你有一个形象的了解： 前提条件：假如有两个外形完全相同箱子，甲箱中有 99 个白球，1 个黑球；乙箱中有 99 个黑球，1 个白球。 问题：当我们进行一次实验，并取出一个球，取出的结果是白球。那么，请问白球是从哪一个箱子里取出的？ 我相信，你的第一印象很可能会是白球从甲箱中取出。因为甲箱中的白球数量多，所以这个推断符合人们经验。其中「最可能」就是「极大似然」。而极大似然估计的目的就是利用已知样本结果，反推最有可能造成这个结果的参数值。 极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：「模型已定，参数未知」。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 在概率论中求解极大似然估计的方法比较复杂，基于实验，我们将讲解 `P(B)` 和 \\(P(B/A)\\) 是如何通过极大似然估计得到的。\\(P(种类)\\) 用数学的方法表示 ： P(y_{i}=c_{k})=\\frac{\\sum_{N}^{i=1}I(y_{i}=c_{k})}{N},k=1,2,3,…,m \\tag{6}公式(6)中的 \\(y_{i}\\) 表示数据的类别，\\(c_{k}\\) 表示每一条数据的类别。 你可以通俗的理解为，在现有的训练集中，每一个类别所占总数的比例，例如:生成的数据中 \\(P(Y=A)=\\frac{8}{15}\\)，表示训练集中总共有 15 条数据，而类别为 A 的有 8 条数据。 下面我们用 Python 代码来实现先验概率 \\(P(种类)\\) 的求解： 1234567891011121314\"\"\"P(种类) 先验概率计算\"\"\"def get_P_labels(labels): labels = list(labels) # 转换为 list 类型 P_label = {} # 设置空字典用于存入 label 的概率 for label in labels: P_label[label] = labels.count(label) / float(len(labels)) # p = count(y) / count(Y) return P_labelP_labels = get_P_labels(data[\"labels\"])P_labels 1{'A': 0.5333333333333333, 'B': 0.4666666666666667} \\(P(特征 \\mid 种类)\\) 由于公式较为繁琐这里先不给出，直接用叙述的方式能更清晰地帮助理解： 实际需要求的先验估计是特征的每一个类别对应的每一个种类的概率，例如：生成数据 中 \\(P(x_{1}=”r” \\mid Y=A)=\\frac{4}{8}\\)， A 的数据有 8 条，而在种类为 A 的数据且特征 x 为 r的有 4 条。 同样我们用代码将先验概率 \\(P(特征 \\mid 种类)\\) 实现求解： 首先我们将特征按序号合并生成一个 numpy 类型的数组。 123456\"\"\"导入特征数据并预览\"\"\"import numpy as nptrain_data = np.array(data.iloc[:, 1:])train_data 123456789101112131415array([['r', 'm'], ['g', 's'], ['r', 'l'], ['b', 's'], ['g', 'm'], ['g', 's'], ['r', 'm'], ['r', 's'], ['b', 'm'], ['g', 'l'], ['g', 'l'], ['r', 's'], ['b', 'm'], ['b', 'm'], ['g', 'l']], dtype=object) 在寻找属于某一类的某一个特征时，我们采用对比索引的方式来完成。开始得到每一个类别的索引： 1234567891011121314\"\"\"类别 A,B 索引\"\"\"labels = data[\"labels\"]label_index = []for y in P_labels.keys(): temp_index = [] # enumerate 函数返回 Series 类型数的索引和值，其中 i 为索引，label 为值 for i, label in enumerate(labels): if (label == y): temp_index.append(i) else: pass label_index.append(temp_index)label_index 1[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14]] 得到 A 和 B 的索引，其中是A类别为前 `8` 条数据，B类别为后 `7` 条数据。 在得到类别的索引之后，接下来就是找到我们需要的特征为 r的索引值。 1234\"\"\"特征 x 为 r 的索引\"\"\"x_index = [i for i, feature in enumerate(train_data[:, 0]) if feature == 'r'] # 效果等同于求类别索引中 for 循环x_index 1[0, 2, 6, 7, 11] 得到的结果为 `x` 特征值为 `r` 的数据索引值。 最后通过对比类别为 A 的索引值，计算出既符合 x = r 又符合 A 类别的数据在 A 类别中所占比例。 1234x_label = set(x_index) &amp; set(label_index[0])print('既符合 x = r 又是 A 类别的索引值：', x_label)x_label_count = len(x_label)print('先验概率 P(r|A):', x_label_count / float(len(label_index[0]))) 12既符合 x = r 又是 A 类别的索引值： {0, 2, 6, 7}先验概率 P(r|A): 0.5 为了方便后面函数调用，我们将求 \\(P(特征\\mid 种类)\\) 代码整合为一个函数。 12345678910111213141516171819202122232425\"\"\"P(特征∣种类) 先验概率计算\"\"\"def get_P_fea_lab(P_label, features, data): P_fea_lab = {} train_data = data.iloc[:, 1:] train_data = np.array(train_data) labels = data[\"labels\"] for each_label in P_label.keys(): label_index = [i for i, label in enumerate( labels) if label == each_label] # labels 中出现 y 值的所有数值的下标索引 # features[0] 在 trainData[:,0] 中出现的值的所有下标索引 for j in range(len(features)): feature_index = [i for i, feature in enumerate( train_data[:, j]) if feature == features[j]] # set(x_index)&amp;set(y_index) 列出两个表相同的元素 fea_lab_count = len(set(feature_index) &amp; set(label_index)) key = str(features[j]) + '|' + str(each_label) P_fea_lab[key] = fea_lab_count / float(len(label_index)) return P_fea_labfeatures = ['r', 'm']get_P_fea_lab(P_labels, features, data) 1234{'r|A': 0.5, 'm|A': 0.375, 'r|B': 0.14285714285714285, 'm|B': 0.42857142857142855} 可以得到当特征 x 和 y 的值为 r 和 m 时，在不同类别下的先验概率。 贝叶斯估计在做极大似然估计时，若类别中缺少一些特征，则就会出现概率值为 0 的情况。此时，就会影响后验概率的计算结果，使得分类产生偏差。而解决这一问题最好的方法就是采用贝叶斯估计。 在计算先验概率 \\(P(种类)\\) 中，贝叶斯估计的数学表达式为： P(y_{i}=c_{k})=\\frac{\\sum_{N}^{i=1}I(y_{i}=c_{k})+\\lambda }{N+k\\lambda} \\tag{8}其中 \\(\\lambda \\geq 0\\) 等价于在随机变量各个取值的频数上赋予一个正数，当 \\(\\lambda=0\\) 时就是极大似然估计。在平时常取 \\(\\lambda=1\\)，这时称为拉普拉斯平滑。例如：生成数据 中，\\(P(Y=A)=\\frac{8+1}{15+2*1}=\\frac{9}{17}\\),取 \\(\\lambda=1\\) 此时由于一共有 A，B 两个类别，则 k 取 2。 同样计算 \\(P(特征 \\mid 种类)\\) 时，也是给计算时的分子分母加上拉普拉斯平滑。例如：生成数据 中，\\(P(x_{1}=”r” \\mid Y=A)=\\frac{4+1}{8+3*1}=\\frac{5}{11}\\) 同样取 \\(\\lambda=1\\) 此时由于 x 中有 r, g, b 三个种类，所以这里 k 取值为 3。 朴素贝叶斯算法实现通过上面的内容，相信你已经对朴素贝叶斯算法原理有一定印象。接下来，我们对朴素贝叶斯分类过程进行完整实现。其中，参数估计方法则使用极大似然估计。注：分类器实现的公式，请参考《机器学习》- 周志华 P151 页 123456789101112131415161718192021222324\"\"\"朴素贝叶斯分类器\"\"\"def classify(data, features): # 求 labels 中每个 label 的先验概率 labels = data['labels'] P_label = get_P_labels(labels) P_fea_lab = get_P_fea_lab(P_label, features, data) P = {} P_show = {} # 后验概率 for each_label in P_label: P[each_label] = P_label[each_label] for each_feature in features: key = str(each_label)+'|'+str(features) P_show[key] = P[each_label] * \\ P_fea_lab[str(each_feature) + '|' + str(each_label)] P[each_label] = P[each_label] * \\ P_fea_lab[str(each_feature) + '|' + str(each_label)] # 由于分母相同，只需要比较分子 print(P_show) features_label = max(P, key=P.get) # 概率最大值对应的类别 return features_label 1classify(data, ['r', 'm']) 12{\"A|['r', 'm']\": 0.1, \"B|['r', 'm']\": 0.02857142857142857}'A' 对于特征为 [r,m] 的数据通过朴素贝叶斯分类得到不同类别的概率值，经过比较后分为 A 类。 朴素贝叶斯的三种常见模型了解完朴素贝叶斯算法原理后，在实际数据中，我们可以依照特征的数据类型不同，在计算先验概率方面对朴素贝叶斯模型进行划分，并分为：多项式模型，伯努利模型和高斯模型。 多项式模型当特征值为离散时，常常使用多项式模型。事实上，在以上实验的参数估计中，我们所应用的就是多项式模型。为避免概率值为 0 的情况出现，多项式模型采用的是贝叶斯估计。 伯努利模型与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是 1 和 0（以文本分类为例，某个单词在文档中出现过，则其特征值为 1，否则为 0）。 在伯努利模型中，条件概率 \\(P(x_{i} \\mid y_{k})\\) 的计算方式为： 当特征值 \\(x_{i}=1\\) 时，\\(P(x_{i} \\mid y_{k})=P(x_{i}=1 \\mid y_{k})\\); 当特征值 \\(x_{i}=0\\) 时，\\(P(x_{i} \\mid y_{k})=P(x_{i}=0 \\mid y_{k})\\)。 高斯模型当特征是连续变量的时候，在不做平滑的情况下，运用多项式模型就会导致很多 \\(P(x_{i} \\mid y_{k})=0\\)，此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，采用高斯模型。高斯模型是假设连续变量的特征数据是服从高斯分布的，高斯分布函数表达式为： P(x_{i}|y_{k})=\\frac{1}{\\sqrt{2\\pi}\\sigma_{y_{k},i}}exp(-\\frac{(x-\\mu_{y_{k},i}) ^{2}}{2\\sigma ^{2}_{y_{k}},i})其中： \\(\\mu_{y_{k},i}\\) 表示类别为 \\(y_{k}\\) 的样本中，第 `i` 维特征的均值。 \\(\\sigma ^{2}_{y_{k}},i\\) 表示类别为 \\(y_{k}\\) 的样本中，第 `i` 维特征的方差。 高斯分布示意图如下： 关于贝叶斯定理，这里有一个有趣的视频，希望能加深大家对该定理的理解。 如何用贝叶斯方法帮助内容审核 | 视频来源：[回形针PaperClip](https://weibo.com/u/6414205745?is_all=1)","link":"/2019/01/19/naive_bayes_basic/"},{"title":"机器学习|装袋和提升方法详解 (Python 语言描述)","text":"前面的文章都是独立的讲解每一个分类器的分类过程，每一个分类器都有其独有的特点并非常适合某些数据。但在实际中，可能没有那样合适的数据，在应用前面的分类器时，可能会出现分类准确率低的问题。为解决这样的问题，集成学习便被提出，利用多个弱分类器结合的方式使得分类准确率提高。本次详细讲解了集成学习中十分经典的几个算法：装袋（Bagging）中的 Bagging tree 和随机森林（Ramdom Forest）以及 提升（Boosting）中的 Adaboost 和梯度提升树（GBDT）。 集成学习集成学习概念在学习装袋和提升算法之前，先引入一个概念：集成学习。集成学习，顾名思义就是通过构建多个分类器并结合使用来完成学习任务，同时也被称为多分类器系统。其最大的特点就是结合各个弱分类器的长处，从而达到“三个臭皮匠顶个诸葛亮”的效果。 每一个弱分类器我们将其称作「个体学习器」，集成学习的基本结构就是生成一组个体学习器，再用某种策略将他们结合起来。 从个体学习器类别来看，集成学习通常分为两种类型： 「同质」集成，在一个集成学习中，「个体学习器」是同一类型，如 「决策树集成」 所有个体学习器都为决策树，「神经网络集成」所有的个体学习器都为神经网络。 「异质」集成，在一个集成学习中，「个体学习器」为不同类型，如一个集成学习中可以包含决策树模型也可以包含神经网络模型。 同样从集成方式来看，集成学习也可以分为两类： 并行式，当个体学习器之间不存在强依赖关系时，可同时生成并行化方法，其中代表算法为装袋（Bagging）算法。 串行式，当个体学习器之间存在强依赖关系时，必须串行生成序列化方法，其中代表算法为提升（Boosting）算法。 结合策略集成学习中，当数据被多个个体学习器学习后，如何最终决定学习结果呢？假定集成包含 `T` 个个体学习器 \\(\\{ h_{1},h_{2},…,h_{T}\\}\\) 。常用的有三种方法：平均法，投票法，学习法。 平均法在数值型输出中，最常用的结合策略为平均法（Averaging），在平均法中有两种方式： 简单平均法： H(x)=\\frac{1}{T}\\sum_{i=1}^{T}h_{i}(x) \\tag{1}取每一个「个体学习器」学习后的平均值。 加权平均法： H(x)=\\sum_{i=1}^{T}w_{i}h_{i}(x) \\tag{2}其中 `w_{i}` 是每一个「个体学习器」 `h_{i}` 的权重，通常为 \\(w_{i}\\geq 0, \\sum^{T}_{i=1}w_{i}=1\\)。 投票法对于分类输出而言，平均法显然效果不太好，最常用的结合策略为投票法(Voting)，在投票法中主要有三种方式： 绝对多数投票法： H(X)=\\left\\{\\begin{matrix}c_{j}, if \\sum_{i=1}^{T}h_{i}^{j}> 0.5\\sum_{k=1}^{N}\\sum_{i=1}^{T}h_{i}^{k}(x); \\\\\\ None, if \\sum_{i=1}^{T}h_{i}^{j} < = 0.5\\sum_{k=1}^{N}\\sum_{i=1}^{T}h_{i}^{k}(x); \\end{matrix}\\right. \\tag{3}简单而言，当某一个输出的分类超过了半数则输出该分类，若未超过半数则不输出分类。 相对多数投票法： H(X)=c_{\\underset{j}{argmax}\\sum_{i=1}^{T}h_{i}^{j}(x)} \\tag{4}即在「个体学习器」分类完成后，通过投票选出分类最多的标签作为此次分类的结果。 加权投票法： H(X)=c_{\\underset{j}{argmax}\\sum_{i=1}^{T}w_{i}h_{i}^{j}(x)} \\tag{5}同加权平均法类似，`w_{i}` 是每一个「个体学习器」 `h_{i}` 的权重，通常为 \\(w_{i}\\geq 0, \\sum^{T}_{i=1}w_{i}=1\\)。 学习法以上两种方法（平均法和投票法）相对比较简单，但是可能学习误差较大，为了解决这种情况，还有一种方法为学习法，其代表方法是 stacking ，当使用stacking 的结合策略时， 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，即把训练集弱学习器的学习结果作为输入，把训练集的输出作为输出，重新训练一个学习器来得到最终结果。 在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。 装袋算法 Bagging在大致了解集成学习相关概念之后，接下来就是对集成学习中常用算法思想之一的装袋算法进行详细的讲解。 装袋算法原理装袋算法是并行式集成学习的代表，其原理也比较简单。算法步骤如下： 数据处理：将数据根据实际情况进行清洗整理。 随机采样：重复 T 次，每一次从样本中随机选出 T 个子样本。 个体训练：将每一个子样本放入个体学习器训练。 分类决策：用投票法集成进行分类决策。 Bagging tree在前一节的决策树讲解中提到，决策树是一个十分「完美」的训练器，但特别容易出现过拟合的情况，最终导致预测准确率低的问题。事实上在装袋算法中，决策树常常被用作弱分类器。下面我们通过具体实验来看看决策树和以决策树作为装袋算法的预测效果。 数据加载本实验我们使用在上一章讲决策树时所用的学生成绩预测数据集。其中数据处理已在上一章详细说明，本次实验我们使用处理过后的数据集。数据集名称为 course-14-student.csv. 数据集下载 👉 传送门 1234import pandas as pdstu_data = pd.read_csv(\"course-14-student.csv\")stu_data.head(10) 数据划分加载好预处理的数据集之后，为了应用装袋算法，我们需要将数据集分为 训练集和测试集，依照经验：训练集占比为 70%，测试集占 30%。 12345from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(stu_data.iloc[:,:-1], stu_data[\"G3\"], test_size=0.3, random_state=35)x_test.head(10) 决策树预测作为比较，首先我们将该数据集用决策树的方式进行预测,使用 scikit-learn 实现决策树预测的用法在前一章节已详细介绍，本实验直接使用。 123456from sklearn.tree import DecisionTreeClassifierdt_model = DecisionTreeClassifier(criterion='entropy', random_state=34)dt_model.fit(x_train, y_train) # 使用训练集训练模型dt_y_predict = dt_model.predict(x_test)dt_y_predict array([2, 0, 3, 2, 1, 2, 2, 2, 3, 3, 0, 2, 1, 3, 3, 2, 3, 0, 1, 2, 1, 2, 1, 2, 3, 3, 3, 0, 3, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 2, 1, 3, 2, 3, 3, 3, 3, 1, 1, 3, 2, 0, 1, 3, 2, 3, 3, 0, 0, 2, 2, 3, 3, 3, 2, 1, 0, 3, 2, 2, 3, 2, 1, 3, 0, 2, 3, 1, 3, 3, 0, 3, 3, 1, 3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 2, 1, 1, 1, 3, 0, 3, 3, 3], dtype=int64) 计算使用决策树预测的准确率。 12345678910111213141516171819import numpy as npdef get_accuracy(test_labels, pred_labels): \"\"\" 参数: test_labels -- 测试集的真实值 pred_labels -- 测试集的预测值 返回: accur -- 准确率 \"\"\" correct = np.sum(test_labels == pred_labels) # 计算预测正确的数据个数 n = len(test_labels) # 总测试集数据个数 accur = correct/n return accurget_accuracy(y_test, dt_y_predict) 0.7899159663865546 由于本次实验所采用的数据集特征值比前一章节多，所以决策树泛化能力更差。 Bagging Tree 数据模型构建单棵决策树的预测结果并不能使我们满意，下面我们使用 装袋（Bagging） 的思想来提高预测准确率。我们通过 scikit-learn 来对 Bagging Tree 算法进行实现。 在 scikit-learn 中 Bagging tree 常用参数如下： 1BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0) 其中： base_estimator：表示基础分类器（弱分类器）种类，默认为决策树 。 n_estimators：表示建立树的个数，默认值为 10 。 max_samples：表示从抽取数据中选取训练样本的数量，int（整型）表示数量，float（浮点型）表示比例，默认为所有样本。 max_features：表示抽取特征的数量，int（整型）表示数量，float（浮点型）表示比例，默认为所有特征。 常用方法: fit(x,y)：训练。 predict(X)：对数据集进行预测返回预测结果。 123456789from sklearn.ensemble import BaggingClassifiertree = DecisionTreeClassifier(criterion='entropy', random_state=28)bag = BaggingClassifier(tree, n_estimators=100, max_samples=1.0, random_state=3) # 使用决策树bag.fit(x_train, y_train)bt_y_predict = bag.predict(x_test)bt_y_predict array([3, 0, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2, 1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 0, 2, 1, 3, 3, 1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 0, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3, 2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 3, 1, 3, 3, 0, 3, 3, 1, 3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 0, 3, 3, 3], dtype=int64) 准确率计算1get_accuracy(y_test,bt_y_predict) 0.8991596638655462 根据准确率可以看到在决策树通过装袋（Bagging）算法后预测准确率有明显提升。 随机森林 Random Forest其实，Bagging tree 算法，是应用子数据集中的所有特征构建一棵完整的树，最终通过投票的方式进行预测。而随机森林就是在 Bagging tree 算法的基础上进行进一步的改进。 随机森林的思想就是将一个大的数据集使用自助采样法进行处理，即从原样本数据集中随机抽取多个子样本集，并基于每一个子样本集生成相应的决策树。这样，就可以构建出由许多小决策树组形成的决策树「森林」。最后，实验通过投票法选择决策树最多的预测结果作为最终的输出。 所以，随机森林的名称来源就是「随机抽样 + 决策树森林」。 随机森林算法原理随机森林作为装袋（Bagging）的代表算法，算法原理和装袋十分相似，但在此基础上做了一些改进： 对于普通的决策树，会在 N 个样本的所有特征中选择一个最优划分特征，但是随机森林首先会从所有特征中随机选择部分特征，再从该部分特征中选择一个最优划分特征。这样进一步增强了模型的泛化能力。 在决定部分特征个数时，通过交叉验证的方式来获取一个合适的值。 随机森林算法流程： 从样本集中有放回随机采样选出 n 个样本； 从所有特征中随机选择 k 个特征，对选出的样本利用这些特征建立决策树； 重复以上两步 m 次，即生成 m 棵决策树，形成随机森林； 对于新数据，经过每棵树决策，最后投票确认分到哪一类。 模型构建和数据预测在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 scikit-learn 来对其进行实现。 在 scikit-learn 随机森林常用参数如下： 1RandomForestClassifier(n_estimators,criterion,max_features,random_state=None) 其中： n_estimators：表示建立树的个数，默认值为 10 。 criterion：表示特征划分方法选择，默认为 gini，可选择为 entropy (信息增益)。 max_features：表示随机选择特征个数，默认为特征数的根号。 常用方法: fit(x,y)：训练随机森林。 predict(X)：对数据集进行预测返回预测结果。 12345678from sklearn.ensemble import RandomForestClassifier# 这里构建 100 棵决策树，采用信息熵来寻找最优划分特征。rf = RandomForestClassifier( n_estimators=100, max_features=None, criterion='entropy')rf.fit(x_train, y_train) # 进行模型的训练rf_y_predict = rf.predict(x_test)rf_y_predict array([3, 0, 3, 2, 1, 2, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2, 1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 3, 1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 2, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3, 2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 0, 3, 3, 1, 3, 3, 1, 3, 3, 3, 2, 0, 2, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 0, 3, 3, 3], dtype=int64) 准确率计算当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。 1get_accuracy(y_test, rf_y_predict) 0.8907563025210085 可以通过结果看到，本次实验的数据集用随机森林预测的准确率和用 Bagging tree 预测的准确率差别不大，但随着数据集的增大和特征数的增多，随机森林的优势就会慢慢显现出来。 提升算法 Boosting当「个体学习器」之间存在较强的依赖时，采用装袋的算法便有些不合适，此时最好的方法就是使用串行集成方式：提升（Boosting）。 提升算法原理提升算法是可以将弱学习器提升为强学习器的算法，其具体思想是从初始训练集训练出一个「个体学习器」，再根据个体学习器的表现对训练样本分布进行调整，使得在个体学习器中判断错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个「个体学习器」。如此重复进行，直至个体学习器数目达到事先指定的值 T，最终将这 T 个「个体学习器」输出的值进行加权结合得到最终的输出值。 Adaboost提升（Boosting）算法中最具代表性的算法为 Adaboost。 AdaBoost（Adaptive Boosting）名为自适应增强，其主要自适应增强表现在：上一个「个体学习器」中被错误分类的样本的权值会增大，正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。 AdaBoost 原理AdaBoost 算法与 Boosting 算法不同的是，其不需要预先知道弱分类器的误差，并且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度。 Adaboost 算法流程： 数据准备：通过数据清理和数据整理的方式得到符合规范的数据。 初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1/N。 弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。 更改权重：如果某个样本点被准确地分类，降低其权值；若被分类错误，那么提高其权值。然后，权值更新过的样本集被用于训练下一个分类器。 强分类器组合：重复 3，4 步骤，直至训练结束，加大分类误差率小的弱分类器的权重（这里的权重和样本权重不一样），使其在最终的分类函数中起着较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用，最终输出结果。 模型构建和数据预测在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 scikit-learn 来对其进行实现。 在 scikit-learn Adaboost 常用参数如下： 1AdaBoostClassifier(base_estimators,n_estimators) 其中： base_estimators：表示弱分类器种类，默认为 CART 分类树。 n_estimators：表示弱学习器的最大个数，默认值为 50。 常用方法: fit(x,y)：训练弱分类器。 predict(X)：对数据集进行预测返回预测结果。 123456from sklearn.ensemble import AdaBoostClassifierad = AdaBoostClassifier(n_estimators=100)ad.fit(x_train, y_train)ad_y_predict = ad.predict(x_test)ad_y_predict array([3, 3, 3, 2, 0, 3, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 3, 1, 3, 3, 3, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 3, 1, 3, 3, 1, 3, 2, 3, 2, 3, 3, 3, 1, 2, 3, 3, 1, 3, 2, 2, 3, 2, 0, 2, 2, 3, 2, 3, 3, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 2, 0, 2, 3, 0, 3, 1, 3, 1, 0, 3, 3, 3, 3, 2, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3], dtype=int64) 准确率计算当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。 1get_accuracy(y_test, ad_y_predict) 0.7983193277310925 通过结果可以看到，应用 Adaboost 算法得到的准确率和决策树相差不大，说明在使用 Adaboost 算法时预测效果不好。 梯度提升树 GBDT梯度提升树（Gradient Boosting Decison Tree，GBDT）同样是 Boosting 算法家族中的一员， Adaboost 是利用前一轮迭代弱学习器的误差率来更新训练集的权重，而梯度提升树所采用的是前向分布算法，且弱学习器限定了只能使用CART树模型。 梯度提升树算法原理在 GBDT 的迭代中，假设我们前一轮迭代得到的强学习器是 \\(f_{t-1}(x)\\), 损失函数是 \\(L(y,f_{t-1}(x))\\), 我们本轮迭代的目标是找到一个 CART 回归树模型的弱学习器 \\(h_{t}(x)\\)，让本轮的损失 \\(L(y,f_{t}(x) = L(y , f_{t−1}(x)+h_{t}(x))\\) 最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 算法流程： 数据准备：通过数据清理和数据整理的方式得到符合规范的数据。 初始化权重：如果有 N 个训练样本数据，在最开始时每一个数据被赋予相同的权值：1/N。 弱分类器预测：将有权重的训练样本放入弱分类器进行分类预测。 CART 树拟合：计算每一个子样本的梯度值，通过梯度值和子样本拟合一棵 CART 树 更新强学习器：在拟合好的 CART树中通过损失函数计算出最佳的拟合值，更新先前组成的强学习器。 强分类器组合：重复 3，4，5 步骤，直至训练结束，得到一个强分类器，最终输出结果。 梯度提升树模型构建及预测在划分好数据集之后，接下来就是进行模型的构建以及预测。下面我们通过 scikit-learn 来对其进行实现。 在 scikit-learn GBDT 常用参数如下： 1GradientBoostingClassifier(max_depth = 3，learning_rate = 0.1, n_estimators = 100，random_state = None) 其中： max_depth:表示生成 CART 树的最大深度，默认为 3 learning_rate:表示学习效率，默认为 0.1。 n_estimators：表示弱学习器的最大个数，默认值为 100。 random_state:表示随机数种子。 常用方法: fit(x,y)：训练弱分类器。 predict(X)：对数据集进行预测返回预测结果。 1234567from sklearn.ensemble import GradientBoostingClassifierclf = GradientBoostingClassifier( n_estimators=100, learning_rate=1.0, random_state=33)clf.fit(x_train, y_train)gt_y_predict = clf.predict(x_test)gt_y_predict array([3, 0, 3, 2, 1, 3, 3, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 0, 1, 2, 1, 2, 1, 3, 3, 2, 3, 0, 2, 3, 3, 3, 2, 2, 3, 3, 0, 1, 2, 2, 2, 1, 3, 3, 1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3, 2, 3, 3, 2, 0, 2, 2, 3, 2, 3, 2, 3, 0, 3, 2, 2, 3, 2, 1, 2, 0, 2, 2, 1, 3, 3, 0, 3, 3, 1, 3, 3, 1, 3, 3, 3, 2, 0, 0, 3, 0, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 2, 3, 3, 3], dtype=int64) 准确率计算1get_accuracy(y_test, gt_y_predict) 0.8739495798319328 可以看到，在使用装袋和提升算法时，在大部分情况下，会产生更好的预测结果，但有时也可能出现没有优化的情况。事实上，机器学习分类器的选择就是如此，没有最好的分类器只有最适合的分类器，不同的数据集，由于其数据特点的不同，在不同的分类器中表现也不同。 拓展阅读： 随机森林 - 维基百科 Bootstrap aggregating - 维基百科","link":"/2019/02/07/Bagging_and_Boosting/"},{"title":"回归预测|梯度下降详解","text":"在了解梯度下降（Gradient Descent）之前，我们先要知道有关线性回归的基本知识，这样可以进一步的加深对梯度下降的理解，当然梯度下降（Gradient Descent）并不单单只能进行回归预测，它还可以进行诸如分类等操作。 关于线性回归的具体讲解本文不详细涉及，只简单列出几个相关公式。(关于线性回归可以看这篇 👉传送门) 线性回归公式 4-1：线性回归模型预测 \\hat{y} = \\theta_0 + \\theta_1x_{1} + \\theta_2x_{2} + ... + \\theta_nx_{n} \\(\\hat{y}\\) 是预测值 `n` 是特征的数量 `x_{i}` 是 `i` 个特征值 \\(\\theta_j\\) 是第 `j` 个模型参数 (包括偏置项 \\(\\theta_0\\) 以及特征权重 \\(\\theta_1, \\theta_2, …, \\theta_n\\) 也可以用更为简洁的向量化形式表达 公式 4-2：线性回归模型预测 (向量化) \\hat{y} = h_{\\theta}(X) = \\theta^T \\cdot X \\(\\theta\\) 是模型的参数向量，包括偏置项 \\(\\theta_0\\) 以及特征权重 \\(\\theta_1\\) 到 \\(\\theta_n\\) \\(\\theta^T\\) 是 \\(\\theta\\) 的转置向量 (为行向量，而不再是列向量) `X` 是实例的特征向量，包括从 \\(\\theta_0\\) 到 \\(\\theta_n, \\theta_0\\) 永远为 `1` \\(\\theta^T \\cdot X\\) 是 \\(\\theta^T\\) 和 `X` 的点积 \\(h_{\\theta}\\) 是模型参数 \\(\\theta\\) 的假设函数 公式 4-3：线性回归模型的 `MSE` 成本函数 MSE(X, h_{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m} (\\theta^T \\cdot X^{(i)} - y^{(i)})^2标准方程为了得到使成本函数最小的 \\(\\theta\\) 值，有一个闭式解方法——也就是一个直接得出结果的数学方程，即标准方程。 公式 4-4：标准方程 MSE(X, h_{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m} (\\theta^T \\cdot X^{(i)} - y^{(i)})^2 \\(\\hat{\\theta}\\) 是使成本函数最小的 \\(\\theta\\) 值 `y` 是包含 \\(y^{(1)}\\) 到 \\(y^{(m)}\\) 的目标值量 我们生成一些线性数据来测试这个公式： 1234import numpy as npX = 2 * np.random.rand(100, 1)y = 4 + 3 * X + np.random.randn(100, 1) 1234567import matplotlib.pyplot as plt%matplotlib inline# 可视化fig, ax = plt.subplots(figsize=(12,8))ax.plot(X, y, \"b.\")plt.show() 现在我们使用标准方程来计算 \\(\\hat{\\theta}\\)。使用 Numpy 的线性代数模块 (np.linalg) 中的 inv() 函数来对矩阵求逆，并用 dot() 方法计算矩阵的内积： 12X_b = np.c_[np.ones((100, 1)), X] # add xo = 1 to each instancetheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) 我们实际用来生成数据的函数是 \\(y = 4 + 3x_0 + 高斯噪声\\)。 1theta_best array([[4.0939709 ], [3.08934507]]) 我们期待的是 \\(\\theta_0 = 4, \\theta_1 = 3\\) 得到的是 \\(\\theta_0 = 4.0939709, \\theta_1 = 3.08934507\\)。非常接近了，因为噪声的存在使其不可能完全还原为原本的函数。 现在可以用 \\(\\hat{\\theta}\\) 做出预测： 1234X_new = np.array([[0], [2]])X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instancey_predict = X_new_b.dot(theta_best)y_predict array([[ 4.0939709 ], [10.27266104]]) 12345# 绘制模型的预测结果fig, ax = plt.subplots(figsize=(12,8))ax.plot(X_new, y_predict, \"r-\")ax.plot(X, y, \"b.\")plt.show() 12345# Scikit-Learn 的等效代码from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(X, y) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) 1lin_reg.intercept_, lin_reg.coef_ (array([4.0939709]), array([[3.08934507]])) 1lin_reg.predict(X_new) array([[ 4.0939709 ], [10.27266104]]) 梯度下降梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。 假设你迷失在山上的浓雾之中，你能感觉到的只有你脚下路面的坡度。快速到达山脚的一个策略就是沿着最陡的方向下坡。这就是梯度下降的做法：通过测量参数向量 \\(\\theta\\) 相关的误差函数的局部梯度，并不断沿着降低梯度的方向调整，直到梯度降为0，到达最小值！ 具体来说，首先使用一个随机的 \\(\\theta\\) 值（这被称为随机初始化）， 然后逐步改进，每次踏出一步，每一步都尝试降低一点成本函数（如 `MSE` ），直到算法收敛出一个最小值（参见图4-3） 梯度下降中一个重要参数是每一步的步长，这取决于超参数学习率。如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间（参见图4-4）。 反过来说，如果学习率太高，那你可能会越过山谷直接到达山的另一边，甚至有可能比之前的起点还要高。这会导致算法发散，值越来越大，最后无法找到好的解决方案（参见图4-5）。 最后，并不是所有的成本函数看起来都像一个漂亮的碗。有的可能看着像洞、像山脉、像高原或者是各种不规则的地形，导致很难收敛到最小值。图4-6显示了梯度下降的两个主要挑战：如果随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值。如果算法从右侧起步，那么需要经过很长时间才能越过整片高原，如果你停下得太早，将永远达不到全局最小值。 幸好，线性回归模型的 `MSE` 成本函数恰好是个凸函数，这意味着连接曲线上任意两个点的线段永远不会跟曲线相交。也就是说不存在局部最小，只有一个全局最小值。它同时也是一个连续函数，所以斜率不会产生陡峭的变化。这两件事保证的结论是：即便是乱走，梯度下降都可以趋近到全局最小值（只要等待时间足够长，学习率也不是太高）。 成本函数虽然是碗状的，但如果不同特征的尺寸差别巨大，那它可能是一个非常细长的碗。如图4-7所示的梯度下降，左边的训练集上特征1和特征2具有相同的数值规模，而右边的训练集上，特征1的值则比特征2要小得多。因为特征1的值较小，所以 \\(\\theta_1\\) 需要更大 的变化来影响成本函数，这就是为什么碗形会沿着 \\(\\theta_1\\) 轴拉长。） 正如你所见，左图的梯度下降算法直接走向最小值，可以快速到达。而在右图中，先是沿着与全局最小值方向近乎垂直的方向前进， 接下来是一段几乎平坦的长长的山谷。最终还是会抵达最小值，但是这需要花费大量的时间。 注意： 应用梯度下降时，需要保证所有特征值的大小比例都差不多 （比如使用Scikit-Learn的StandardScaler类），否则收敛的时间会长很多。 这张图也说明，训练模型也就是搜寻使成本函数（在训练集上）最小化的参数组合。这是模型参数空间层面上的搜索：模型的参数越多，这个空间的维度就越多，搜索就越难。同样是在干草堆里寻找一根针，在一个三百维的空间里就比在一个三维空间里要棘手得多。幸运的是，线性回归模型的成本函数是凸函数，针就躺在碗底。 批量梯度下降要实现梯度下降，你需要计算每个模型关于参数 \\(\\theta_j\\) 的成本函数的梯度。换言之，你需要计算的是如果改变 \\(\\theta_j\\) ，成本函数会改变多少。 这被称为偏导数。这就好比是在问 “如果我面向东，我脚下的坡度斜率是多少？” 然后面向北问同样的问题（如果你想象超过三个维度的宇宙，对于其他的维度以此类推）。公式4-5计算了关于参数 \\(\\theta_j\\) 的成本函数的偏导数，计作 \\(\\frac{\\partial}{\\partial \\theta_j}MSE(\\theta)\\) 公式 4-5 ：成本函数的偏导数 \\frac{\\partial}{\\partial \\theta_j}MSE(\\theta) = \\frac{2}{m}\\sum_{i=1}^{m} (\\theta^T \\cdot X^{(i)} - y^{(i)})x_j^{(i)}如果不想单独计算这些梯度，可以使用公式4-6对其进行一次性计算。梯度向量，记作 \\(\\nabla_\\theta MSE(\\theta)\\) ，包含所有成本函数（每个模型参数一个）的偏导数。 公式 4-6 ：成本函数的梯度向量 \\nabla_\\theta MSE(\\theta) = \\left[ \\begin{matrix} \\frac{\\partial}{\\partial \\theta_0}MSE(\\theta) \\\\ \\frac{\\partial}{\\partial \\theta_1}MSE(\\theta) \\\\ ... \\\\ \\frac{\\partial}{\\partial \\theta_n}MSE(\\theta) \\end{matrix} \\right] = \\frac{2}{m}X^T \\cdot (X \\cdot \\theta - y) 注意： 公式4-6在计算梯度下降的每一步时，都是基于完整的训练集 `X` 的。这就是为什么该算法会被称为批量梯度下降：每一步都使用整批训练数据。因此，面对非常庞大的训练集时，算法会变得极慢（不过我们即将看到快得多的梯度下降算法）。但是，梯度下降算法随特征数量扩展的表现比较好：如果要训练的线性模型拥有几十万个特征，使用梯度下降比标准方程要快得多。 一旦有了梯度向量，哪个点向上，就朝反方向下坡。也就是从 \\(\\theta\\) 中减去 \\(\\nabla_\\theta MSE(\\theta)\\) 。这时学习率 \\(\\eta\\) 就发挥作用了：用梯度向量乘以 \\(\\eta\\) 确定下坡步长的大小（公式4-7）。 公式 4-6 ：梯度下降步长 \\theta^{(next \\ step)} = \\theta - \\eta\\nabla_\\theta MSE(\\theta)123456789eta = 0.1 # learning rate n_iterations = 1000 m = 100theta = np.random.randn(2,1) # random initializationfor iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients 1theta array([[4.0939709 ], [3.08934507]]) 这不正是标准方程的发现么！梯度下降表现完美。如果使用了其他的学习率 \\(\\eta\\) 呢？图4-8展现了分别使用三种不同的学习率时， 梯度下降的前十步（虚线表示起点）。 左图的学习率太低：算法最终还是能找到解决方法，就是需要太长时间。中间的学习率看起来非常棒：几次迭代就收敛出了最终解。 而右边的学习率太高：算法发散，直接跳过了数据区域，并且每一步都离实际解决方案越来越远。 要找到合适的学习率，可以使用网格搜索。但是你可能需要限制迭代次数，这样网格搜索可以淘汰掉那些收敛耗时太长的模型。 你可能会问，要怎么限制迭代次数呢？如果设置太低，算法可能在离最优解还很远时就停了；但是如果设置得太高，模型达到最优解后，继续迭代参数不再变化，又会浪费时间。一个简单的办法是，在 开始时设置一个非常大的迭代次数，但是当梯度向量的值变得很微小时中断算法——也就是当它的范数变得低于 \\(\\varepsilon\\)（称为容差）时，因为这时梯度下降已经（几乎）到达了最小值。 收敛率 成本函数为凸函数，并且斜率没有陡峭的变化时（如 `MSE` 成本函数），通过批量梯度下降可以看出一个固定的学习率有一个收敛率，为 \\(0(\\dfrac{1}{迭代次数})\\)。换句话说，如果将容差 \\(\\varepsilon\\) 缩小为原来的 \\(\\dfrac{1}{10}\\)（以得到更精确的解），算法将不得不运行10倍的迭代次数 随机梯度下降批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度，所以训练集很大时，算法会特别慢。与之相反的极端是随机梯度下降，每一步在训练集中随机选择一个实例，并且仅基于该单个实例来计算梯度。显然，这让算法变得快多了，因为每个迭代都只需要操作少量的数据。它也可以被用来训练海量的数据集，因为每次迭代只需要在内存中运行一个实例即可（`SGD` 可以作为核外算法实现）。 另一方面，由于算法的随机性质，它比批量梯度下降要不规则得多。成本函数将不再是缓缓降低直到抵达最小值，而是不断上上下下，但是从整体来看，还是在慢慢下降。随着时间推移，最终会非常接近最小值，但是即使它到达了最小值，依旧还会持续反弹，永远不会停止（见图4-9）。所以算法停下来的参数值肯定是足够好的，但不是最优的。 当成本函数非常不规则时（见图4-6），随机梯度下降其实可以帮助算法跳出局部最小值，所以相比批量梯度下降，它对找到全局最小值更有优势。 因此，随机性的好处在于可以逃离局部最优，但缺点是永远定位不出最小值。要解决这个困境，有一个办法是逐步降低学习率。开始的步长比较大（这有助于快速进展和逃离局部最小值），然后越来越小，让算法尽量靠近全局最小值。这个过程叫作模拟退火，因为它类似于冶金时熔化的金属慢慢冷却的退火过程。确定每个迭代学习率的函数叫作学习计划。如果学习率降得太快，可能会陷入局部最小值， 甚至是停留在走向最小值的半途中。如果学习率降得太慢，你需要太长时间才能跳到差不多最小值附近，如果提早结束训练，可能只得到一个次优的解决方案。 12345678910111213141516n_epochs = 50 t0, t1 = 5, 50 # learning schedule hyperparametersdef learning_schedule(t): return t0 / (t + t1)theta = np.random.randn(2,1) # random initializationfor epoch in range(n_epochs): for i in range(m): random_index = np.random.randint(m) xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2 * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(epoch * m + i) theta = theta - eta * gradients 按照惯例，我们用 `m` 来表示迭代次数，每一次迭代称为一轮。前面的批量梯度下降需要在整个训练集上迭代 `1000`次，而这段代码只迭代了 `50` 次就得到了一个相当不错的解： 1theta array([[4.11135275], [3.06756448]]) 图 4-10 显示了训练过程的前 10 步 (注意不规则的步子) 因为实例是随机挑选，所以在同一轮里某些实例可能被挑选多次，而有些实例则完全没被选到。如果你希望每一轮算法都能遍历每个实例，有一种办法是将训练集洗牌打乱，然后一个接一个的使用实例，用完再重新洗牌，以此继续。不过这种方法通常收敛得更慢。 在 Scikit-Learn 里，用 `SGD` 执行线性回归可以使用 SGDRegressor 类，其默认优化的成本函数是平方误差。下面这段代码从学习率 0.1 开始（eta0=0.1），使用默认的学习计划（跟前面的学习计划不同） 运行了50 轮，而且没有使用任何正则化（penalty=None）： 123456import warningswarnings.filterwarnings('ignore')from sklearn.linear_model import SGDRegressor sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1) sgd_reg.fit(X, y.ravel()) SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling', loss='squared_loss', max_iter=None, n_iter=50, n_iter_no_change=5, penalty=None, power_t=0.25, random_state=None, shuffle=True, tol=None, validation_fraction=0.1, verbose=0, warm_start=False) 你再次得到了一个跟标准方程的解非常相近的解决方案： 1sgd_reg.intercept_, sgd_reg.coef_ (array([4.08805401]), array([3.08242337])) 小批量梯度下降我们要了解的最后一个梯度下降算法叫作小批量梯度下降。一旦理解了批量梯度下降和随机梯度下降，这个算法就非常容易理解了： 每一步的梯度计算，既不是基于整个训练集（如批量梯度下降）也不是基于单个实例（如随机梯度下降），而是基于一小部分随机的实例集也就是小批量。相比随机梯度下降，小批量梯度下降的主要优势在于可以从矩阵运算的硬件优化中获得显著的性能提升，特别是需要用到图形处理器时。 这个算法在参数空间层面的前进过程也不像 `SGD` 那样不稳定，特别是批量较大时。所以小批量梯度下降最终会比 `SGD` 更接近最小值一 些。但是另一方面，它可能更难从局部最小值中逃脱（不是我们前面看到的线性回归问题，而是对于那些深受局部最小值陷阱困扰的问题）。图 4-11 显示了三种梯度下降算法在训练过程中参数空间里的行进路线。它们最终都汇聚在最小值附近，批量梯度下降最终停在了最小值上，而随机梯度下降和小批量梯度下降还在继续游走。但是，别忘了批量梯度可是花费了大量时间来计算每一步的，如果用好了学习计划，随机梯度下降和小批量梯度下降也同样能到达最小值。 最后，我们来比较一下到目前为止所讨论过的线性回归算法 (`m` 是训练实例的数量，`n` 是特征数量)。 参考： Hands-On Machine Learning with Scikit-Learn and TensorFlow","link":"/2019/03/12/Gradient_Descent_Plus/"},{"title":"机器学习| 支持向量机详解 (Python 语言描述)","text":"在前面的文章中，我们对线性分布和非线性分布的数据处理方法进行了简单的介绍和实际的实验操作，当前还有一种机器学习方法，它在解决小样本、非线性及高维模式识别中都表现出了许多独特的优势，在样本量较小的情况，其实际运用效果甚至超过了神经网络，并且其不仅可以应用于线性分布数据，还可以用于非线性分布数据，相比于其他基本机器学习分类算法如逻辑回归、KNN、朴素贝叶斯等，其最终效果的表现一般都会优于这些方法。 线性分类支持向量机在逻辑回归中，我们尝试通过一条直线针对线性可分数据完成分类。同时，通过最小化对数损失函数来找到最优分割边界，也就是下图中的紫色直线。 逻辑回归是一种简单高效的线性分类方法。而在这里中，我们将接触到另一种针对线性可分数据进行分类的思路，并把这种方法称之为支持向量机（英语：Support vector machine，简称：SVM）。 如果你第一次接触支持向量机这个名字，可能会感觉读起来比较拗口。至少我当时初次接触支持向量机时，完全不知道为什么会有这样一个怪异的名字。假如你和当时的我一样，那么当你看完下面这段介绍内容后，就应该会对支持向量机这个名词有更深刻的认识了。 支持向量机分类特点假设给定一个训练数据集 \\(T=\\lbrace(x_1,y_1),(x_2,y_2),\\cdots ,(x_n,y_n)\\rbrace\\) 。同时，假定已经找到样本空间中的分割平面，其划分公式可以通过以下线性方程来描述： wx+b=0\\tag{1}使用一条直线对线性可分数据集进行分类的过程中，我们已经知道这样的直线可能有很多条： 问题来了！哪一条直线是最优的划分方法呢？ 在逻辑回归中，我们引入了 S 形曲线和对数损失函数进行优化求解。如今，支持向量机给了一种从几何学上更加直观的方法进行求解，如下图所示： 上图展示了支持向量机分类的过程。图中 \\(wx-b=0\\) 为分割直线，我们通过这条直线将数据点分开。与此同时，分割时会在直线的两边再设立两个互相平行的虚线，这两条虚线与分割直线的距离一致。这里的距离往往也被我们称之为「间隔」，而支持向量机的分割特点在于，要使得分割直线和虚线之间的间隔最大化。同时也就是两虚线之间的间隔最大化。 对于线性可分的正负样本点而言，位于 \\(wx-b=1\\) 虚线外的点就是正样本点，而位于 \\(wx-b=-1\\) 虚线外的点就是负样本点。另外，正好位于两条虚线上方的样本点就被我们称为支持向量，这也就是支持向量机的名字来源。 支持向量机分类演示下面，我们使用 Python 代码来演示支持向量机的分类过程。 首先，我们介绍一种新的示例数据生成方法。即通过 scikit-learn 提供的 samples_generator() 类完成。通过 samples_generator() 类下面提供的不同方法，可以产生不同分布状态的示例数据。首先要用到 make_blobs 方法，该方法可以生成团状数据。 123import numpy as npimport matplotlib.pyplot as plt%matplotlib inline 123456from sklearn.datasets import samples_generatorx, y = samples_generator.make_blobs(n_samples=60, centers=2, random_state=30, cluster_std=0.8) # 生成示例数据plt.figure(figsize=(10, 8)) # 绘图plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap='bwr') 接下来，我们在示例数据中绘制任意 3 条分割线把示例数据分开。 12345678plt.figure(figsize=(10, 8))plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap='bwr')# 绘制 3 条不同的分割线x_temp = np.linspace(0, 6)for m, b in [(1, -8), (0.5, -6.5), (-0.2, -4.25)]: y_temp = m * x_temp + b plt.plot(x_temp, y_temp, '-k') 然后，可以使用 fill_between 方法手动绘制出分类硬间隔。 123456789plt.figure(figsize=(10, 8))plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap='bwr')# 绘制 3 条不同的分割线x_temp = np.linspace(0, 6)for m, b, d in [(1, -8, 0.2), (0.5, -6.5, 0.55), (-0.2, -4.25, 0.75)]: y_temp = m * x_temp + b plt.plot(x_temp, y_temp, '-k') plt.fill_between(x_temp, y_temp - d, y_temp + d, color='#f3e17d', alpha=0.5) 上图为了呈现出分类间隔的效果，手动指定了参数。 可以看出，不同的分割线所对应的间隔大小是不一致的，而支持向量机的目标是找到最大的分类硬间隔所对应的分割线。 硬间隔表示及求解我们已经知道支持向量机是根据最大间隔来划分，下面考虑如何求得一个几何间隔最大的分割线。 对于线性可分数据而言，几何间隔最大的分离超平面是唯一的，这里的间隔也被我们称之为「硬间隔」，而间隔最大化也就称为硬间隔最大化。上图实际上就是硬间隔的典型例子。 最大间隔分离超平面，我们希望最大化超平面 \\((w,b)\\) 关于训练数据集的几何间隔 \\(\\gamma\\)，满足以下约束条件：每个训练样本点到超平面 \\((w,b)\\) 的几何间隔至少都是 \\(\\gamma\\) ，因此可以转化为以下的约束最优化问题： \\max\\limits_{w,b}\\gamma =\\frac{2}{\\left \\|w\\right \\|} \\tag{2a} \\begin{equation} \\textrm s.t. y_i(\\frac{w}{\\left \\|w\\right \\|}x_i+\\frac{b}{\\left \\|w\\right \\|})\\geq \\frac{\\gamma}{2} \\tag{2b} \\end{equation}实际上，\\(\\gamma\\) 的取值并不会影响最优化问题的解，同时，我们根据数学对偶性原则，可以得到面向硬间隔的线性可分数据的支持向量机的最优化问题： \\min\\limits_{w,b}\\frac{1}{2}\\left \\|w\\right \\|^2 \\tag{3a} \\begin{equation} \\textrm s.t. y_i(wx_i+b)-1\\geq 0\\tag{3b} \\end{equation}我们通常使用拉格朗日乘子法来求解最优化问题，将原始问题转化为对偶问题，通过解对偶问题得到原始问题的解。对公式（3）使用拉格朗日乘子法可得到其「对偶问题」。具体来说，对每条约束添加拉格朗日乘子 \\(\\alpha_i \\geq 0\\)，则该问题的拉格朗日函数可写为： L(w,b,\\alpha)=\\frac{1}{2}\\left \\| w\\right \\|^2+\\sum\\limits_{i=1}^{m}\\alpha_i(1-y_i(wx_i+b)) \\tag{4}我们通过将公式（4）分别对 \\(w\\) 和 `b` 求偏导为 0 并代入原式中，可以将 `w` 和 `b` 消去，得到公式（3）的对偶问题： \\max\\limits_{\\alpha} \\sum\\limits_{i=1}^{N}\\alpha_i-\\frac{1}{2}\\sum\\limits_{i=1}^{N}\\sum\\limits_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j x_i x_j \\tag{5a} s.t. \\sum\\limits_{i=1}^{N}\\alpha_i y_i=0,\\tag{5b} \\alpha_i \\geq 0,i=1,2,\\cdots,N \\tag{5c}解出最优解 \\(\\alpha^*=(\\alpha_1^*,\\alpha_2^*,…,\\alpha_N^*)\\) 后，基于此我们可以求得最优解 \\(w^*\\), \\(b^*\\)，由此得到分离超平面： \\(w^*x+b^*=0 \\tag{6}\\) 使用符号函数求得正负类之间的分类决策函数为： \\(f(x)=sign(w^*x+b^*) \\tag{7}\\) 软间隔表示及求解上面，我们介绍了线性可分条件下的最大硬间隔的推导求解方法。在很多时候，我们还会遇到下面这种情况。你可以发现，在实心点和空心点中各混入了零星的不同类别的数据点。对于这种情况，数据集就变成了严格意义上的线性不可分。但是，造成这种线性不可分的原因往往是因为包含「噪声」数据，它同样可以被看作是不严格条件下的线性可分。 当我们使用支持向量机求解这类问题时，就会把最大间隔称之为最大「软间隔」，而软间隔就意味着可以容许零星噪声数据被误分类。 当出现上图所示的样本点不是严格线性可分的情况时，某些样本点 \\((x_i,y_i)\\) 就不能满足函数间隔 \\(\\geqslant 1\\) 的约束条件，即公式（3b）中的约束条件。为了解决这个问题，可以对每个样本点 \\((x_i,y_i)\\) 引入一个松弛变量 \\(\\xi_i \\geq 0\\)，使得函数间隔加上松弛变量 \\(\\geqslant 1\\)，即约束条件转化为： y_i(wx_i+b) \\geq 1-\\xi_i \\tag{8}同时，对每个松弛变量 \\(\\xi_i\\) 支付一个代价 \\(\\xi_i\\)，目标函数由原来的 \\(\\frac{1}{2}||w||^2\\) 变成： \\frac{1}{2}\\left \\| w \\right \\|^2+C\\sum\\limits_{j=1}^{N}\\xi_i \\tag{9}这里，`C&gt;0` 称为惩罚参数，一般根据实际情况确定。`C` 值越大对误分类的惩罚增大，最优化问题即为： \\min\\limits_{w,b,\\xi} \\frac{1}{2}\\left \\| w \\right \\|^2+C\\sum\\limits_{i=1}^{N}\\xi_i \\tag{10a} s.t. y_i(wx_i+b) \\geq 1-\\xi_i,i=1,2,...,N \\tag{10b} \\xi_i\\geq 0,i=1,2,...,N \\tag{10c}这就是软间隔支持向量机的表示过程。同理，我们可以使用拉格朗日乘子法将其转换为对偶问题求解： \\max\\limits_{\\alpha} \\frac{1}{2}\\sum\\limits_{i=1}^{N}\\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_jy_iy_j(x_i*x_j)-\\sum\\limits_{i=1}^{N}\\alpha_i \\tag{11a} s.t. \\sum\\limits_{i=1}^{N}\\alpha_iy_i=0 \\tag{11b} 0 \\leq \\alpha_i \\leq C ,i=1,2,...,N\\tag{11c}解出最优解 \\(\\alpha^*=(\\alpha_1^*,\\alpha_2^*,…,\\alpha_N^*)\\) 后，基于此我们可以求得最优解 \\(w^*\\), \\(b^*\\)，由此得到分离超平面： \\(w^*x+b^*=0 \\tag{12}\\) 使用符号函数求得正负类之间的分类决策函数为：\\(f(x)=sign(w^*x+b^*) \\tag{13}\\) 线性支持向量机分类实现上面，我们对硬间隔和软间隔支持向量机的求解过程进行了推演，推导过程比较复杂不需要完全掌握，但至少要知道硬间隔和软间隔区别。接下来，我们就使用 Python 对支持向量机找寻最大间隔的过程进行实战。由于支持向量机纯 Python 实现太过复杂，所以本次直接使用 scikit-learn 完成。 scikit-learn 中的支持向量机分类器对应的类及参数为： 1sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None) 主要的参数如下： C: 软间隔支持向量机对应的惩罚参数，详见公式（9）. kernel: 核函数，linear, poly, rbf, sigmoid, precomputed 可选，下文详细介绍。 degree: poly 多项式核函数的指数。 tol: 收敛停止的容许值。 这里，我们还是使用上面生成的示例数据训练支持向量机模型。由于是线性可分数据，kernel 参数指定为 linear 即可。 首先，训练支持向量机线性分类模型： 1234from sklearn.svm import SVClinear_svc = SVC(kernel='linear')linear_svc.fit(x, y) 1234SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto_deprecated', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 对于训练完成的模型，我们可以通过 support_vectors_ 属性输出它对应的支持向量： 1linear_svc.support_vectors_ 123array([[ 2.57325754, -3.92687452], [ 2.49156506, -5.96321164], [ 4.62473719, -6.02504452]]) 可以看到，一共有 3 个支持向量。如果你输出 x, y 的坐标值，就能看到这 3 个支持向量所对应的数据。 接下来，我们可以使用 Matplotlib 绘制出训练完成的支持向量机对于的分割线和间隔。为了方便后文重复使用，这里将绘图操作写入到 svc_plot() 函数中： 123456789101112131415def svc_plot(model): # 获取到当前 Axes 子图数据，并为绘制分割线做准备 ax = plt.gca() x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 50) y = np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 50) Y, X = np.meshgrid(y, x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) # 使用轮廓线方法绘制分割线 ax.contour(X, Y, P, colors='green', levels=[-1, 0, 1], linestyles=['--', '-', '--']) # 标记出支持向量的位置 ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], c='green', s=100) 1234# 绘制最大间隔支持向量图plt.figure(figsize=(10, 8))plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap='bwr')svc_plot(linear_svc) 如上图所示，绿色实线代表最终找到的分割线，绿色虚线之间的间隔也就是最大间隔。同时，绿色实心点即代表 3 个支持向量的位置。 上面的数据点可以被线性可分，所以得到的也就是硬间隔支持向量机的分类结果。那么，如果我们加入噪声使得数据集变成不完美线性可分，结果会怎么样呢？ 接下来，我们就来还原软间隔支持向量机的分类过程： 123456# 向原数据集中加入噪声点x = np.concatenate((x, np.array([[3, -4], [4, -3.8], [2.5, -6.3], [3.3, -5.8]])))y = np.concatenate((y, np.array([1, 1, 0, 0])))plt.figure(figsize=(10, 8))plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap='bwr') 可以看到，此时的红蓝数据团中各混入了两个噪声点。 训练软间隔支持向量机模型并绘制成分割线和最大间隔： 123456linear_svc.fit(x, y) # 训练# 绘图plt.figure(figsize=(10, 8))plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap='bwr')svc_plot(linear_svc) 由于噪声点的混入，此时支持向量的数量由原来的 3 个变成了 13 个。 前面的实验中，我们提到了惩罚系数 `C`，下面可以通过更改 `C` 的取值来观察支持向量的变化过程。与此同时，我们要引入一个可以在 Notebook 中实现交互操作的模块。你可以通过选择不同的 `C` 查看最终绘图的效果。 1234567891011from ipywidgets import interactimport ipywidgets as widgetsdef change_c(c): linear_svc.C = c linear_svc.fit(x, y) plt.figure(figsize=(10, 8)) plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap='bwr') svc_plot(linear_svc) interact(change_c, c=[1, 10000, 1000000]) 非线性分类支持向量机上面的内容中，我们假设样本是线性可分或不严格线性可分，然后通过支持向量机建立最大硬间隔或软间隔实现样本分类。然而，线性可分的样本往往只是理想情况，现实中的原始样本大多数情况下是线性不可分。此时，还能用支持向量机吗？ 其实，对于线性不可分的数据集，我们也可以通过支持向量机去完成分类。但是，这里需要增加一个技巧把线性不可分数据转换为线性可分数据之后，再完成分类。 与此同时，我们把这种数据转换的技巧称作「核技巧」，实现数据转换的函数称之为「核函数」。 核技巧与核函数根据上面的介绍，我们提到一个思路就是核技巧，即先把线性不可分数据转换为线性可分数据，然后再使用支持向量机去完成分类。那么，具体是怎样操作呢？ 核技巧的关键在于空间映射，即将低维数据映射到高维空间中，使得数据集在高维空间能被线性可分。 * 核技巧是一种数学方法，仅针对于其在支持向量机中的应用场景进行讲解。 如上图所示，假设我们在二维空间中有蓝色和红色代表的两类数据点，很明显无法使用一条直线把这两类数据分开。此时，如果我们使用核技巧将其映射到三维空间中，就变成了可以被平面线性可分的状态。 对于「映射」过程，我们还可以这样理解：分布在二维桌面上的红蓝小球无法被线性分开，此时将手掌拍向桌面（好疼），小球在力的作用下跳跃到三维空间中，这也就是一个直观的映射过程。 同时，「映射」的过程也就是通过核函数转换的过程。这里需要补充说明一点，那就是将数据点从低维度空间转换到高维度空间的方法有很多，但往往涉及到庞大的计算量，而数学家们从中发现了几种特殊的函数，这类函数能大大降低计算的复杂度，于是被命名为「核函数」。也就是说，核技巧是一种特殊的「映射」技巧，而核函数是核技巧的实现方法。 下面，我们就认识几种常见的核函数： 线性核函数 k\\left ( x_i, x_j \\right )=x_i*x_j \\tag{14}多项式核函数 k\\left ( x_i, x_j \\right )=\\left ( x_i*x_j \\right )^d, d \\geq 1 \\tag{15}高斯径向基核函数 k\\left ( x_i, x_j \\right ) = \\exp \\left(-{\\frac {\\left \\|{\\mathbf {x_i}}-{\\mathbf {x_j}}\\right \\|_{2}^{2}}{2\\sigma ^{2}}}\\right)=exp\\left ( -\\gamma * \\left \\| x_i-x_j \\right \\|_{2} ^2 \\right ), \\gamma>0 \\tag{16}Sigmoid 核函数 k\\left ( x_i, x_j \\right )=tanh\\left ( \\beta * x_ix_j+\\theta \\right ), \\beta > 0 , \\theta < 0 \\tag{17}这 4 个核函数也就分别对应着上文介绍 sklearn 中 SVC 方法中 kernel 参数的 linear, poly, rbf, sigmoid 等 4 种不同取值。 此外，核函数还可以通过函数组合得到，例如： 若 `k_1` 和 `k_2` 是核函数，那么对于任意正数 `\\lambda_1,\\lambda_2`，其线性组合： \\lambda_1 k_1+\\lambda_2 k_2 \\tag{18}引入核函数的间隔表示及求解我们通过直接引入核函数 `k(x_i,x_j)`，而不需要显式的定义高维特征空间和映射函数，就可以利用解线性分类问题的方法来求解非线性分类问题的支持向量机。引入核函数以后，对偶问题就变为： \\max\\limits_{\\alpha} \\frac{1}{2}\\sum\\limits_{i=1}^{N}\\sum\\limits_{j=1}^{N}\\alpha_i\\alpha_jy_iy_jk(x_i*x_j)-\\sum\\limits_{i=1}^{N}\\alpha_i \\tag{19a} s.t. \\sum\\limits_{i=1}^{N}\\alpha_iy_i=0 \\tag{19b} 0 \\leq \\alpha_i \\leq C ,i=1,2,...,N \\tag{19c}同样，解出最优解 \\(\\alpha^*=(\\alpha_1^*,\\alpha_2^*,…,\\alpha_N^*)\\) 后，基于此我们可以求得最优解 \\(w^*\\), \\(b^*\\)，由此得到分离超平面： w^\\*x+b^\\*=0 \\tag{20}使用符号函数求得正负类之间的分类决策函数为： f(x)=sign(w^\\*x+b^\\*) \\tag{21}非线性支持向量机分类实现同样，我们使用 scikit-learn 中提供的 SVC 类来构建非线性支持向量机模型，并绘制决策边界。 首先，实验需要生成一组示例数据。上面我们使用了 make_blobs 生成一组线性可分数据，这里使用 make_circles 生成一组线性不可分数据。 1234x2, y2 = samples_generator.make_circles(150, factor=.5, noise=.1, random_state=30) # 生成示例数据plt.figure(figsize=(8, 8)) # 绘图plt.scatter(x2[:, 0], x2[:, 1], c=y2, s=40, cmap='bwr') 上图明显是一组线性不可分数据，当我们训练支持向量机模型时就需要引入核技巧。例如，我们这里使用下式做一个简单的非线性映射： k\\left ( x_i, x_j \\right )=x_i^2 + x_j^2 \\tag{22}123def kernel_function(xi, xj): poly = xi**2 + xj**2 return poly 12345678910from mpl_toolkits import mplot3dfrom ipywidgets import interact, fixedr = kernel_function(x2[:,0], x2[:,1])plt.figure(figsize=(10, 8))ax = plt.subplot(projection='3d')ax.scatter3D(x2[:, 0], x2[:, 1], r, c=y2, s=40, cmap='bwr')ax.set_xlabel('x')ax.set_ylabel('y')ax.set_zlabel('r') 上面展示了二维空间点映射到效果维空间的效果。接下来，我们使用 sklearn 中 SVC 方法提供的 RBF 高斯径向基核函数完成实验。 12rbf_svc = SVC(kernel='rbf')rbf_svc.fit(x2, y2) 1234SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto_deprecated', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 1234plt.figure(figsize=(8, 8))plt.scatter(x2[:, 0], x2[:, 1], c=y2, s=40, cmap='bwr')svc_plot(rbf_svc) 同样，我们可以挑战不同的惩罚系数 `C`，看一看决策边界和支持向量的变化情况： 12345678def change_c(c): rbf_svc.C = c rbf_svc.fit(x2, y2) plt.figure(figsize=(8, 8)) plt.scatter(x2[:, 0], x2[:, 1], c=y2, s=40, cmap='bwr') svc_plot(rbf_svc) interact(change_c, c=[1, 100, 10000]) 多分类支持向量机支持向量机最初是为二分类问题设计的，当我们面对多分类问题时，其实同样可以使用支持向量机解决。而解决的方法就是通过组合多个二分类器来实现多分类器的构造。根据构造的方式又分为 2 种方法： 一对多法：即训练时依次把某个类别的样本归为一类，剩余的样本归为另一类，这样 `k` 个类别的样本就构造出了 `k` 个支持向量机。 一对一法：即在任意两类样本之间构造一个支持向量机，因此 `k` 个类别的样本就需要设计 \\(k(k-1) \\div 2\\) 个支持向量机。 而在 scikit-learn，实现多分类支持向量机通过设定参数 decision_function_shape 来确定，其中： decision_function_shape='ovo'：代表一对一法。 decision_function_shape='ovr'：代表一对多法。 由于这里只需要修改参数，所以就不再赘述了。 拓展阅读： 支持向量机 - 维基百科 知乎上关于支持向量机的问题讨论 机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM","link":"/2019/01/20/Support Vector Machine/"},{"title":"机器学习|决策树详解 (Python 语言描述)","text":"决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类，而有向边包含判断条件。决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。举一个通俗的例子，假设在B站工作多年仍然单身的小B和他母亲在给他介绍对象时的一段对话： 母亲：小B，你都 28 了还是单身，明天亲戚家要来个姑娘要不要去见见。小B：多大年纪？母亲：26。小B：有多高？母亲：165厘米。小B：长的好看不。母亲：还行，比较朴素。小B：温柔不？母亲：看起来挺温柔的，很有礼貌。小B：好，去见见。 作为程序员的小B的思考逻辑就是典型的决策树分类逻辑，将年龄，身高，长相，是否温柔作为特征，并最后对见或者不见进行决策。其决策逻辑如图所示： 决策树算法实现其实决策树算法如同上面场景一样，其思想非常容易理解，具体的算法流程为： 第 1 步: 数据准备：通过数据清洗和数据处理，将数据整理为没有缺省值的向量。 第 2 步: 寻找最佳特征：遍历每个特征的每一种划分方式，找到最好的划分特征。 第 3 步: 生成分支：划分成两个或多个节点。 第 4 步: 生成决策树：对分裂后的节点分别继续执行2-3步，直到每个节点只有一种类别。 第 5 步: 决策分类：根据训练决策树模型，将预测数据进行分类。 数据生成下面我们依照决策树的算法流程，用 python 来实现决策树构建和分类。首先生成一组数据，数据包含两个类别 man 和 woman,特征分别为: hair:头发长短(long:长,short:短) voice:声音粗细(thick:粗,thin:细) height:身高 ear_stud:是否带有耳钉(yes:是,no:没有) 12345678910111213141516171819202122232425262728\"\"\"生成示例数据\"\"\"import numpy as npimport pandas as pddef create_data(): data_value = np.array( [['long', 'thick', 175, 'no', 'man'], ['short', 'medium', 168, 'no', 'man'], ['short', 'thin', 178, 'yes', 'man'], ['short', 'thick', 172, 'no', 'man'], ['long', 'medium', 163, 'no', 'man'], ['short', 'thick', 180, 'no', 'man'], ['long', 'thick', 173, 'yes', 'man'], ['short', 'thin', 174, 'no', 'man'], ['long', 'thin', 164, 'yes', 'woman'], ['long', 'medium', 158, 'yes', 'woman'], ['long', 'thick', 161, 'yes', 'woman'], ['short', 'thin', 166, 'yes', 'woman'], ['long', 'thin', 158, 'no', 'woman'], ['short', 'medium', 163, 'no', 'woman'], ['long', 'thick', 161, 'yes', 'woman'], ['long', 'thin', 164, 'no', 'woman'], ['short', 'medium', 172, 'yes', 'woman']]) columns = np.array(['hair', 'voice', 'height', 'ear_stud', 'labels']) data = pd.DataFrame(data_value.reshape(17, 5), columns=columns) return data 在创建好数据之后，加载并打印出这些数据 12data = create_data()data 划分选择在得到数据后，根据算法流程，接下来需要寻找最优的划分特征，随着划分的不断进行，我们尽可能的将划分的分支所包含的样本归于同一类别，即结点的“纯度”越来越高。而常用的特征划分方式为信息增益和增益率。 信息增益（ID3）在介绍信息增益之前，先引入“信息熵”的概念。“信息熵”是度量样本纯度最常用的一种指标，其公式为： Ent(D)=-\\sum_{k=1}^{\\left |y \\right |}p_{k}\\; log_{2}p_{k} \\tag{1}其中 `D` 表示样本集合，`p_{k}` 表示第 `k` 类样本所占的比例。其中 `Ent(D)` 的值越小，则 `D` 的纯度越高。根据以上数据，在计算数据集的“信息熵”时，\\(\\left | y \\right |\\) 显然只有 man,woman 共 2 种，其中为 man 的概率为 \\(\\frac{8}{17}\\), woman 的概率为 \\(\\frac{9}{17}\\),则根据公式(1)得到数据集的纯度为： Ent(data)=-\\sum_{k=1}^{2}p_{k}\\;log_{2}p_{k}=-(\\frac{8}{17}log_{2}\\frac{8}{17}+\\frac{9}{17}log_{2}\\frac{9}{17})=0.99751234567891011121314151617181920212223242526272829\"\"\"计算信息熵\"\"\"import mathdef get_Ent(data): \"\"\" 参数: data -- 数据集 返回: Ent -- 信息熵 \"\"\" num_sample = len(data) # 样本个数 label_counts = {} # 初始化标签统计字典 for i in range(num_sample): each_data = data.iloc[i, :] current_label = each_data[\"labels\"] # 得到当前元素的标签（label） # 如果标签不在当前字典中，添加该类标签并初始化 value=0,否则该类标签 value+1 if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 Ent = 0.0 # 初始化信息熵 for key in label_counts: prob = float(label_counts[key])/num_sample Ent -= prob * math.log(prob, 2) # 应用信息熵公式计算信息熵 return Ent 通过计算信息熵函数，计算根节点的信息熵： 12base_ent = get_Ent(data)base_ent 0.9975025463691153 信息增益 就是建立在信息熵的基础上，在离散特征 `x` 有 `M` 个取值，如果用 `x` 对样本 `D` 来进行划分，就会产生 `M` 个分支，其中第 `m` 个分支包含了集合 `D` 的所有在特征 `x` 上取值为 `m` 的样本，记为 `D^{m}`（例如：根据以上生成数据，如果我们用 hair 进行划分，则会产生long，short两个分支，每一个分支中分别包含了整个集合中属于 long 或者 short 的数据）。 考虑到不同分支节点包含样本数不同，给分支赋予权重 \\(\\frac{\\left | D^{m}\\right |}{\\left | D \\right |}\\) ,使得样本越多的分支节点影响越大，则 信息增益 的公式就可以得到： Gain(D,x)=Ent(D)- \\sum_{m=1}^{M}\\frac{\\left | D^{m}\\right |}{\\left | D \\right |}Ent(D^{m}) \\tag{2}一般情况下，信息增益越大，则说明用 `x` 来划分样本集合 `D` 的纯度越高。以 hair 为例，其中它有 short 和 long 两个可能取值，则分别用 `D^{1}` (hair = long) 和 `D^{2} (hair = short)来表示。 其中为 \\(D^{1}\\) 的数据编号为 \\(\\{0，4，6，8，9，10，12，14，15\\}\\) 共 9 个，在这之中为 man 的有 {0，4，6} 共3 个占比为\\(\\frac{3}{9}\\)，为 woman 的有{8, 9，10，12，14，15}共 6 个占比为\\(\\frac{6}{9}\\); 同样 `D^{2}` 编号为{1，2，3，5，7，11，13, 16}共 8 个，其中为 man 的有{1，2，3，5，7}共 5 个占比\\(\\frac{5}{8}\\),为 woman 的有{11，13, 16}共 3 个占比 \\(\\frac{3}{8}\\),若按照 hair 进行划分，则两个分支点的信息熵为： Ent(D^{1})=-(\\frac{3}{9}\\;log_{2}\\frac{3}{9}+\\frac{6}{9}\\;log_{2}\\frac{6}{9})=0.449 Ent(D^{2})=-(\\frac{5}{8}\\;log_{2}\\frac{5}{8}+\\frac{3}{8}\\;log_{2}\\frac{3}{8})=0.486根据信息增益的公式可以计算出 hair 的信息增益为： Gain(D,hair)=Ent(D)-\\sum_{m=1}^{2}\\frac{\\left | D^{m} \\right |}{\\left | D \\right |}Ent(D^{m})=0.9975-(\\frac{9}{17}\\*0.449+\\frac{8}{17}\\*0.486)=0.062下面我们用 python 来实现信息增益（ID3）算法： 123456789101112131415161718192021222324252627\"\"\"计算信息增益\"\"\"def get_gain(data, base_ent, feature): \"\"\" 参数: data -- 数据集 base_ent -- 根节点的信息熵 feature -- 计算信息增益的特征 返回: Ent -- 信息熵 \"\"\" feature_list = data[feature] # 得到一个特征的全部取值 unique_value = set(feature_list) # 特征取值的类别 feature_ent = 0.0 for each_feature in unique_value: temp_data = data[data[feature] == each_feature] weight = len(temp_data)/len(feature_list) # 计算该特征的权重值 temp_ent = weight*get_Ent(temp_data) feature_ent = feature_ent+temp_ent gain = base_ent - feature_ent # 信息增益 return gain 完成 信息增益 函数后，尝试计算特征 hair 的信息增益值。 1get_gain(data,base_ent,'hair') 0.062200515199107964 信息增益率（C4.5）信息增益也存在许多不足之处，经过大量的实验发现，当信息增益作为标准时，易偏向于取值较多的特征，为了避免这种偏好给预测结果带来的不好影响，可以使用增益率来选择最优划分。增益率的公式定义为: GainRatio(D,a)=\\frac{Gain(D,a)}{IV(a)} \\tag{3}其中： IV(a)=-\\sum_{m=1}^{M}\\frac{\\left | D^{m} \\right |}{\\left | D \\right |}\\; log_{2}\\frac{\\left | D^{m} \\right |}{\\left | D \\right |} \\tag{4}`IV(a)` 称为特征 `a` 的固有值，当 `a` 的取值数目越多，则 `IV(a)` 的值通常会比较大。例如： IV(hair)= -\\frac{9}{17}\\; log_{2}\\frac{9}{17}-\\frac{8}{17}\\; log_{2}\\frac{8}{17}=0.998 IV(voice)= -\\frac{7}{17}\\; log_{2}\\frac{7}{17} -\\frac{5}{17}\\; log_{2}\\frac{5}{17} - \\frac{5}{17}\\; log_{2}\\frac{5}{17} = 1.566连续值处理在前面介绍的特征选择中，都是对离散型数据进行处理，但在实际的生活中数据常常会出现连续值的情况，如生成数据中的身高，当数据较少时，可以将每一个值作为一个类别，但当数据量大时，这样是不可取的，在 C4.5 算法中采用二分法对连续值进行处理。 对于连续的属性 `X` 假设共出现了 n 个不同的取值，将这些取值从小到大排序\\(\\{x_{1},x_{2},x_{3},…,x_{n} \\} \\)，其中找一点作为划分点 t ，则将数据划分为两类，大于 t 的为一类，小于 t 的为另一类。而 t 的取值通常为相邻两点的平均数 \\(t=\\frac{x_{i}+x_{i+1}}{2}\\)。 则在 n 个连续值之中，可以作为划分点的 t 有 n-1 个。通过遍历可以像离散型一样来考察这些划分点。 Gain(D,X)=Ent(D)-\\frac{\\left | D_{t}) \\tag{5}其中得到样本 `D` 基于划分点 t 二分后的信息增益，于是我们可以选择使得 `Gain(D,X)` 值最大的划分点。 12345678910111213141516171819202122232425262728293031323334353637383940\"\"\"计算连续值的划分点\"\"\"def get_splitpoint(data, base_ent, feature): \"\"\" 参数: data -- 数据集 base_ent -- 根节点的信息熵 feature -- 需要划分的连续特征 返回: final_t -- 连续值最优划分点 \"\"\" # 将连续值进行排序并转化为浮点类型 continues_value = data[feature].sort_values().astype(np.float64) continues_value = [i for i in continues_value] # 不保留原来的索引 t_set = [] t_ent = {} # 得到划分点 t 的集合 for i in range(len(continues_value)-1): temp_t = (continues_value[i]+continues_value[i+1])/2 t_set.append(temp_t) # 计算最优划分点 for each_t in t_set: # 将大于划分点的分为一类 temp1_data = data[data[feature].astype(np.float64) &gt; each_t] # 将小于划分点的分为一类 temp2_data = data[data[feature].astype(np.float64) &lt; each_t] weight1 = len(temp1_data)/len(data) weight2 = len(temp2_data)/len(data) # 计算每个划分点的信息增益 temp_ent = base_ent-weight1 * \\ get_Ent(temp1_data)-weight2*get_Ent(temp2_data) t_ent[each_t] = temp_ent print(\"t_ent:\", t_ent) final_t = max(t_ent, key=t_ent.get) return final_t 实现连续值最优划分点的函数后，寻找 height 连续特征值的划分点。 12final_t = get_splitpoint(data, base_ent, 'height')final_t t_ent: {158.0: 0.1179805181500242, 159.5: 0.1179805181500242, 161.0: 0.2624392604045631, 162.0: 0.2624392604045631, 163.0: 0.3856047022157598, 163.5: 0.15618502398692893, 164.0: 0.3635040117533678, 165.0: 0.33712865788827096, 167.0: 0.4752766311586692, 170.0: 0.32920899348970845, 172.0: 0.5728389611412551, 172.5: 0.4248356349861979, 173.5: 0.3165383509071513, 174.5: 0.22314940393447813, 176.5: 0.14078143361499595, 179.0: 0.06696192680347068} 172.0 算法实现在对决策树中最佳特征选择和连续值处理之后，接下来就是对决策树的构建。 数据预处理首先我们将连续值进行处理，在找到最佳划分点之后，将 \\(&lt; t\\) 的值设为 0，将 \\(&gt;= t\\) 的值设为 1。 123456789101112def choice_1(x, t): if x &gt; t: return \"&gt;{}\".format(t) else: return \"&lt;{}\".format(t)deal_data = data.copy()# 使用lambda和map函数将 height 按照final_t划分为两个类别deal_data[\"height\"] = pd.Series( map(lambda x: choice_1(int(x), final_t), deal_data[\"height\"]))deal_data 选择最优划分特征将数据进行预处理之后，接下来就是选择最优的划分特征。 12345678910111213141516171819202122\"\"\"选择最优划分特征\"\"\"def choose_feature(data): \"\"\" 参数: data -- 数据集 返回: best_feature -- 最优的划分特征 \"\"\" num_features = len(data.columns) - 1 # 特征数量 base_ent = get_Ent(data) best_gain = 0.0 # 初始化信息增益 best_feature = data.columns[0] for i in range(num_features): # 遍历所有特征 temp_gain = get_gain(data, base_ent, data.columns[i]) # 计算信息增益 if (temp_gain &gt; best_gain): # 选择最大的信息增益 best_gain = temp_gain best_feature = data.columns[i] return best_feature # 返回最优特征 完成函数之后，我们首先看看数据集中信息增益值最大的特征是什么？ 1choose_feature(deal_data) 'height' 构建决策树在将所有的子模块构建好之后，最后就是对核心决策树的构建，本次实验采用信息增益（ID3）的方式构建决策树。在构建的过程中，根据算法流程，我们反复遍历数据集，计算每一个特征的信息增益，通过比较将最好的特征作为父节点，根据特征的值确定分支子节点，然后重复以上操作，直到某一个分支全部属于同一类别，或者遍历完所有的数据特征，当遍历到最后一个特征时，若分支数据依然“不纯”，就将其中数量较多的类别作为子节点。 因此最好采用递归的方式来构建决策树。 1234567891011121314151617181920212223242526272829\"\"\"构建决策树\"\"\"def create_tree(data): \"\"\" 参数: data -- 数据集 返回: tree -- 以字典的形式返回决策树 \"\"\" feature_list = data.columns[:-1].tolist() label_list = data.iloc[:, -1] if len(data[\"labels\"].value_counts()) == 1: leaf_node = data[\"labels\"].mode().values return leaf_node # 第一个递归结束条件：所有的类标签完全相同 if len(feature_list) == 1: leaf_node = data[\"labels\"].mode().values return leaf_node # 第二个递归结束条件：用完了所有特征 best_feature = choose_feature(data) # 最优划分特征 tree = {best_feature: {}} feat_values = data[best_feature] unique_value = set(feat_values) for value in unique_value: temp_data = data[data[best_feature] == value] temp_data = temp_data.drop([best_feature], axis=1) tree[best_feature][value] = create_tree(temp_data) return tree 完成创建决策树函数后，接下来对我们第一棵树进行创建。 12tree = create_tree(deal_data)tree {'height': {'&lt;172.0': {'ear_stud': {'no': {'voice': {'thick': array(['man'], dtype=object), 'medium': array(['man'], dtype=object), 'thin': array(['woman'], dtype=object)}}, 'yes': array(['woman'], dtype=object)}}, '&gt;172.0': array(['man'], dtype=object)}} 通过字典的方式表示构建好的树，可以通过图像的方式更加直观的了解。 通过图形可以看出，在构建决策树时不一定每一个特征都会成为树的节点（如同 hair）。 决策分类在构建好决策树之后，最终就可以使用未知样本进行预测分类。 123456789101112131415161718192021222324\"\"\"决策分类\"\"\"def classify(tree, test): \"\"\" 参数: data -- 数据集 test -- 需要测试的数据 返回: class_label -- 分类结果 \"\"\" first_feature = list(tree.keys())[0] # 获取根节点 feature_dict = tree[first_feature] # 根节点下的树 labels = test.columns.tolist() value = test[first_feature][0] for key in feature_dict.keys(): if value == key: if type(feature_dict[key]).__name__ == 'dict': # 判断该节点是否为叶节点 class_label = classify(feature_dict[key], test) # 采用递归直到遍历到叶节点 else: class_label = feature_dict[key] return class_label 在分类函数完成之后，接下来我们尝试对未知数据进行分类。 12test = pd.DataFrame({\"hair\": [\"long\"], \"voice\": [\"thin\"], \"height\": [163], \"ear_stud\": [\"yes\"]})test 对连续值进行预处理。 12test[\"height\"] = pd.Series(map(lambda x: choice_1(int(x), final_t), test[\"height\"]))test 分类预测。 1classify(tree,test) array(['woman'], dtype=object) 一个身高 163 厘米，长发，带着耳钉且声音纤细的人，在我们构建的决策树判断后预测为一名女性。 上面的实验中，我们没有考虑 =划分点 的情况，你可以自行尝试将 &gt;=划分点 或 &lt;=划分点 归为一类，看看结果又有哪些不同？ 预剪枝和后剪枝在决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多。对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝。 预剪枝预剪枝，顾名思义预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点。例如前面构造的决策树，按照决策树的构建原则，通过 height 特征进行划分后 &lt;172 分支中又按照 ear_stud 特征值进行继续划分。如果应用预剪枝，则当通过 height 进行特征划分之后，对 &lt;172 分支是否进行 ear_stud 特征进行划分时计算划分前后的准确度，如果划分后的更高则按照 ear_stud 继续划分，如果更低则停止划分。 后剪枝跟预剪枝在构建决策树的过程中判断是否继续特征划分所不同的是，后剪枝在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。例如在前面构建好决策树之后，&gt;172分支的 voice 特征，将其替换为叶节点如（man），计算替换前后划分准确度，如果替换后准确度更高则进行修剪（用叶节点替换分支节点），否则不修剪。 预测分类在前面我们使用 python 将决策树的特征选择，连续值处理和预测分类做了详细的讲解。接下来我们应用决策树模型对真实的数据进行分类预测。 导入数据本次应用到的数据为学生成绩数据集 course-13-student.csv，一共有 395 条数据，26 个特征。 数据集下载 👉 传送门 123456\"\"\"导入数据集并预览\"\"\"import pandas as pdstu_grade = pd.read_csv('course-13-student.csv')stu_grade.head() 由于特征过多，我们选择部分特征作为决策树模型的分类特征,分别为： school：学生所读学校(GP，MS) sex: 性别(F：女，M：男) address: 家庭住址(U：城市，R：郊区) Pstatus: 父母状态(A：同居，T：分居) Pedu: 父母学历由低到高 reason: 选择这所学校的原因(home：家庭,course：课程设计，reputation：学校地位，other：其他) guardian: 监护人(mother：母亲，father：父亲，other：其他) studytime: 周末学习时长 schoolsup: 额外教育支持(yes：有，no：没有) famsup: 家庭教育支持(yes：有，no：没有) paid: 是否上补习班(yes：是，no：否) higher: 是否想受更好的教育(yes：是，no：否) internet: 是否家里联网(yes：是，no：否) G1: 一阶段测试成绩 G2: 二阶段测试成绩 G3: 最终成绩 12new_data = stu_grade.iloc[:, [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 15, 24, 25, 26]]new_data.head() 数据预处理首先我们将成绩 G1，G2，G3 根据分数进行等级划分，将 0-4 划分为 bad，5-9 划分为 medium ， 1234567891011121314151617def choice_2(x): x = int(x) if x &lt; 5: return \"bad\" elif x &gt;= 5 and x &lt; 10: return \"medium\" elif x &gt;= 10 and x &lt; 15: return \"good\" else: return \"excellent\"stu_data = new_data.copy()stu_data[\"G1\"] = pd.Series(map(lambda x: choice_2(x), stu_data[\"G1\"]))stu_data[\"G2\"] = pd.Series(map(lambda x: choice_2(x), stu_data[\"G2\"]))stu_data[\"G3\"] = pd.Series(map(lambda x: choice_2(x), stu_data[\"G3\"]))stu_data.head() 同样我们对 Pedu （父母教育程度）也进行划分 123456789101112def choice_3(x): x = int(x) if x &gt; 3: return \"high\" elif x &gt; 1.5: return \"medium\" else: return \"low\"stu_data[\"Pedu\"] = pd.Series(map(lambda x: choice_3(x), stu_data[\"Pedu\"]))stu_data.head() 在等级划分之后，为遵循 scikit-learn 函数的输入规范，需要将数据特征进行替换。 12345678910111213141516171819\"\"\"特征值替换\"\"\"def replace_feature(data): \"\"\" 参数: data -- 数据集 返回: data -- 将特征值替换后的数据集 \"\"\" for each in data.columns: # 遍历每一个特征名称 feature_list = data[each] unique_value = set(feature_list) i = 0 for fea_value in unique_value: data[each] = data[each].replace(fea_value, i) i += 1 return data 将特征值进行替换后展示。 12stu_data = replace_feature(stu_data)stu_data.head(10) 数据划分加载好预处理的数据集之后，为了实现决策树算法，同样我们需要将数据集分为 训练集和测试集，依照经验：训练集占比为 70%，测试集占 30%。 同样在此我们使用 scikit-learn 模块的 train_test_split 函数完成数据集切分。123from sklearn.model_selection import train_test_splitx_train,x_test, y_train, y_test =train_test_split(train_data,train_target,test_size=0.4, random_state=0) 其中： x_train,x_test, y_train, y_test 分别表示，切分后的 特征的训练集，特征的测试集，标签的训练集，标签的测试集；其中特征和标签的值是一一对应的。 train_data,train_target分别表示为待划分的特征集和待划分的标签集。 test_size：测试样本所占比例。 random_state：随机数种子,在需要重复实验时，保证在随机数种子一样时能得到一组一样的随机数。 123456from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(stu_data.iloc[:, :-1], stu_data[\"G3\"], test_size=0.3, random_state=5)x_test 决策树构建在划分好数据集之后，接下来就是进行预测。在前面的实验中我们采用 python 对决策树算法进行实现，下面我们通过 scikit-learn 来对其进行实现。 scikit-learn 决策树类及常用参数如下： 1DecisionTreeClassifier(criterion=’gini’，random_state=None) 其中： criterion 表示特征划分方法选择，默认为 gini (在后面会讲到)，可选择为 entropy (信息增益)。 ramdom_state 表示随机数种子，当特征特别多时 scikit-learn 为了提高效率，随机选取部分特征来进行特征选择，即找到所有特征中较优的特征。 常用方法: fit(x,y)训练决策树。 predict(X) 对数据集进行预测返回预测结果。 1234from sklearn.tree import DecisionTreeClassifierdt_model = DecisionTreeClassifier(criterion='entropy', random_state=34)dt_model.fit(x_train,y_train) # 使用训练集训练模型 DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=34, splitter='best') 决策树可视化在构建好决策树之后，我们需要对创建好的决策树进行可视化展示，引入 export_graphviz 进行画图。由于环境中没有函数需要进行安装。 123456# Linux!apt-get install --yes graphviz # 安装所需模块!pip install graphviz# windows Anacondaconda install graphviz 下面开始生成决策树图像，其中生成决策树较大需要拖动滑动条进行查看。 1234567891011from sklearn.tree import export_graphvizimport graphvizimg = export_graphviz( dt_model, out_file=None, feature_names=stu_data.columns[:-1].values.tolist(), # 传入特征名称 class_names=np.array([\"bad\", \"medium\", \"good\", \"excellent\"]), # 传入类别值 filled=True, node_ids=True, rounded=True)graphviz.Source(img) # 展示决策树 模型预测12y_predict = dt_model.predict(x_test) # 使用模型对测试集进行预测y_predict array([3, 1, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 3, 0, 3, 2, 0, 3, 2, 2, 0, 3, 2, 2, 0, 3, 3, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 3, 0, 3, 3, 2, 2, 0, 2, 2, 2, 2, 2, 3, 2, 2, 0, 3, 0, 2, 2, 2, 1, 3, 0, 2, 2, 2, 2, 3, 3, 2, 0, 0, 0, 1, 2, 2, 0, 0, 3, 0, 3, 2, 3, 2, 2, 3, 1, 0, 0, 0, 2, 1, 2, 2, 2, 2, 3, 0, 0, 3, 0, 0, 2, 3, 2, 1, 2, 2, 0, 0, 2, 0, 2, 0, 3, 2, 2, 2, 3, 2], dtype=int64) 分类准确率计算当我们训练好模型并进行分类预测之后，可以通过比对预测结果和真实结果得到预测的准确率。 accur=\\frac{\\sum_{i=1}^{N}I(\\bar{y_{i}}=y_{i})}{N} \\tag{6}公式(6)中 `N` 表示数据总条数，\\(\\bar{y_{i}}\\) 表示第 `i` 条数据的种类预测值，`y_{i}` 表示第 `i` 条数据的种类真实值，`I` 同样是指示函数，表示 \\(\\bar{y_{i}}\\) 和 `y_{i}` 相同的个数。 12345678910111213141516171819\"\"\"准确率计算\"\"\"def get_accuracy(test_labels, pred_labels): \"\"\" 参数: test_labels -- 测试集的真实值 pred_labels -- 测试集的预测值 返回: accur -- 准确率 \"\"\" correct = np.sum(test_labels == pred_labels) # 计算预测正确的数据个数 n = len(test_labels) # 总测试集数据个数 accur = correct/n return accurget_accuracy(y_test, y_predict) 0.6974789915966386 CART 决策树分类与回归树（classification and regression tree, CART）同样也是应用广泛的决策树学习算法，CART 算法是按照特征划分，由树的生成和树的剪枝构成，既可以进行分类又可以用于回归，按照作用将其分为决策树和回归树，由于本实验设计为决策树的概念，所以回归树的部分有兴趣的同学可以自己查找相关资料进一步学习。 CART决策树的构建和常见的 ID3 和 C4.5 算法的流程相似，但在特征划分选择上CART选择了 基尼指数 作为划分标准。数据集 `D` 的纯度可用基尼值来度量： Gini(D) = \\sum_{y=1}^{\\left|y \\right|}\\sum_{k'\\neq k}^{}p_{k}p_{k}'\\tag{7}基尼指数表示随机抽取两个样本，两个样本类别不一致的概率，基尼指数越小则数据集的纯度越高。同样对于每一个特征值的基尼指数计算，其和 ID3 、 C4.5 相似，定义为： GiniValue(D,a)=\\sum_{m=1}^{M}\\frac{\\left |D^{m} \\right |}{\\left |D \\right |}Gini(D^{m}) \\tag{8}在进行特征划分的时候，选择特征中基尼值最小的作为最优特征划分点。 实际上，在应用过程中，更多的会使用 基尼指数 对特征划分点进行决策，最重要的原因是计算复杂度相较于 ID3 和 C4.5 小很多（没有对数运算）。 拓展阅读： 决策树- 维基百科","link":"/2019/02/01/Decision_Tree/"},{"title":"机器学习|划分聚类之 K-Means 详解 (Python 语言描述)","text":"划分聚类，顾名思义，通过划分的方式将数据集划分为多个不重叠的子集（簇），每一个子集作为一个聚类（类别）。 在划分的过程中，首先由用户确定划分子集的个数 `k`，然后随机选定 `k` 个点作为每一个子集的中心点，接下来通过迭代的方式：计算数据集中每个点与各个中心点之间的距离，更新中心点的位置；最终将数据集划分为 `k` 个子集，即将数据划分为 `k` 类。 而评估划分的好坏标准就是：保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大。 K-Means 聚类方法在划分聚类中，K-Means 是最具有代表性的算法，下面用图片的方式演示 K-Means 的基本算法流程。希望大家能通过简单的图文演示，对 K-Means 方法的原理过程产生大致的印象。 [1] 对于未聚类数据集，首先随机初始化 K 个（代表拟聚类簇个数）中心点，如图红色五角星所示。 [2] 每一个样本按照距离自身最近的中心点进行聚类，等效于通过两中心点连线的中垂线划分区域。 [3] 依据上次聚类结果，移动中心点到个簇的质心位置，并将此质心作为新的中心点 [4] 反复迭代，直至中心点的变化满足收敛条件（变化很小或几乎不变化），最终得到聚类结果。 在对 K-Means 有了一个直观了解后，下面我们用 Python 来进行实现。 生成示例数据首先通过 scikit-learn 模块的 make_blobs() 函数生成本次实验所需的示例数据。该方法可以按照我们的要求，生成特定的团状数据。 1data,label = sklearn.datasets.make_blobs(n_samples=100,n_features=2,centers=3,center_box=(-10.0,10.0),random_state=None) 其中参数为： n_samples：表示生成数据总个数,默认为 100 个。 n_features：表示每一个样本的特征个数，默认为 2 个。 centers：表示中心点的个数，默认为 3 个。 center_box：表示每一个中心的边界,默认为 -10.0到10.0。 random_state：表示生成数据的随机数种子。 返回值为： data：表示数据信息。 label：表示数据类别。 根据上面函数，在 0.0 到 10.0 上生成 200 条数据，大致包含 3 个中心。由于是用于演示聚类效果，数据标签就不是必须的了，在生成数据时赋值给 _，后面也不会使用到。 123456\"\"\"构造数据\"\"\"from sklearn.datasets import make_blobsblobs, _ = make_blobs(n_samples=200, centers=3, random_state=18)blobs[:10] # 打印出前 10 条数据的信息 array([[ 8.28390539, 4.98011149], [ 7.05638504, 7.00948082], [ 7.43101466, -6.56941148], [ 8.20192526, -6.4442691 ], [ 3.15614247, 0.46193832], [ 7.7037692 , 6.14317389], [ 5.62705611, -0.35067953], [ 7.53828533, -4.86595492], [ 8.649291 , 3.98488194], [ 7.91651636, 4.54935348]]) 数据可视化为了更加直观的查看数据分布情况，使用 matplotlib 将生成数据绘画出来。 123456\"\"\"数据展示\"\"\"%matplotlib inlineimport matplotlib.pyplot as pltplt.scatter(blobs[:, 0], blobs[:, 1], s=20); 随机初始化中心点当我们得到数据时，依照划分聚类方法的思想，首先需要随机选取 `k` 个点作为每一个子集的中心点。从图像中，通过肉眼很容易的发现该数据集有 3 个子集。接下来，用 numpy 模块随机生成 3 个中心点，为了更方便展示，这里我们加入了随机数种子以便每一次运行结果相同。 123456789101112131415161718192021\"\"\"初始化中心点\"\"\"import numpy as npdef random_k(k, data): \"\"\" 参数: k -- 中心点个数 data -- 数据集 返回: init_centers -- 初始化中心点 \"\"\" prng = np.random.RandomState(27) # 定义随机种子 num_feature=np.shape(data)[1] init_centers = prng.randn(k, num_feature)*5 # 由于初始化的随机数是从-1到1，为了更加贴近数据集这里乘了一个 5 return init_centersinit_centers=random_k(3, blobs)init_centers array([[ 6.42802708, -1.51776689], [ 3.09537831, 1.97999275], [ 1.11702824, -0.27169709]]) 在随机生成好中心点之后，将其在图像中表示出来，这里同样使用红色五角星表示。 1234\"\"\"初始中心点展示\"\"\"plt.scatter(blobs[:, 0], blobs[:, 1], s=20);plt.scatter(init_centers[:,0], init_centers[:,1], s=100, marker='*', c=\"r\") 计算样本与中心点的距离为了找到最合适的中心点位置，需要计算每一个样本和中心点的距离，从而根据距离更新中心点位置。常见的距离计算方法有欧几里得距离和余弦相似度，本实验采用更常见且更易于理解的欧几里得距离（欧式距离）。 欧式距离源自 `N` 维欧氏空间中两点之间的距离公式。表达式如下: d_{euc}= \\sqrt{\\sum_{i=1}^{N}(X_{i}-Y_{i})^{2}} \\tag{1}其中： `X`, `Y` ：两个数据点 `N`：每个数据中有 `N` 个特征值， `X_{i}` ：数据 `X` 的第 `i` 个特征值 将两个数据 `X` 和 `Y` 中的每一个对应的特征值之间差值的平方，再求和，最后开平方，便是欧式距离。 12345678910111213\"\"\"计算欧氏距离\"\"\"def d_euc(x, y): \"\"\" 参数: x -- 数据 a y -- 数据 b 返回: d -- 数据 a 和 b 的欧氏距离 \"\"\" d = np.sqrt(np.sum(np.square(x - y))) return d 最小化 SSE，更新聚类中心和第一章的回归算法通过减小目标函数（如：损失函数）的值拟合数据集一样，聚类算法通常也是优化一个目标函数，从而提高聚类的质量。在聚类算法中，常常使用误差的平方和 SSE（Sum of squared errors）作为度量聚类效果的标准，当 SSE 越小表示聚类效果越好。其中 SSE 表示为： SSE(C)=\\sum_{k=1}^{K}\\sum_{x_{i}\\in C_{k}}\\left \\| x_{i}-c_{k} \\right \\|^{2} \\tag{2}其中数据集 \\(D=\\{ x_{1},x_{2},…,x_{n} \\}\\)，\\(x_{i}\\)表示每一个样本值，`C` 表示通过 K-Means 聚类分析后的产生类别集合 \\(C=\\{ C_{1},C_{2},…,C_{K} \\}\\) ，\\(c_{k}\\) 是类别 \\(C_{k}\\) 的中心点，其中 \\(c_{k}\\) 计算方式为： c_{k}=\\frac{\\sum_{x_{i} \\in C_{k}}x_{i}}{I(C_{k})} \\tag{3}\\(I(C_{k})\\) 表示在第 `k` 个集合 \\(C_{k}\\) 中数据的个数。 当然，我们希望同最小化损失函数一样，最小化 SSE 函数，从而找出最优化的聚类模型，但是求其最小值并不容易，是一个 NP 难（非确定性多项式）的问题，其中 NP 难问题是一个经典图论问题，至今也没有找到一个完美且有效的算法。 下面我们对中心点的更新用代码的方式进行实现： 1234567891011121314151617181920212223242526272829303132\"\"\"中心点的更新\"\"\"def update_center(clusters, data, centers): \"\"\" 参数: clusters -- 每一点分好的类别 data -- 数据集 centers -- 中心点集合 返回: new_centers.reshape(num_centers,num_features) -- 新中心点集合 \"\"\" num_centers = np.shape(centers)[0] # 中心点的个数 num_features = np.shape(centers)[1] # 每一个中心点的特征数 container = [] for x in range(num_centers): each_container = [] container.append(each_container) # 首先创建一个容器,将相同类别数据存放到一起 for i, cluster in enumerate(clusters): container[cluster].append(data[i]) # 为方便计算，将 list 类型转换为 np.array 类型 container = np.array(list(map(lambda x: np.array(x), container))) new_centers = np.array([]) # 创建一个容器，存放中心点的坐标 for i in range(len(container)): each_center = np.mean(container[i], axis=0) # 计算每一子集中数据均值作为中心点 new_centers = np.append(new_centers, each_center) return new_centers.reshape(num_centers, num_features) # 以矩阵的方式返回中心点坐标 K-Means 聚类算法实现K-Means 算法则采用的是迭代算法，避开优化 SSE 函数，通过不断移动中心点的距离，最终达到聚类的效果。 算法流程 初始化中心点：判断数据集可能被分为 `k` 个子集，随机生成 `k` 个随机点作为每一个子集的中心点。 距离计算，类别标记：样本和每一个中心点进行距离计算，将距离最近的中心点所代表的类别标记为该样本的类别。 中心点位置更新：计算每一个类别中的所有样本的均值，作为新的中心点位置。 重复 2，3 步骤，直到中心点位置不再变化。 算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748\"\"\"K-Means 聚类\"\"\"def kmeans_cluster(data, init_centers, k): \"\"\" 参数: data -- 数据集 init_centers -- 初始化中心点集合 k -- 中心点个数 返回: centers_container -- 每一次更新中心点的集合 cluster_container -- 每一次更新类别的集合 \"\"\" max_step = 50 # 定义最大迭代次数，中心点最多移动的次数。 epsilon = 0.001 # 定义一个足够小的数，通过中心点变化的距离是否小于该数，判断中心点是否变化。 old_centers = init_centers centers_container = [] # 建立一个中心点容器，存放每一次变化后的中心点，以便后面的绘图。 cluster_container = [] # 建立一个分类容器，存放每一次中心点变化后数据的类别 centers_container.append(old_centers) for step in range(max_step): cluster = np.array([], dtype=int) for each_data in data: distances = np.array([]) for each_center in old_centers: temp_distance = d_euc(each_data, each_center) # 计算样本和中心点的欧式距离 distances = np.append(distances, temp_distance) lab = np.argmin(distances) # 返回距离最近中心点的索引，即按照最近中心点分类 cluster = np.append(cluster, lab) cluster_container.append(cluster) new_centers = update_center(cluster, data, old_centers) # 根据子集分类更新中心点 # 计算每个中心点更新前后之间的欧式距离 difference = [] for each_old_center, each_new_center in zip(old_centers, new_centers): difference.append(d_euc(each_old_center, each_new_center)) if (np.array(difference) &lt; epsilon).all(): # 判断每个中心点移动是否均小于 epsilon return centers_container, cluster_container centers_container.append(new_centers) old_centers = new_centers return centers_container, cluster_container 完成 K-Means 聚类函数后，接下来用函数得到最终中心点的位置。 123456\"\"\"计算最终中心点\"\"\"centers_container, cluster_container = kmeans_cluster(blobs, init_centers, 3)final_center = centers_container[-1]final_cluster = cluster_container[-1]final_center array([[ 7.67007252, -6.44697348], [ 6.83832746, 4.98604668], [ 3.28477676, 0.15456871]]) 最后，我们把聚类得到的中心绘制到原图中看一看聚类效果。 1234\"\"\"可视化展示\"\"\"plt.scatter(blobs[:, 0], blobs[:, 1], s=20, c=final_cluster);plt.scatter(final_center[:,0], final_center[:,1], s=100, marker='*', c=\"r\") 中心点移动过程可视化截止上小节，已经完成了 K-Means 聚类的流程。为了帮助大家理解，我们尝试将 K-Means 聚类过程中，中心点移动变化的过程绘制出来。 1234567891011121314151617num_axes = len(centers_container)fig, axes = plt.subplots(1, num_axes, figsize=(20, 4))axes[0].scatter(blobs[:, 0], blobs[:, 1], s=20, c=cluster_container[0])axes[0].scatter(init_centers[:, 0], init_centers[:, 1], s=100, marker='*', c=\"r\")axes[0].set_title(\"initial center\")for i in range(1, num_axes-1): axes[i].scatter(blobs[:, 0], blobs[:, 1], s=20, c=cluster_container[i]) axes[i].scatter(centers_container[i][:, 0], centers_container[i][:, 1], s=100, marker='*', c=\"r\") axes[i].set_title(\"step {}\".format(i))axes[-1].scatter(blobs[:, 0], blobs[:, 1], s=20, c=cluster_container[-1])axes[-1].scatter(final_center[:, 0], final_center[:, 1], s=100, marker='*', c=\"r\")axes[-1].set_title(\"final center\") 你会惊讶的发现，对于示例数据集，虽然我们先前将最大迭代次数 max_step 设为了 50，但实际上 K-Means 迭代 3 次即收敛。原因主要有 2 点： 初始化中心点的位置很好，比较均匀分布在了数据范围中。如果初始化中心点集中分布在某一角落，迭代次数肯定会增加。 示例数据分布规整和简单，使得无需迭代多次就能收敛。 K-Means 算法聚类中的 K 值选择不知道你是否还记得，前面在学习分类算法 K-近邻的时候，我们讲到了 K 值的选择。而在使用 K-Means 算法聚类时，由于要提前确定随机初始化中心点的数量，同样面临着 K 值选择问题。 在前面寻找 K 值时，我们通过肉眼观察认为应该聚为 3 类。那么，如果我们设定聚类为 5 类呢？ 这一次，我们尝试通过 scikit-learn 模块中的 K-Means 算法完成聚类。 123from sklearn.cluster import k_meansk_means(X, n_clusters) 其中参数为： X：表示需要聚类的数据。 n_clusters：表示聚类的个数，也就是 K 值。 返回值包含： centroid：表示中心点坐标。 label：表示聚类后每一个样本的类别。 inertia：每一个样本与最近中心点距离的平方和，即 SSE。 123456789\"\"\"用 scikit-learn 聚类并绘图\"\"\"from sklearn.cluster import k_meansmodel = k_means(blobs, n_clusters=5)centers = model[0]clusters_info = model[1]plt.scatter(blobs[:, 0], blobs[:, 1], s=20, c=clusters_info)plt.scatter(centers[:, 0], centers[:, 1], s=100, marker='*', c=\"r\") 从图片上来看，聚为 5 类效果明显不如聚为 3 类的好。当然，我们提前用肉眼就能看出数据大致为 3 团。 实际的应用过程中，如果通过肉眼无法判断数据应该聚为几类？或者是高维数据无法可视化展示。面对这样的情况，我们就要从数值计算的角度去判断 K 值的大小。 接下来，将介绍一种启发式学习算法，被称之为 肘部法则，可以帮助我们选取 K 值。 使用 K-Means 算法聚类时，我们可以计算出按不同 K 值聚类后，每一个样本距离最近中心点距离的平方和 SSE。 随着 K 值增加时，也就是类别增加时，每个类别中的类内相似性也随之增加，由此造成的 SSE 的变化是单调减小的。可以想象一下，聚类类别的数量和样本的总数相同时，也就是说一个样本就代表一个类别时，这个数值会变成 0。 下面我们通过代码将不同的数量的聚类下，样本和最近中心点的距离和绘制出来。 1234567891011index = [] # 横坐标数组inertia = [] # 纵坐标数组# K 从 1~ 6 聚类for i in range(6): model = k_means(blobs, n_clusters=i + 1) index.append(i + 1) inertia.append(model[2])# 绘制折线图plt.plot(index, inertia, \"-o\") 通过上图可以看到，和预想的一样，样本距离最近中心点距离的总和会随着 K 值的增大而降低。 现在，回想本实验划分聚类中所讲评估划分的好坏标准：「保证同一划分的样本之间的差异尽可能的小，且不同划分中的样本差异尽可能的大」。 当 K 值越大时，越满足「同一划分的样本之间的差异尽可能的小」。而当 K 值越小时，越满足「不同划分中的样本差异尽可能的大畸变程度最大」。那么如何做到两端的平衡呢？ 于是，我们通过 SSE 所绘制出来的图，将畸变程度最大的点称之为「肘部」。从图中可以看到，这里的「肘部」是 K = 3（内角最小，弯曲度最大）。这也说明，将样本聚为 3 类是最佳选择（K = 2 比较接近）。这就是所谓的「肘部法则」，你明白了吗？ K-Means++ 聚类算法问题引入随着数据量的增长，分类数目增多时，由于 K-Means 中初始化中心点是随机的，常常会出现：一个较大子集有多个中心点，而其他多个较小子集公用一个中心点的问题。即算法陷入局部最优解而不是达到全局最优解的问题。 造成这种问题主要原因就是：一部分中心点在初始化时离的太近。下面我们通过例子来进一步了解。 生成示例数据同样，我们先使用 scikit-learn 模块的 make_blobs 函数生成本次实验所需数据，本次生成 800 条数据，共 5 堆。 12345\"\"\"生成数据并展示\"\"\"blobs_plus, _ = make_blobs(n_samples=800, centers=5, random_state=18) # 生成数据plt.scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20) # 将数据可视化展示 随机初始化中心点从数据点分布中可以很容易的观测出聚类数量应该为 5 类，我们先用 K-Means 中随机初始中心点的方法完成聚类： 1234km_init_center=random_k(5, blobs_plus)plt.scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20);plt.scatter(km_init_center[:,0], km_init_center[:,1], s=100, marker='*', c=\"r\") K-Means 聚类用传统的 K-Means 算法，将数据集进行聚类，聚类数量为 5。 12345km_centers, km_clusters = kmeans_cluster(blobs_plus, km_init_center, 5)km_final_center = km_centers[-1]km_final_cluster = km_clusters[-1]plt.scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20, c=km_final_cluster)plt.scatter(km_final_center[:, 0], km_final_center[:, 1], s=100, marker='*', c=\"r\") 通过传统 K-Means 算法聚类后，你会发现聚类效果和我们预想不同，我们预想的结果应该是下面这样的： 1plt.scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20, c=_) 对比 K-Means 聚类和预想聚类的两张图，可以直观的看出 K-Means 算法显然没有达到最优的聚类效果，出现了本章开头所提到的局部最优解的问题。 对于局部最优问题是可以通过 SSE 来解决的，即在同一数据集上运行多次 K-Means 算法聚类，之后选取 SSE 最小的那次作为最终的聚类结果。虽然通过 SSE 找到最优解十分困难，但通过 SSE 判断最优解是十分容易的。 但当遇到更大的数据集，每一次 K-Means 算法会花费大量时间时，如果使用多次运行通过 SSE 来判断最优解，显然不是好的选择。是否有一种方法在初始化中心点时，就能有效避免局部最优问题的出现呢？ 在 K-Means 的基础上，D.Arthur 等人在 2007 年提出了 K-Means++ 算法。其中 K-Means++ 算法主要针对初始化中心点问题进行改进，这样就可以从源头上解决局部最优解的问题。 K-Means++ 算法流程K-Means++ 相较于 K-Means 在初始化中心点上做了改进，在其他方面和 K-Means 相同。 在数据集中随机选择一个样本点作为第一个初始化的聚类中心。 计算样本中的非中心点与最近中心点之间的距离 `D(x)` 并保存于一个数组里，将数组中的这些距离加起来得到 \\(Sum(D(x))\\)。 取一个落在 \\(Sum(D(x))\\)范围中的随机值 `R` ，重复计算 \\(R=R-D(x)\\) 直至得到 \\(R\\leq0\\) ，选取此时的点作为下一个中心点。 重复 2,3 步骤，直到 `K` 个聚类中心都被确定。 对 `K` 个初始化的聚类中心，利用 K-Means 算法计算最终的聚类中心。 看完整个算法流程，可能会出现一个疑问：为避免初始点距离太近，直接选取距离最远的点不就好了，为什么要引入一个随机值 `R` 呢？ 其实当采用直接选取距离最远的点作为初始点的方法，会容易受到数据集中离群点的干扰。采用引入随机值 `R` 的方法避免数据集中所包含的离群点对算法思想中要选择相距最远的中心点的目标干扰。 相对于正常的数据点，离群点所计算得出的 `D(x)` 距离一定比较大，这样在选取的过程中，它被选中的概率也就相对较大，但是离群点在整个数据集中只占一小部分，大部分依然是正常的点，这样离群点由于距离大而造成的概率大，就被正常点的数量大给平衡掉。从而保证了整个算法的平衡性。 K-Means++ 算法实现K-Means++ 在初始化样本点之后，计算其他样本与其最近的中心点距离之和，以备下一个中心点的选择，下面用 Python 来进行实现： 1234567891011121314151617181920def get_sum_dis(centers, data): \"\"\" 参数: centers -- 中心点集合 data -- 数据集 返回: np.sum(dis_container) -- 样本距离最近中心点的距离之和 dis_container -- 样本距离最近中心点的距离集合 \"\"\" dis_container = np.array([]) for each_data in data: distances = np.array([]) for each_center in centers: temp_distance = d_euc(each_data, each_center) # 计算样本和中心点的欧式距离 distances = np.append(distances, temp_distance) lab = np.min(distances) dis_container = np.append(dis_container, lab) return np.sum(dis_container), dis_container 接下来，我们初始化中心点： 12345678910111213141516171819202122232425262728293031\"\"\"K-Means++ 初始化中心点\"\"\"def get_init_center(data, k): \"\"\" 参数: data -- 数据集 k -- 中心点个数 返回: np.array(center_container) -- 初始化中心点集合 \"\"\" seed = np.random.RandomState(20) p = seed.randint(0, len(data)) first_center = data[p] center_container = [] center_container.append(first_center) for i in range(k-1): sum_dis, dis_con = get_sum_dis(center_container, data) r = np.random.randint(0, sum_dis) for j in range(len(dis_con)): r = r - dis_con[j] if r &lt;= 0: center_container.append(data[j]) break else: pass return np.array(center_container) 实现 K-Means++ 初始化中心点函数之后，根据生成数据，得到初始化的中心点坐标。 12plus_init_center = get_init_center(blobs_plus, 5)plus_init_center array([[ 4.1661903 , 0.81807492], [ 8.9161603 , 5.58757202], [ 7.62699601, 2.3492678 ], [-3.42049424, -9.57117787], [ 3.35681598, -0.54000802]]) 为了让你更清晰的看到 K-Means++ 初始化中心点的过程，我们用 matplotlib 进行展示。 1234567891011121314151617num = len(plus_init_center)fig, axes = plt.subplots(1, num, figsize=(25, 4))axes[0].scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20, c=\"b\")axes[0].scatter(plus_init_center[0, 0], plus_init_center[0, 1], s=100, marker='*', c=\"r\")axes[0].set_title(\"first center\")for i in range(1, num): axes[i].scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20, c=\"b\") axes[i].scatter(plus_init_center[:i+1, 0], plus_init_center[:i+1, 1], s=100, marker='*', c=\"r\") axes[i].set_title(\"step{}\".format(i))axes[-1].scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20, c=\"b\")axes[-1].scatter(plus_init_center[:, 0], plus_init_center[:, 1], s=100, marker='*', c=\"r\")axes[-1].set_title(\"final center\") 通过上图可以看到点的变化，即除了最初随机选择点之外，之后的每一个点都是尽可能选择远一些的点。这样就很好的保证初始中心点的分散。 通过多次执行代码可以看到，使用 K-Means++ 同样可能出现两个中心点较近的情况，因此，在极端情况也可能出现局部最优的问题。但相比于 K-Means 算法的随机选取，K-Means++ 的初始化中心点会在很大程度上降低局部最优问题出现的概率。 在通过 K-Means++ 算法初始化中心点后，下面我们通过 K-Means 算法对数据进行聚类。 123456plus_centers, plus_clusters = kmeans_cluster(blobs_plus, plus_init_center, 5)plus_final_center = plus_centers[-1]plus_final_cluster = plus_clusters[-1]plt.scatter(blobs_plus[:, 0], blobs_plus[:, 1], s=20, c=plus_final_cluster)plt.scatter(plus_final_center[:, 0], plus_final_center[:, 1], s=100, marker='*', c=\"r\") 在 K-Means++ 算法中，我们依旧无法完全避免随机选择中心点带来的不稳定性，所以偶尔也会得到不太好的结果。当然，K-Means++ 算法得到不太好的聚类的概率远小于 K-Means 算法。所以，如果你并没有得到一个较好的聚类效果，可以再次初始化中心点尝试。 Mini-Batch K-Means 聚类算法在「大数据」如此火的时代，K-Means 算法是否还能一如既往优秀的处理大数据呢？现在我们重新回顾下 K-Means 的算法原理：首先，计算每一个样本同所有中心点的距离，通过比较找到最近的中心点，将距离最近中心点的距离进行存储并归类。然后通过相同类别样本的特征值，更新中心点的位置。至此完成一次迭代，经过多次迭代后最终进行聚类。 通过上面的表述，你是否感觉到不断计算距离的过程，涉及到的计算量有多大呢？那么，设想一下数据量达到十万，百万，千万级别，且如果每一条数据有上百个特征，这将会消耗大量的计算资源。 为了解决大规模数据的聚类问题，我们就可以使用 K-Means 的另外一个变种 Mini Batch K-Means 来完成。 其算法原理也十分简单：在每一次迭代过程中，从数据集中随机抽取一部分数据形成小批量数据集，用该部分数据集进行距离计算和中心点的更新。由于每一次都是随机抽取，所以每一次抽取的数据能很好的表现原本数据集的特性。 下面，我们生成一组测试数据，并测试 K-Means 算法和 Mini Batch K-Means 在同一组数据上聚类时间和 SSE 上的差异。由于 scikit-learn 中 MiniBatchKMeans() 和 KMeans() 方法的参数几乎一致，这里就不再赘述了。 1234567891011121314151617181920212223import timefrom sklearn.cluster import MiniBatchKMeans, KMeanstest_data, _ = make_blobs(2000, n_features=2, cluster_std=2, centers=5)km = KMeans(n_clusters=5)mini_km = MiniBatchKMeans(n_clusters=5)fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))for i, model in enumerate([km, mini_km]): t0 = time.time() model.fit(test_data) t1 = time.time() t = t1 - t0 sse = model.inertia_ axes[i].scatter(test_data[:, 0], test_data[:, 1], c=model.labels_) axes[i].set_xlabel(\"time: {:.4f} s\".format(t)) axes[i].set_ylabel(\"SSE: {:.4f}\".format(sse))axes[0].set_title(\"K-Means\")axes[1].set_title(\"Mini Batch K-Means\") 以上是对 2000 条数据分别用 K-Means 和 Mini Batch K-Means进行聚类，从图像中可以看出，Mini Batch K-Means 在训练时间上明显比 K-Means 快（大于 2 倍不等），且聚类得到的 SSE 值比较接近。 拓展阅读： K-平均算法- 维基百科 The 5 Clustering Algorithms Data Scientists Need to Know Visualizing K-Means Clustering","link":"/2019/02/09/Cluster_Of_K-Means/"},{"title":"深度学习|卷积神经网络概述","text":"实现简单手写识别完整代码地址 ：👉 https://github.com/laugh12321/Handwriting-Recognition 提到人工智能，人们都会希望自己的智能机器具备「听说读写」这样像人一样的基本感知表达能力，计算机视觉就是研究如何让机器学会「看」的学科。如今，借助深度学习的推动，计算机视觉已经来到了一个飞速发展的时代。人脸识别，自动驾驶，医学图像分析，计算机视觉的成熟让这一切变得可能甚至远高于我们人类的工作效率。在计算机视觉领域，卷积神经网络发挥了重要价值。其实它的思想来源很简单，就像神经网络通过模拟生物神经元工作的思路创建，卷积神经网络也在模拟我们的视觉。1959 年，Hubel 和 Wiesel 的实验表明，生物的视觉处理是从简单的形状开始的，比如边缘、直线、曲线。凭借这一发现，Hubel 和 Wiesel 获得了1981 年的诺贝尔生理学或医学奖。卷积神经网络在逐层学习过程中，也模拟着这个过程，第一层学习比较低层的图片结构，第二层学习依据第一层的结果学习高一级的特征，这样最后一层依据前一层学到的高级特征就能完成我们的学习任务。 ©️ 图片来源 比如识别人脸，第一层可能学到了简单的线条，第二层学到了轮廓，倒数第二层学到了双眼皮和耳垂，最后一层识别出了她是你的高中班主任。可以说，卷积神经网络现在主宰了计算机视觉领域，通过灵活运用卷积神经网络的结构，会让我们的机器「看」得更多、更清楚。 卷积神经网络一般是由卷积层、池化层和全连接层堆叠而成的前馈神经网络结构。与前面讲到的前馈神经网络相似，卷积神经网络同样使用反向传播算法进行训练。 上图展示了一个基础的卷积神经网络结构。当然，我们通常使用到的网络结构更深、更复杂，一般是由许多卷积层、池化层和全连接层交叉堆叠而成。 接下来的实验内容将带你逐步了解卷积层和池化层的概念，并构建一个深度卷积神经网络。 卷积层 Convolution Layer现在我们来逐步深入了解卷积神经网络。首先，你需要了解一张图片在计算机眼中的样子。 下图表述了像素矩阵的概念。具体来讲，我们可能看到的是一张图的整体视觉效果，但从计算机中读取图片，图片是通过像素矩阵标识的。每一个像素具有一个数值（0-255），代表图片在该信道像素点位置的大小。 ©️ 图片来源 信道这个概念，大家可以理解为颜色。我们常用的 RGB 信道分别代表红绿蓝三种。通过把三个信道的像素矩阵叠加起来，就可以组成我们看到的色彩斑斓的图片了。 所以在计算机中，图片是一个 \\(m\\times n\\times k\\) 的三维矩阵，其中 `m` 是矩阵的高度，`n` 是矩阵的宽度，`k` 是矩阵的深度。基于这样的数据结构，接下来可以进行卷积运算了。 卷积核 Convolution Kernel接下来，我们首先需要了解卷积核的概念。如下图所示，我们现在有一个图像矩阵，同时定义了一个卷积核（权值矩阵）。 其中，卷积核的作用就是从输入矩阵（图像）中提取一定的特征。动画如下： ©️ 图片来源 上面的动画过程，其实就是一个卷积的过程，不知你是否能一眼就看明白？ 简单来讲，我们将卷积核沿着图像矩阵的左上角开始移动，不断运算得到右边的矩阵。卷积计算的过程非常简单，就是乘法和加法的运算。例如，当我们第一次把卷积核置于图像矩阵的左上角时： 1 \\times 0 + 2 \\times 1 + 2 \\times 1 + 1 \\times 2 = 6通过不断平移卷积核，同时与输入矩阵进行卷积操作，就可以得到卷积后矩阵。 如何理解卷积的过程，最简单的就是把卷积核看作是一片滤镜，原矩阵通过滤镜之后就得到了新的特征矩阵。 如果你想形象一点了解卷积核究竟在做一件什么事情，就可以把卷积后的图片与原图对比呈现。下图就是一张图片通过不同卷积核做卷积操作后的结果，我们可以明显察觉到不同的卷积核可以帮助我们分解出原图片不同层次的特征。 所以，如果卷积核能够帮助分解图片特征并运用到机器学习的过程中，例如识别一辆车能看到四个轮子以及车窗的细节，你可以想象这将会对提升准确率有极大帮助。 卷积步长 Stride上面的卷积过程相信你已经看明白了。这时候有一个问题，如果我们在对图片卷积时都依次移动，效率会不会比较低？ 是的，对于一些很大的图片，每次只移动一步运算效率会很低下，同时分解得到的特征也很容易冗余。于是，可以引入一个超参数能对每一次移动的步数进行调节，我们把它称之为卷积步长 Stride。 同样，我们通过一组动图来查看不同卷积步长 Stride 的移动效果： 当卷积步长为 1 时，也就是和上面相同的移动过程： ©️ 图片来源 此时，输出矩阵大小为： output = (input - kernel) + 1如果我们将卷积步长设为 2，就是下面的效果： ©️ 图片来源 可以发现，当 Stride=2 横向移动以及纵向移动时，每一步都跳过了一个单元格。也就是从移动 1 步变成了 2 步。 此时，输出矩阵大小为（向下取整）： output = \\lfloor \\frac{input - kernel}{stride} \\rfloor + 1边距扩展 Padding介绍了步长的概念，接下来再介绍一个卷积操作过程中的重要概念，也就是 Padding。Padding 也就是边距，当然我们一般都直接使用英文。步长解决了卷积效率低的问题，但又造成了新的问题。你会发现，随着步长的增大，卷积输出的矩阵将持续变小。但是，在构建网络的时候我们往往希望输出矩阵的大小为指定大小，而不完全由卷积步长的变换而左右。于是，就有了 Padding 的操作。 一旦我们确定了原矩阵的大小和步长的大小，得到的卷积矩阵大小将是确定的。假如我们希望卷积得到的矩阵大小变大，在不改变卷积核大小和步长的前提下，唯一的方法就是调整原矩阵的大小。于是，我们就通过对原矩阵进行 Padding 操作（扩大边距）达到目的。 Padding 比较常见的操作是在输入矩阵最外围补上一圈 0 之后，再参与卷积运算。这样，我们就可以自行控制输出矩阵的大小。下面列举几个常见的 Padding 方式： Arbitrary Padding 在输入图像周围填充 0，使输出矩阵尺寸大于输入尺寸。例如，下图对 \\(5\\times5\\) 的输入矩阵外围补上 2 圈 0 后，与 \\(4\\times4\\) 的卷积核做卷积运算，最终得到 \\(6\\times6\\) 的输出矩阵的过程。 ©️ 图片来源 此时，输出矩阵大小为： output = (input - kernel) + 2 * padding + 1Half Padding 也叫 Same Padding，它希望得到的输出矩阵尺寸与输入尺寸一致。下图即为我们对 \\(5\\times5\\) 的输入矩阵外围补上一圈 0 后，与 \\(3\\times3\\) 的卷积核做卷积运算，最终依然得到 \\(5\\times5\\) 的输出矩阵的过程。 ©️ 图片来源 还有一种是 Full Padding，卷积核从输入矩阵左角第一个方块开始，依次移动到右下角的最后一个方块。 ©️ 图片来源 其实，Padding 并不是说只有这几种，还有其他的一些形式，包括 Padding 和 Stride 结合在一起的一些卷积形式，甚至你可以自定义 Padding 和 Stride 的规则，这里也就不再一一介绍了。 高维多卷积核过程前面，我们已经介绍了矩阵通过单个卷积核的过程，相信你已经对卷积核、步长、Padding 等概念非常熟悉了。 一张黑白图片形成了单个像素矩阵，但如果是一张用 RGB 通道的彩色图片，将会形成 3 维的像素矩阵。这该怎么办呢？ 对于一个 \\(m\\times n\\times k\\) 的三维矩阵，它是如何在这种二维结构的卷积下操作的呢？其实，在卷积神经网络设计上有一个重要原则，就是卷积核也是一个三维 \\(a\\times b \\times k\\) 的矩阵(通常 \\(a=b\\)，且常取 1，3，5，7 这样的数)。更重要的是，一定要保证输入矩阵大小与卷积核大小在第三个维度上值相等。 这样，我们在第三个维度上对相同深度的输入矩阵与卷积核做如上的卷积运算时，最后叠加所有深度的值得到一个二维的输出矩阵。也就是说，对 \\(m\\times n\\times k\\) 的输入矩阵与 \\(a\\times b \\times k\\) 的卷积核，我们如果做一个 \\(stride=s\\)，\\(padding=p\\) 的卷积操作，那么将会得到一个 \\([(m+2p-a)/s+1]\\times [(n+2p-a)/s+1]\\) 的二维矩阵。 那么如何保证输出矩阵依然是一个三维矩阵呢？其实，只需要每一层都由多个卷积核来实现操作就行了，最后把得到的所有二维矩阵叠起来，就得到了一个 \\([(m+2p-a)/s+1]\\times [(n+2p-a)/s+1]\\times c\\)，\\(c\\) 为卷积核的个数的输出矩阵。 整个过程可以由下图更直观地表示，一个 \\(5\\times5\\times3\\) 的输入矩阵被两个 \\(3\\times3\\times3\\) 的卷积核执行 \\(stride = 2\\)，\\(padding = 1\\) 卷积后，得到了 \\(3\\times3\\times2\\) 的输出矩阵： ©️ 图片来源 以上就是一个高维矩阵通过多卷积核的执行过程。 池化层 Pooling Layer上面介绍了卷积层，接着介绍卷积神经网络中另一个非常重要的层，也就是池化层。池化操作其实就是降采样操作过程。我们都知道，图片是一个非常大的训练数据，所以想达到很好的训练性能，只有卷积层是不够的。池化层通过降采样的方式，在不影响图像质量的情况下，压缩图片，达到减少训练参数的目的。 需要注意的是，往往在几个卷积层之后我们就会引入池化层，池化层没有需要学习的参数，且池化层在每一个深度上独立完成，就是说池化后图像的纵深保持不变。下面介绍两种常用的池化方式。 最大值池化对于池化操作，我们需要界定一个过滤器和步长。最大值池化，取输入矩阵在过滤器大小上矩阵块的最大值作为输出矩阵对应位置输出。如下图： 平均值池化除了最大值池化，另一个常用的池化方法是平均值池化。顾名思义，这是取输入矩阵在过滤器大小上矩阵块的平均值作为输出矩阵对应位置输出。如下图： 相比于卷积，池化的过程就更加简单了。相信你通过上面的示意图能很清楚的看出最大池化和平均池化的过程。 CNN 实现数字手写识别的完整代码可以在我的 GitHub 上查看 ： 👉 https://github.com/laugh12321/Handwriting-Recognition/blob/master/CNN_TF.ipynb 经典的卷积神经网络深度学习领域有很多的技巧需要大量的探索与实践。比如对于一个人脸识别任务，我们搭建的网络具有很强的多样性。大量数据集和多次实验结果下，有些经典的神经网络孕育而生。这些网络从诞生时到现在都有着独特的优越性能和革命性的创新思想。所以，建议大家在深度学习的领域学习中，熟悉这些经典神经网络的架构，这将进一步深化我们对深度神经网络的理解。 实际上，在对神经网络的理论探索中，尚不具有十分充分的理论证明为什么一些经典网络有如此强大的性能。当然，深度学习还有很多未得到解释的地方，包括神经网络每一步究竟在学习什么，每个参数是如何影响学习性能或者迭代收敛过程，这些问题都有待进一步探寻。所以说，深度神经网络的调参过程充满了「玄学」的感觉，这需要大量的实践积累。 LeNet 神经网络LeNet-5 是一个很早的经典卷积神经网络。Yann LeCun 大牛（被称作卷积神经网络之父）在 1998 年搭建了 LeNet，并且在手写识别网络(MNIST)上广泛使用。LeNet 神经网络的架构如下图： ©️ 图片来源 其中，网络中的下采样采用了平均值池化。前面各层都由 tanh 激活，最后一层由 RBF 激活。经过测试，LeNet 在 6 万张原始图片的数据集中，错误率能降低到 0.95%。 LeNet 一经发布就带来了一波卷积神经网络潮流。很可惜，由于那个年代计算能力不强，使得卷积神经网络的多样性受到严重限制，研究者都不愿意将网络深化，卷积神经网络只掀起了一波热潮后又几乎无人问津。 AlexNet 神经网络2012 年 ImageNet 比赛上，一个卷积神经网络的表现力大放异彩，这就是 AlexNet。经过了十几年的硬件发展，计算机运算能力大幅增强，我们可运用的数据也大幅增加，AlexNet 由此孕育而生。 AlexNet 的结构很像 LeNet-5，但是网络变得更大更深。同时，AlexNet 是第一个将卷积层叠层后再叠上池化层的网络。不仅如此，AlexNet 运用上 Dropout（训练时，按照一定的概率将神经元暂时从网络中丢弃）和数据增强两种方法降低过拟合，以及 LRN(local responce normalization) 做为规划层来促使神经元学到更加广泛的特征。可以说，AlexNet 的出色表现，让计算机视觉领域开始逐渐由卷积神经网络主宰。 下面就是 AlexNet 的网络结构图： ©️ 图片来源 原作者在训练 AlexNet 时，一个 GPU 负责图中顶层部分，一个 GPU 运行图中底层部分，GPU 之间仅在某些层互相通信。5 个卷积层，5 个池化层，3 个全连接层，大约 5000 万个可调参数组成了这个经典的卷积神经网络。最后的全连接层输出到 1000 维的 softmax 层，产生一个覆盖 1000 类标记的分布。最终完成了对 ImageNet 的分类任务。 模仿 AlexNet 搭建一个卷积神经网络用于手写识别网络的完整代码可以在我的 GitHub 上查看 ： 👉 https://github.com/laugh12321/Handwriting-Recognition/blob/master/AlexNet_TF.ipynb VGG 神经网络VGG Net 可以看成是 Alex Net 的加深版本，同样运用 Alex Net 所带来的思路。VGG Net 拥有 5 个卷积组，2 层全图像连接特征和 1 层全连接分类特征。另外，Alex Net 只有 8 层，但是 VGG Net 通常有 16 到 19 层。 VGG Net 的革命性在于，它在论文中给出了不同的卷积层配置方法，而且从论文实验结果可以看出，随着卷积层从 8 到 16 一步步加深，通过加深卷积层数准确率已经达到了瓶颈数值。这项研究表明了单纯添加卷积层往往并不能起到更好的效果，这将促使以后的卷积神经网络朝向增强卷积层功能的思路发展。 下图描述了一种配置的 VGG 网络结构： ©️ 图片来源 Google Net 神经网络自 VGG Net 之后，卷积神经网络朝向另一个方向演化，那就是增强卷积模块的功能。 谷歌公司提出的 Google Net 可以说带来了革命性的成果。虽然乍一看好像是因为它将网络变深了，实际上是其中的一个小模块的设计技巧加强了所有卷积层的学习性能。谷歌公司把它称作 Inception，灵感来源于当时同时期的电影 Inception（盗梦空间）。Inception 模块如下图所示： ©️ 图片来源 这个多层感知卷积层的引入，来源于一个很直观的思想，就是一个卷积网络里面的局部稀疏最优结构往往可以由简单可复用的密集组合来近似或者替代。就像上图里面 1x1, 3x3, 5x5 的卷积层，与 3x3 的池化层的组合一个 Inception。这样做是出于以下几点考虑： 不同尺寸的卷积核可以提取不同尺度的信息。 采用 1x1,3x3, 5x5 可以方便对齐，Padding 分别为 0,1,2 就可以对齐。 由于池化层在 CNN 网络里面的成功运用，也把池化层当做组合的一部分，在 GoogleNet 论文里也说明了它的有效性。 由于 Google Net 是好几个 Inception 模块的堆叠，而且往往越后面的 Inception 模块能够提取到更加高级抽象的特征。 前一层经过 \\(1\\times 1\\) 或者池化层降维后，在全连接层过滤器将 \\(1\\times1\\), \\(3\\times3\\), \\(5\\times5\\) 的卷积结果连接起来，使网络的深度和宽度均可以扩大。论文显示，Inception 模块使整个网络的训练过程有 2 到 3 倍的加速，且更容易捕捉到关键特征用以学习。同时，由于网络很深，为了避免梯度消失的问题，Google Net 还巧妙地在不同深度增加了两个损失函数来训练。 整个 Google Net 结构如下图： ©️ 图片来源 可以看出 Google Net 已经是非常深的网络了，你可以通过新标签页打开图片查看细节。Google Net 所依赖的训练时间与运算成本都上了一个台阶，Inception 这个创意性的设计，极大地优化了卷积神经网络的学习过程。 ResNet 神经网络如果把网络加深和增强卷积模块功能两个演化方向相结合，就形成了 ResNet。2015 年何明凯团队发布的 ResNet 可以说影响了整个深度学习界。首先让我们看看这个网络有多么震撼： ©️ 图片来源 ResNet 的训练深度达到了 152 层，上图（最右）仅是一个迷你型的 34 层 ResNet 结构。左图是 VGG-19，中间是不具备 Residual 结构的深度神经网络。 可以很明显的看出，ResNet 将部分网络进行了一个跳层传递的操作。网络在深化的过程中，可能会出现一种叫网络退化的问题，ResNet 通过引入这个跳层传递，将前面某层的结果与这一层的卷积结果相加，去优化一个残差来规避这些问题。 ResNet 将我们的网络深度倍数化增加，从最多 20 层一下子拉到了 100 层以上，而且网络也不再是平常的层数堆叠，最终模型预测准确率已经达到了一个巨大的量变提高。 这些经典的神经网络都有它的开创性和典型性，我们可以像上面实现 AlexNet 一样，用 Tensorflow 实现其他网络。但后面的网络结构都太复杂，实验就不再动手搭建。如果你有兴趣，可以依据网络结构，仔细研读该网络的论文，试着动手搭建这些网络。 Deep Residual Learning for Image Recognition: arXiv:1512.03385 卷积神经网络的发展史最后，我们通过一张图来总结卷积神经网络的发展史： 为什么要使用卷积神经网络？这是一个很关键的问题，大家肯定也有这样的疑问。之前我们的神经网络像全连接网络这样，让每个神经元互相连接然后激活，已经可以达到不错的学习效果，也可以拟合去学习各种复杂的过程。 那么，经过以上对卷积神经网络的深入了解后，你能否简要回答这个问题呢？ 形象地来讲，卷积神经网络保留了图像的空间信息。 19 世纪 60 年代，科学家通过对猫的视觉皮层细胞研究发现，每一个视觉神经元只会处理一小块区域的视觉图像，即感受野。我们也看到在卷积神经网络中，一个卷积层可以有多个不同的卷积核（权值），而每个卷积核在输入图像上滑动且每次只处理一小块图像。 这样，输入端的卷积层可以提取到图像中最基础的特征，比如不同方向的直线或者拐角；接着再组合成高阶特征，比如三角形、正方形等；再继续抽象组合，得到眼睛、鼻子和嘴等五官；最后再将五官组合成一张脸，完成匹配识别。过程中，每个卷积层提取的特征，在后面的层中都会抽象组合成更高阶的特征，最终习得足够多足够关键的特征，完成学习任务。 从运算上讲，卷积层具有两个关键优势，分别是局部连接和权值共享。 每个神经元只与上一层的一个局部区域连接，该连接的空间大小可视为神经网络神经元的感受野。权值共享的意义在于，当前层在深度方向上每层神经元都使用同样的权重和偏差。 这里作了一个合理的假设：如果一个特征在计算某个空间位置 \\((x,y)\\) 的时候有用，那么它在计算另一个不同位置 \\((x2,y2)\\) 的时候也有用。我们将局部连接和权值共享结合，就降低了参数量，使训练复杂度大大下降。这样的操作还能减轻过拟合，权值共享同时还赋予了卷积网络对平移的容忍性。 试想，如果我们用全连接神经网络来做图像识别任务，图片是由像素点组成的，用矩阵表示的，\\(28\\times28\\) 的矩阵，我们得把它「拍平」后，变成一个 \\(784\\) 的一列向量。如果该列向量和隐含层的 `15` 个神经元连接，就有 \\(784\\times15=11760\\) 个权重 `w`。如果，隐含层和最后的输出层的 `10` 个神经元连接，就有 \\(11760\\times10=117600\\) 个权重 `w` 。最后，再加上隐含层的偏置项 `15` 个和输出层的偏置项 `10` 个，就得到了 `117625` 个参数。这是多么恐怖。 下图表示一个三层全连接网络的结构，输入为图片即 `784` 个神经元，隐藏层 `15` 个神经元，输出层 `10` 个神经元： ©️ 图片来源 那么，试想我们如果用卷积神经网络来解决。卷积层里，我们只需要学习卷积核（权值），就像前面搭建的卷积神经网络中，第一个卷积层只需要学习 \\(3\\times3\\times1\\times8=72\\) 个参数，第二层学习 \\(3\\times3\\times8\\times10=720\\) 个参数，池化层不需要学习任何参数，全连接层参数也大幅下降。这样算下来，所需要学习的参数相比上面的三层全连接层已经大幅减少。这就是卷积神经网络的好处。","link":"/2019/03/26/CNN_overview/"},{"title":"机器学习|感知机和人工神经网络详解 (Python 语言描述)","text":"人工神经网络是一种发展时间较早且十分常用的机器学习算法。因其模仿人类神经元工作的特点，在监督学习和非监督学习领域都给予了人工神经网络较高的期望。目前，由传统人工神经网络发展而来的卷积神经网络、循环神经网络已经成为了深度学习的基石。本篇文章中，我们将从人工神经网络的原型感知机出发，介绍机器学习中人工神经网络的特点及应用。 感知机在介绍人工神经网络之前，我们先介绍它的原型：感知机。关于感知机，我们先引用一段来自维基百科的背景介绍： 感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。 如果你之前从未接触过人工神经网络，那么上面这句话或许还需要等到阅读完这篇文章才能完整理解。不过，你可以初步发现，感知机其实就是人工神经网络，只不过是其初级形态。 感知机的推导过程那么，感知机到底是什么？它是怎样被发明出来的呢？ 要搞清楚上面的问题，我们就需要提到前面学习过的一个非常熟悉的知识点：线性回归。回忆关于逻辑回归的内容，你应该还能记得当初我们说过逻辑回归起源于线性回归。而感知机作为一种最简单的二分类模型，它其实就是使用了线性回归的方法完成平面数据点的分类。而逻辑回归后面引入了逻辑斯蒂估计来计算分类概率的方法甚至可以被当作是感知机的进步。 你还记得上面这张图片吗？当数据点处于线性可分时，我们可以使用一条直线将其分开，而分割线的函数为： f(x) = w_1x_1+w_2x_2+ \\cdots +w_nx_n + b = WX+b \\tag{1}对于公式（1）而言，我们可以认为分割直线方程其实就是针对数据集的每一个特征 \\(x_{1}, x_{2}, \\cdots, x_{n}\\) 依次乘上权重 \\(w_{1}, w_{2}, \\cdots, w_{n}\\) 所得。 当我们确定好公式（1）的参数后，每次输入一个数据点对应的特征 \\(x_{1}, x_{2}, \\cdots, x_{n}\\) 就能得到对应的函数值 \\(f(x)\\)。那么，怎样判定这个数据点属于哪一个类别呢？ 在二分类问题中，我们最终的类别共有两个，通常被称之为正类别和负类别。而当我们使用线性回归中对应的公式（1）完成分类时，不同于逻辑回归中将 \\(f(x)\\) 传入 sigmoid 函数，这里我们将 \\(f(x)\\) 传入如下所示的 sign 函数。 sign(x) = \\begin{cases}+1, & \\text{if } x \\geq 0\\\\\\-1, & \\text{if } x < 0\\end{cases} \\tag{2}sign() 函数又被称之为符号函数，它的函数值只有 2 个。即当自变量 \\(x \\geq 0\\) 时，因变量为 1。同理，当 \\(x &lt; 0\\) 时，因变量为 -1。函数图像如下： 于是，当我们将公式（1）中的 \\(f(x)\\) 传入公式（2），就能得到 \\(sign( f(x) )\\) 的值。其中，当 \\(sign( f(x) ) = 1\\) 时，就为正分类点，而 \\(sign(f(x)) = -1\\) 时，则为负分类点。 综上所示，我们就假设输入空间(特征向量)为 \\(X \\subseteq R^n\\)，输出空间为 \\(Y={-1, +1}\\)。输入 \\(x \\subseteq X\\) 表示实例的特征向量，对应于输入空间的点；输出 \\(y \\subseteq Y\\) 表示示例的类别。由输入空间到输出空间的函数如下： f(x) = sign(w*x +b) \\tag{3}公式（3）就被称之为感知机。注意，公式（3）中的 \\(f(x)\\) 和公式（1）中的 \\(f(x)\\) 不是同一个 \\(f(x)\\)。 感知机计算流程图上面，我们针对感知机进行了数学推导。为了更加清晰地展示出感知机的计算过程，我们将其绘制成如下所示的流程图。 感知机的损失函数前面的文章中，我们已经介绍过损失函数的定义。在感知机的学习过程中，我们同样需要确定每一个特征变量对应的参数，而损失函数的极小值往往就意味着参数最佳。那么，感知机学习的策略，也就是其通常采用哪种形式的损失函数呢？ 如下图所示，当我们使用一条直线去分隔一个线性可分的数据集时，有可能会出现「误分类」的状况。 而在感知机的学习过程中，我们通常会使用误分类点到分割线（面）的距离去定义损失函数。 点到直线的距离中学阶段，我们学过点到直线的距离公式推导。对于 `n` 维实数向量空间中任意一点 `x_0` 到直线 `W*x+b=0` 的距离为： d= \\dfrac{1}{\\parallel W\\parallel}|W*x_{0}+b| \\tag{4}其中 `||W||` 表示 `L_2` 范数，即向量各元素的平方和然后开方。 然后，对于误分类点 `(x_i,y_i)` 来讲，公式（5）成立。 y_i(W * x_{i}+b)>0 \\tag{5}那么，误分类点 `(x_i,y_i)` 到分割线（面）的距离就为： d=-\\dfrac{1}{\\parallel W\\parallel}y_i(W*x_{i}+b) \\tag{6}于是，假设所有误分类点的集合为 `M`，全部误分类点到分割线（面）的距离就为： -\\dfrac{1}{\\parallel W\\parallel}\\sum_{x_i\\epsilon M} y_i(W*x_{i}+b) \\tag{7}最后得到感知机的损失函数为： J(W,b) = - \\sum_{x_i\\epsilon M} y_i(W*x_{i}+b) \\tag{8}从公式（8）可以看出，损失函数 `J(W,b)` 是非负的。也就是说，当没有误分类点时，损失函数的值为 0。同时，误分类点越少，误分类点距离分割线（面）就越近，损失函数值就越小。同时，损失函数 `J(W,b)` 是连续可导函数。 随机梯度下降法当我们在实现分类时，最终想要的结果肯定是没有误分类的点，也就是损失函数取极小值时的结果。在逻辑回归的中，为了找到损失函数的极小值，我们使用到了一种叫做梯度下降法（Gradient descent）。而在这篇中，我们尝试一种梯度下降法的改进方法，也称之为随机梯度下降法（Stochastic gradient descent，简称：SGD)。 实验 SGD 计算公式（8）的极小值时，首先任选一个分割面 `W_0` 和 `b_0`，然后使用梯度下降法不断地极小化损失函数： min_{W,b} J(W,b) = - \\sum_{x_i\\epsilon M} y_i(W*x_{i}+b) \\tag{9}随机梯度下降的特点在于，极小化过程中不是一次针对 `M` 中的所有误分类点执行梯度下降，而是每次随机选取一个误分类点执行梯度下降。等到更新完 `W` 和 `b` 之后，下一次再另随机选择一个误分类点执行梯度下降直到收敛。 计算损失函数的偏导数： \\frac{\\partial J(W,b)}{\\partial W} = - \\sum_{x_i\\epsilon M}y_ix_i \\\\ \\frac{\\partial J(W,b)}{\\partial b} = - \\sum_{x_i\\epsilon M}y_i \\tag{10}如果 \\(y_i(W * x_{i}+b)\\leq0\\) 更新 `W` 和 `b` ： W \\leftarrow W + \\lambda y_ix_i \\\\ b \\leftarrow b + \\lambda y_i \\tag{11}同前面的梯度下降一致，\\(\\lambda\\) 为学习率，也就是每次梯度下降的步长。 下面，我们使用 Python 将上面的随机梯度下降算法进行实现。 12345678910111213141516171819202122232425\"\"\"感知机随机梯度下降算法实现\"\"\"def perceptron_sgd(X, Y, alpha, epochs): \"\"\" 参数: X -- 自变量数据矩阵 Y -- 因变量数据矩阵 alpha -- lamda 参数 epochs -- 迭代次数 返回: w -- 权重系数 b -- 截距项 \"\"\" w = np.zeros(len(X[0])) # 初始化参数为 0 b = np.zeros(1) for t in range(epochs): # 迭代 for i, x in enumerate(X): if ((np.dot(X[i], w)+b)*Y[i]) &lt;= 0: # 判断条件 w = w + alpha*X[i]*Y[i] # 更新参数 b = b + alpha*Y[i] return w, b 感知机分类实例前面的内容中，我们讨论了感知机的计算流程，感知机的损失函数，以及如何使用随机梯度下降求解感知机的参数。理论说了这么多，下面就举一个实际的例子看一看。 示例数据集为了方便绘图到二维平面，这里只使用包含两个特征变量的数据，数据集名称为 course-12-data.csv。 数据集下载 👉 传送门 123456\"\"\"加载数据集\"\"\"import pandas as pddf = pd.read_csv(\"course-12-data.csv\", header=0) # 加载数据集df.head() # 预览前 5 行数据 可以看到，该数据集共有两个特征变量 X0 和 X1, 以及一个目标值 Y。其中，目标值 Y 只包含 -1 和 1。我们尝试将该数据集绘制成图，看一看数据的分布情况。 1234567\"\"\"绘制数据集\"\"\"from matplotlib import pyplot as plt%matplotlib inlineplt.figure(figsize=(10, 6))plt.scatter(df['X0'],df['X1'], c=df['Y']) 感知机训练接下来，我们就使用感知机求解最佳分割线。 123456789import numpy as npX = df[['X0','X1']].valuesY = df['Y'].valuesalpha = 0.1epochs = 150perceptron_sgd(X, Y, alpha, epochs) (array([ 4.93, -6.98]), array([-3.3])) 于是，我们求得的最佳分割线方程为： f(x) = 4.93\\*x_1-6.98\\*x_2 -3.3 \\tag{12}此时，可以求解一下分类的正确率： 1234567L = perceptron_sgd(X, Y, alpha, epochs)w1 = L[0][0]w2 = L[0][1]b = L[1]z = np.dot(X, np.array([w1, w2]).T) + bnp.sign(z) array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., 1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., 1., -1., -1., -1., -1., -1., -1., -1., -1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) 为了方便，我们就直接使用 scikit-learn 提供的准确率计算方法 accuracy_score()，该方法相信你已经非常熟悉了 123from sklearn.metrics import accuracy_scoreaccuracy_score(Y, np.sign(z)) 0.9866666666666667 所以，最终的分类准确率约为 0.987。 绘制决策边界线123456789101112# 绘制轮廓线图，不需要掌握plt.figure(figsize=(10, 6))plt.scatter(df['X0'],df['X1'], c=df['Y'])x1_min, x1_max = df['X0'].min(), df['X0'].max(),x2_min, x2_max = df['X1'].min(), df['X1'].max(),xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))grid = np.c_[xx1.ravel(), xx2.ravel()]probs = (np.dot(grid, np.array([L[0][0], L[0][1]]).T) + L[1]).reshape(xx1.shape)plt.contour(xx1, xx2, probs, [0], linewidths=1, colors='red'); 可以看到，上图中的红色直线就是我们最终的分割线，分类的效果还是不错的。 绘制损失函数变换曲线除了绘制决策边界，也就是分割线。我们也可以将损失函数的变化过程绘制处理，看一看梯度下降的执行过程。 1234567891011121314151617181920212223242526272829\"\"\"计算每次迭代后的损失函数值\"\"\"def perceptron_loss(X, Y, alpha, epochs): \"\"\" 参数: X -- 自变量数据矩阵 Y -- 因变量数据矩阵 alpha -- lamda 参数 epochs -- 迭代次数 返回: loss_list -- 每次迭代损失函数值列表 \"\"\" w = np.zeros(len(X[0])) # 初始化参数为 0 b = np.zeros(1) loss_list = [] for t in range(epochs): # 迭代 loss_init = 0 for i, x in enumerate(X): if ((np.dot(X[i], w)+b)*Y[i]) &lt;= 0: # 判断条件 loss_init += (((np.dot(X[i], w)+b)*Y[i])) w = w + alpha*X[i]*Y[i] # 更新参数 b = b + alpha*Y[i] loss_list.append(loss_init * -1) return loss_list 123456loss_list = perceptron_loss(X, Y, alpha, epochs)plt.figure(figsize=(10, 6))plt.plot([i for i in range(len(loss_list))], loss_list)plt.xlabel(\"Learning rate {}, Epochs {}\".format(alpha, epochs))plt.ylabel(\"Loss function\") 如上图所示，你会发现，让我们按照 0.1 的学习率迭代 150 次后，损失函数依旧无法到达 0。一般情况下，当我们的数据不是线性可分时，损失函数就会出现如上图所示的震荡线性。 不过，如果你仔细观察上方数据的散点图，你会发现这个数据集看起来是线性可分的。那么，当数据集线性可分，却造成损失函数变换曲线震荡的原因一般有两点：学习率太大或者迭代次数太少。 其中，迭代次数太少很好理解，也就是说我们迭代的次数还不足以求得极小值。至于学习率太大，可以看下方的示意图。 如上图所示，当我们的学习率太大时，往往容易出现在损失函数底部来回震荡的现象而无法到达极小值点。所以，面对上面这种情况，我们可以采取减小学习率 + 增加迭代次数的方法找到损失函数极小值点。 所以，下面就再试一次。 123456789alpha = 0.05 # 减小学习率epochs = 1000 # 增加迭代次数loss_list = perceptron_loss(X, Y, alpha, epochs)plt.figure(figsize=(10, 6))plt.plot([i for i in range(len(loss_list))], loss_list)plt.xlabel(\"Learning rate {}, Epochs {}\".format(alpha, epochs))plt.ylabel(\"Loss function\") 可以看到，当迭代次数约为 700 次，即上图后半段时，损失函数的值等于 0。根据我们在 1.3 小节中介绍的内容，当损失函数为 0 时，就代表没有误分类点存在。 此时，我们再一次计算分类准确率。 123L = perceptron_sgd(X, Y, alpha, epochs)z = np.dot(X, L[0].T) + L[1]accuracy_score(Y, np.sign(z)) 1.0 和损失函数变化曲线得到的结论一致，分类准确率已经 100%，表示全部数据点被正确分类。 人工神经网络 神经网络的结构 | 视频来源：[3Blue1Brown](https://www.bilibili.com/video/av15532370) 上面的内容中，我们已经了解到了什么是感知机，以及如何构建一个感知机分类模型。你会发现，感知机只能处理二分类问题，且必须是线性可分问题。如果是这样的话，该方法的局限性就比较大了。那么，面对线性不可分或者多分类问题时，我们有没有一个更好的方法呢？ 多层感知机与人工神经网络这里，就要提到本文的主角，也就是人工神经网络（英语：Artificial neural network，简称：ANN）。如果你第一次接触到人工神经网络，不要将其想的太神秘。其实，上面的感知机模型就是一个人工神经网络，只不过它是一个结构简单的单层神经网络。而如果我们要解决线性不可分或者多分类问题，往往会尝试将多个感知机组合在一起，变成一个更复杂的神经网络结构。 * 由于一些历史遗留问题，感知机、多层感知机、人工神经网络三种说法界限模糊，文中介绍到的人工神经网络从某种意义上代指多层感知机。 在上文 1.2 小节中，我们通过一张图展示了感知机的工作流程，我们将该流程图进一步精简如下： 这张图展示了一个感知机模型的执行流程。我们可以把输入称之为「输入层」，输出称之为「输出层」。对于像这样只包含一个输入层的网络结构就可以称之为单层神经网络结构。 单个感知机组成了单层神经网络，如果我们将一个感知机的输出作为另一个感知机的输入，就组成了多层感知机，也就是一个多层神经网络。其中，我们将输入和输出层之间的称为隐含层。如下图所示，这就是包含 1 个隐含层的神经网络结构。 一个神经网络结构在计算层数的时候，我们一般只计算输入和隐含层的数量，即上方是一个 2 层神经网络结构。 激活函数目前，我们已经接触过逻辑回归、感知机、多层感知机与人工神经网络 4 个概念。你可能隐约感觉到，似乎这 4 种方法都与线性函数有关，而区别在于对线性函数的因变量的不同处理方式上面。 f(x) = w_1x_1+w_2x_2+ \\cdots +w_nx_n + b = WX+b \\tag{13} 对于逻辑回归而言，我们是采用了 \\(sigmoid\\) 函数将 `f(x)` 转换为概率，最终实现二分类。 对于感知机而言，我们是采用了 `sign` 函数将 `f(x)` 转换为 -1 和 +1 最终实现二分类。 对于多层感知机而言，具有多层神经网络结构，在 `f(x)` 的处理方式上，一般会有更多的操作。 于是，\\(sigmoid\\) 函数和 `sign` 函数还有另外一个称谓，叫做「激活函数（Activation function）」。听到激活函数，大家首先不要觉得它有多么的高级。之所以有这样一个称谓，是因为函数本身有一些特点，但归根结底还是数学函数。下面，我们就列举一下常见的激活函数及其图像。 \\(sigmoid\\) 函数\\(sigmoid\\) 函数应该已经非常熟悉了吧，它的公式如下： sigmoid(x)=\\frac{1}{1+e^{-x}} \\tag{14}\\(sigmoid\\) 函数的图像呈 S 型，函数值介于 `(0, 1)` 之间： \\(Tanh\\) 函数\\(Tanh\\) 函数与 \\(sigmoid\\) 函数的图像很相似，都呈 S 型，只不过 \\(Tanh\\) 函数值介于 `(-1, 1)` 之间，公式如下： tanh(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}\\tag{15} \\(ReLU\\) 函数\\(ReLU\\) 函数全称叫做 Rectified Linear Unit，也就是修正线性单元，公式如下： ReLU(x) = max(0,x)\\tag{16}\\(ReLU\\) 有很多优点，比如收敛速度会较快且不容易出现梯度消失。由于这次不会用到，我们之后再说。\\(ReLU\\)的图像如下： 激活函数的作用上面列举了 3 种常用的激活函数，其中 \\(sigmoid\\) 函数是介绍的人工神经网络中十分常用的一种激活函数。谈到激活函数的作用，直白地讲就是针对数据进行非线性变换。只是不同的激活函数适用于不同的场景，而这些都是机器学习专家根据应用经验总结得到的。 在神经网络结构中，我们通过线性函数不断的连接输入和输出。你可以设想，在这种结构中，每一层输出都是上层输入的线性变换。于是，无论神经网络有多少层，最终输出都是输入的线性组合。这样的话，单层神经网络和多层神经网络有什么区别呢？（没有区别） 如上图所示，线性变换的多重组合依旧还是线性变换。如果我们在网络结构中加入激活函数，就相当于引入了非线性因素，这样就可以解决线性模型无法完成的分类任务。 反向传播算法（BP）直观认识 直观理解反向传播 | 视频来源：[3Blue1Brown](https://www.bilibili.com/video/av16577449/?p=1) 前面感知机的章节中，我们定义了一个损失函数，并通过一种叫做随机梯度下降的方法去求解最优参数。如果你仔细观察随机梯度下降的过程，其实就是通过求解偏导数并组合成梯度用于更新权重 `W` 和 `b`。感知机只有一层网络结构，求解梯度的过程还比较简单。但是，当我们组合成多层神经网络之后，更新权重的过程就变得复杂起来，而反向传播算法正是为了快速求解梯度而生。 反向传播的算法说起来很简单，但要顺利理解还比较复杂。这里，我们引用了波兰 AGH 科技大学的一篇 科普文章 中的配图来帮助理解反向传播的过程。 下图呈现了一个经典的 3 层神经网络结构，其包含有 2 个输入 `x_{1}` 和 `x_{2}` 以及 1 个输出 `y`。 网络中的每个紫色单元代表一个独立的神经元，它分别由两个单元组成。一个单元是权重和输入信号，而另一个则是上面提到的激活函数。其中，`e` 代表激活信号，所以 `y = f(e)` 就是被激活函数处理之后的非线性输出，也就是整个神经元的输出。 * 注：此处与下文使用g()作为激活函数稍有不同 下面开始训练神经网络，训练数据由输入信号 `x_{1}` 和 `x_{2}` 以及期望输出 `z` 组成，首先计算第 1 个隐含层中第 1 个神经元 `y_{1} = f_{1}(e)` 对应的值。 接下来，计算第 1 个隐含层中第 2 个神经元 `y_{2} = f_{2}(e)` 对应的值。 然后是计算第 1 个隐含层中第 3 个神经元 `y_{3} = f_{3}(e)` 对应的值。 与计算第 1 个隐含层的过程相似，我们可以计算第 2 个隐含层的数值。 最后，得到输出层的结果： 上面这个过程被称为前向传播过程，那什么是反向传播呢？接着来看： 当我们得到输出结果 `y` 时，可以与期望输出 `z` 对比得到误差 \\(\\delta\\)。 然后，我们将计算得到的误差 \\(\\delta\\) 沿着神经元回路反向传递到前 1 个隐含层，而每个神经元对应的误差为传递过来的误差乘以权重。 同理，我们将第 2 个隐含层的误差继续向第 1 个隐含层反向传递。 此时，我们就可以利用反向传递过来的误差对从输入层到第 1 个隐含层之间的权值 `w` 进行更新，如下图所示： 同样，对第 1 个隐含层与第 2 个隐含层之间的权值 `w` 进行更新，如下图所示： 最后，更新第 2 个隐含层与输出层之间的权值 `w` ，如下图所示： 图中的 \\(\\eta\\) 表示学习速率。这就完成了一个迭代过程。更新完权重之后，又开始下一轮的前向传播得到输出，再反向传播误差更新权重，依次迭代下去。 所以，反向传播其实代表的是反向传播误差。 使用 Python 实现人工神经网络上面的内容，我们介绍了人工神经网络的构成和最重要的反向传播算法。接下来，尝试通过 Python 来实现一个神经网络运行的完整流程。 定义神经网络结构为了让推导过程足够清晰，这里我们只构建包含 1 个隐含层的人工神经网络结构。其中，输入层为 2 个神经元，隐含层为 3 个神经元，并通过输出层实现 2 分类问题的求解。该神经网络的结构如下： 在此中，我们使用的激活函数为 \\(sigmoid\\) 函数： \\mathit{sigmoid}(x) = \\frac{1}{1+e^{-x}} \\tag{17a}由于下面要使用 \\(sigmoid\\) 函数的导数，所以同样将其导数公式写出来： \\Delta \\mathit{sigmoid}(x) = \\mathit{sigmoid}(x)(1 - \\mathit{sigmoid}(x)) \\tag{17b}然后，我们通过 Python 实现公式（17）： 1234567# sigmoid 函数def sigmoid(x): return 1 / (1 + np.exp(-x))# sigmoid 函数求导def sigmoid_derivative(x): return sigmoid(x) * (1 - sigmoid(x)) 前向传播前向（正向）传播中，每一个神经元的计算流程为：线性变换 → 激活函数→输出值。 同时，我们约定： `Z` 表示隐含层输出，`Y` 则为输出层最终输出。 `w_{ij}` 表示从第 `i` 层的第 `j` 个权重。 于是，上图中的前向传播的代数计算过程如下。 神经网络的输入 `X`，第一层权重 `W_1`，第二层权重 `W_2`。为了演示方便，`X` 为单样本，因为是矩阵运算，我们很容易就能扩充为多样本输入。 X = \\begin{bmatrix} x_{1} & x_{2} \\end{bmatrix} \\tag{18} W_1 = \\begin{bmatrix} w_{11} & w_{12} & w_{13}\\\\ w_{14} & w_{15} & w_{16}\\\\ \\end{bmatrix} \\tag{19} W_2 = \\begin{bmatrix} w_{21} \\\\ w_{22} \\\\ w_{23} \\end{bmatrix} \\tag{20}接下来，计算隐含层神经元输出 `Z`（线性变换 → 激活函数）。同样，为了使计算过程足够清晰，我们这里将截距项表示为 0。 Z = \\mathit{sigmoid}(X \\cdot W_{1}) \\tag{21}最后，计算输出层 `Y`（线性变换 → 激活函数）： Y = \\mathit{sigmoid}(Z \\cdot W_{2}) \\tag{22}下面实现前向传播计算过程，将上面的公式转化为代码如下： 12345# 示例样本X = np.array([[1, 1]])y = np.array([[1]])X, y (array([[1, 1]]), array([[1]])) 然后，随机初始化隐含层权重。 1234W1 = np.random.rand(2, 3)W2 = np.random.rand(3, 1)W1, W2 (array([[0.4985742 , 0.16703231, 0.51487393], [0.63075313, 0.46386686, 0.44365266]]), array([[0.69320812], [0.74352002], [0.2403471 ]])) 前向传播的过程实现基于公式（21）和公式（22）完成。 12345input_layer = X # 输入层hidden_layer = sigmoid(np.dot(input_layer, W1)) # 隐含层，公式 20output_layer = sigmoid(np.dot(hidden_layer, W2)) # 输出层，公式 22output_layer array([[0.76546658]]) 反向传播 反向传播的微积分原理 | 视频来源：[3Blue1Brown](https://www.bilibili.com/video/av16577449/?p=2) 接下来，我们使用梯度下降法的方式来优化神经网络的参数。那么首先需要定义损失函数，然后计算损失函数关于神经网络中各层的权重的偏导数（梯度）。 此时，设神经网络的输出值为 Y，真实值为 y。然后，定义平方损失函数如下： Loss(y, Y) = \\sum (y - Y)^2 \\tag{23}接下来，求解梯度 \\(\\frac{\\partial Loss(y, Y)}{\\partial{W_2}}\\)，需要使用链式求导法则： \\frac{\\partial Loss(y, Y)}{\\partial{W_2}} = \\frac{\\partial Loss(y, Y)}{\\partial{Y}} \\frac{\\partial Y}{\\partial{W_2}}\\tag{24a} \\frac{\\partial Loss(y, Y)}{\\partial{W_2}} = 2(Y-y) * \\Delta \\mathit{sigmoid}(Z \\cdot W_2) \\cdot Z\\tag{24b}同理，梯度 \\(\\frac{\\partial Loss(y, Y)}{\\partial{W_1}}\\) 得： \\frac{\\partial Loss(y, Y)}{\\partial{W_1}} = \\frac{\\partial Loss(y, Y)}{\\partial{Y}} \\frac{\\partial Y }{\\partial{Z}} \\frac{\\partial Z}{\\partial{W_1}} \\tag{25a} \\frac{\\partial Loss(y, Y)}{\\partial{W_1}} = 2(Y-y) \\* \\Delta \\mathit{sigmoid}(Z \\cdot W_2) \\cdot W_2 \\* \\Delta \\mathit{sigmoid}(X \\cdot W_1) \\cdot X \\tag{25b}其中，\\(\\frac{\\partial Y}{\\partial{W_2}}\\)，\\(\\frac{\\partial Y}{\\partial{W_1}}\\) 分别通过公式（22）和（21）求得。接下来，我们基于公式对反向传播过程进行代码实现。 12345678910# 公式 24d_W2 = np.dot(hidden_layer.T, (2 * (output_layer - y) * sigmoid_derivative(np.dot(hidden_layer, W2))))# 公式 25d_W1 = np.dot(input_layer.T, ( np.dot(2 * (output_layer - y) * sigmoid_derivative( np.dot(hidden_layer, W2)), W2.T) * sigmoid_derivative(np.dot(input_layer, W1))))d_W2, d_W1 (array([[-0.06363904], [-0.05496356], [-0.06086952]]), array([[-0.01077667, -0.01419321, -0.00405499], [-0.01077667, -0.01419321, -0.00405499]])) 现在，就可以设置学习率，并对 $W_1$, $W_2$ 进行一次更新了。 123456# 梯度下降更新权重, 学习率为 0.05W1 -= 0.05 * d_W1 # 如果上面是 y - output_layer，则改成 +=W2 -= 0.05 * d_W2d_W2, d_W1 (array([[-0.06363904], [-0.05496356], [-0.06086952]]), array([[-0.01077667, -0.01419321, -0.00405499], [-0.01077667, -0.01419321, -0.00405499]])) 以上，我们就实现了单个样本在神经网络中的 1 次前向 → 反向传递，并使用梯度下降完成 1 次权重更新。那么，下面我们完整实现该网络，并对多样本数据集进行学习。 123456789101112131415161718192021222324252627282930# 示例神经网络完整实现class NeuralNetwork: # 初始化参数 def __init__(self, X, y, lr): self.input_layer = X self.W1 = np.random.rand(self.input_layer.shape[1], 3) self.W2 = np.random.rand(3, 1) self.y = y self.lr = lr self.output_layer = np.zeros(self.y.shape) # 前向传播 def forward(self): self.hidden_layer = sigmoid(np.dot(self.input_layer, self.W1)) self.output_layer = sigmoid(np.dot(self.hidden_layer, self.W2)) # 反向传播 def backward(self): d_W2 = np.dot(self.hidden_layer.T, (2 * (self.output_layer - self.y) * sigmoid_derivative(np.dot(self.hidden_layer, self.W2)))) d_W1 = np.dot(self.input_layer.T, ( np.dot(2 * (self.output_layer - self.y) * sigmoid_derivative( np.dot(self.hidden_layer, self.W2)), self.W2.T) * sigmoid_derivative( np.dot(self.input_layer, self.W1)))) # 参数更新 self.W1 -= self.lr * d_W1 self.W2 -= self.lr * d_W2 接下来，我们使用实验一开始的示例数据集测试，首先我们要对数据形状进行调整，以满足需要。 12X = df[['X0','X1']].values # 输入值y = df['Y'].values.reshape(len(X), -1) # 真实 y，处理成 [[],...,[]] 形状 接下来，我们将其输入到网络中，并迭代 100 次： 1234567891011nn = NeuralNetwork(X, y, lr=0.001) # 定义模型loss_list = [] # 存放损失数值变化for i in range(100): nn.forward() # 前向传播 nn.backward() # 反向传播 loss = np.sum((y - nn.output_layer) ** 2) # 计算平方损失 loss_list.append(loss)print(\"final loss:\", loss)plt.plot(loss_list) # 绘制 loss 曲线变化图 final loss: 133.4221126605534 可以看到，损失函数逐渐减小并接近收敛，变化曲线比感知机计算会平滑很多。不过，由于我们去掉了截距项，且网络结构太过简单，导致收敛情况并不理想。本实验重点再于搞清楚 BP 的中间过程，准确度和学习难度不可两全。另外，需要注意的是由于权重是随机初始化，多次运行的结果会不同。","link":"/2019/01/31/Neural_Network/"},{"title":"机器学习|层次聚类方法 (Python 语言描述)","text":"在之前的文章中，我们学习了划分聚类方法，并着重介绍了其中的 K-Means 算法。K-Means 算法可以说是用处非常广泛的聚类算法之一，它非常好用。但是，当你使用过这种算法之后，你就会发现一个比较让人「头疼」的问题，那就是我们需要手动指定 K 值，也就是聚类的类别数量。 预先确定聚类的类别数量看起来是个小事情，但是在很多时候是比较麻烦的，因为我们可能在聚类前并不知道数据集到底要被聚成几类。例如，下面的示意图中，感觉聚成 2 类或者 4 类都是比较合理的。 今天要学习的层次聚类方法与划分聚类最大的区别之一，就是我们无需提前指定需要聚类类别的数量。这听起来是非常诱人的，到底是怎样做到的呢？ 简单来讲，层次聚类方法的特点在于通过计算数据集元素间的相似度来生成一颗具备层次结构的树。首先在这里强调一点，这里的「层次树」和之前学习的「决策树」是完全不同，不要混淆。 与此同时，当我们使用层次聚类方法时，可以根据创建树的方式分为两种情况： 自底向上层次聚类法：该方法的过程被称为「凝聚」(Agglomerative)，也就是把数据集中的每个元素看作是一个类别，然后进行迭代合并成为更大的类别，直到满足某个终止条件。 自顶向下层次聚类法：该方法的过程被称为「分裂」(Divisive)，也就是凝聚的反向过程。首先，把数据集看作是一个类别，然后递归地划分为多个较小的子类，直到满足某个终止条件。 自底向上层次聚类法自底向上层次聚类法也就是 Agglomerative Clustering 算法。这种算法的主要特点在于，我们使用「自底向上」进行聚类的思路来帮助距离相近的样本被放在同一类别中。 自底向上层次聚类流程具体来讲，这种方法的主要步骤如下： 对于数据集 `D`，\\(D=\\left ( {x_1,x_2,\\cdots,x_n} \\right )\\)： 将数据集中每个样本标记为 1 类，即 $D$ 初始时包含的类别（Class）为 `C`，\\(C=\\left ( {c_1,c_2,\\cdots,c_n} \\right )\\)。 计算并找出 `C` 中距离最近的 2 个类别，合并为 1 类。 依次合并直到最后仅剩下一个列表，即建立起一颗完整的层次树。 我们通过下图来演示自底向上层次聚类法的过程，首先平面上有 5 个样本点，我们将每个样本点都单独划为 1 类。 接下来，我们可以计算元素间的距离，并将距离最近的合并为 1 类。于是，总类别变为 3 类。 重复上面的步骤，总类别变为 2 类。 最后，合并为 1 类，聚类终止。 我们将上面的聚类过程变为层次树就为： 看完上面的演示过程，你会发现「自底向上层次聚类法」原来这么简单呀！ 距离计算方法虽然聚类过程看似简单，但不知道你是否意识到一个问题：当一个类别中包含多个元素时，类别与类别之间的距离是怎样确定的呢？ 也就是说，上面的演示过程中，为什么要把 `5` 归为 `` 组成的那一类，而不是 `` 组成的类呢？或者说，为什么不先把 `` 与 `` 合并，最后才合并 `5` 呢？ 这就涉及到 Agglomerative 聚类过程中的距离计算方式。简单来讲，我们一般有 3 种不同的距离计算方式： 单连接（Single-linkage）单连接的计算方式是根据两种类别之间最近的元素间距离作为两类别之间的距离。 全连接（Complete-linkage）全连接的计算方式是根据两种类别之间最远的元素间距离作为两类别之间的距离。 平均连接（Average-linkage）平均连接的计算方式是依次计算两种类别之间两两元素间距离，并最终求得平均值作为两类别之间的距离。 中心连接（Center-linkage）平均连接虽然看起来更加合理，但是两两元素间的距离计算量往往非常庞大。有时候，也可以使用中心连接计算方法。即先计算类别中心，再以中心连线作为两类别之间的距离。 总之，上面 4 种距离计算方法中，一般常用「平均连接」和「中心连接」方法，因为「单连接」和「全连接」都相对极端，容易受到噪声点和分布不均匀数据造成的干扰。 Agglomerative 聚类 Python 实现下面，我们尝试通过 Python 实现自底向上层次聚类算法。首先导入实验必要的模块： 1234import numpy as npfrom sklearn import datasetsfrom matplotlib import pyplot as plt%matplotlib inline 然后，通过 make_blobs 方法随机生成一组示例数据，且使得示例数据呈现出 2 类数据的趋势。这里，我们设定随机数种子 random_state=10 以保证你的结果和实验结果一致。 12data = datasets.make_blobs(10, n_features=2, centers=2, random_state=10)data (array([[ 6.04774884, -10.30504657], [ 2.90159483, 5.42121526], [ 4.1575017 , 3.89627276], [ 1.53636249, 5.11121453], [ 3.88101257, -9.59334486], [ 1.70789903, 6.00435173], [ 5.69192445, -9.47641249], [ 5.4307043 , -9.75956122], [ 5.85943906, -8.38192364], [ 0.69523642, 3.23270535]]), array([0, 1, 1, 1, 0, 1, 0, 0, 0, 1])) 使用 Matplotlib 绘制示例数据结果，可以看到数据的确呈现出 2 种类别的趋势。其中 data[1] 的结果即为生成数据时预设的类别，当然接下来的聚类过程，我们是不知道数据的预设类别。 1plt.scatter(data[0][:,0], data[0][:,1], c=data[1], s=60) 首先，我们实现欧式距离的计算函数： 1234567891011121314151617\"\"\"欧式距离\"\"\"def euclidean_distance(a, b): \"\"\" 参数: a -- 数组 a b -- 数组 b 返回: dist -- a, b 间欧式距离 \"\"\" x = float(a[0]) - float(b[0]) x = x * x y = float(a[1]) - float(b[1]) y = y * y dist = round(np.sqrt(x + y), 2) return dist 然后，实现 Agglomerative 聚类函数，这里使用中心连接的方法。为了更加具体的展示层次聚类的过程，这里在函数中添加一些多余的 print() 函数。 12345678910111213141516171819202122232425\"\"\"Agglomerative 聚类计算过程\"\"\"def agglomerative_clustering(data): while len(data) &gt; 1: print(\"☞ 第 {} 次迭代\\n\".format(10 - len(data) + 1)) min_distance = float('inf') # 设定初始距离为无穷大 for i in range(len(data)): print(\"---\") for j in range(i + 1, len(data)): distance = euclidean_distance(data[i], data[j]) print(\"计算 {} 与 {} 距离为 {}\".format(data[i], data[j],distance)) if distance &lt; min_distance: min_distance = distance min_ij = (i, j) i, j = min_ij # 最近数据点序号 data1 = data[i] data2 = data[j] data = np.delete(data, j, 0) # 删除原数据 data = np.delete(data, i, 0) # 删除原数据 b = np.atleast_2d([(data1[0] + data2[0]) / 2, (data1[1] + data2[1]) / 2]) # 计算两点新中心 data = np.concatenate((data, b), axis=0) # 将新数据点添加到迭代过程 print(\"\\n最近距离:{} &amp; {} = {}, 合并后中心:{}\\n\".format(data1, data2, min_distance, b)) return data 1agglomerative_clustering(data[0]) ☞ 第 1 次迭代 --- 计算 [ 6.04774884 -10.30504657] 与 [2.90159483 5.42121526] 距离为 16.04 计算 [ 6.04774884 -10.30504657] 与 [4.1575017 3.89627276] 距离为 14.33 计算 [ 6.04774884 -10.30504657] 与 [1.53636249 5.11121453] 距离为 16.06 计算 [ 6.04774884 -10.30504657] 与 [ 3.88101257 -9.59334486] 距离为 2.28 计算 [ 6.04774884 -10.30504657] 与 [1.70789903 6.00435173] 距离为 16.88 计算 [ 6.04774884 -10.30504657] 与 [ 5.69192445 -9.47641249] 距离为 0.9 计算 [ 6.04774884 -10.30504657] 与 [ 5.4307043 -9.75956122] 距离为 0.82 计算 [ 6.04774884 -10.30504657] 与 [ 5.85943906 -8.38192364] 距离为 1.93 计算 [ 6.04774884 -10.30504657] 与 [0.69523642 3.23270535] 距离为 14.56 --- 计算 [2.90159483 5.42121526] 与 [4.1575017 3.89627276] 距离为 1.98 计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4 计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05 计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33 计算 [2.90159483 5.42121526] 与 [ 5.69192445 -9.47641249] 距离为 15.16 计算 [2.90159483 5.42121526] 与 [ 5.4307043 -9.75956122] 距离为 15.39 计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12 计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11 --- 计算 [4.1575017 3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89 计算 [4.1575017 3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49 计算 [4.1575017 3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23 计算 [4.1575017 3.89627276] 与 [ 5.69192445 -9.47641249] 距离为 13.46 计算 [4.1575017 3.89627276] 与 [ 5.4307043 -9.75956122] 距离为 13.72 计算 [4.1575017 3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4 计算 [4.1575017 3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53 --- 计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89 计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91 计算 [1.53636249 5.11121453] 与 [ 5.69192445 -9.47641249] 距离为 15.17 计算 [1.53636249 5.11121453] 与 [ 5.4307043 -9.75956122] 距离为 15.37 计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17 计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06 --- 计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75 计算 [ 3.88101257 -9.59334486] 与 [ 5.69192445 -9.47641249] 距离为 1.81 计算 [ 3.88101257 -9.59334486] 与 [ 5.4307043 -9.75956122] 距离为 1.56 计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32 计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22 --- 计算 [1.70789903 6.00435173] 与 [ 5.69192445 -9.47641249] 距离为 15.99 计算 [1.70789903 6.00435173] 与 [ 5.4307043 -9.75956122] 距离为 16.2 计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97 计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95 --- 计算 [ 5.69192445 -9.47641249] 与 [ 5.4307043 -9.75956122] 距离为 0.39 计算 [ 5.69192445 -9.47641249] 与 [ 5.85943906 -8.38192364] 距离为 1.11 计算 [ 5.69192445 -9.47641249] 与 [0.69523642 3.23270535] 距离为 13.66 --- 计算 [ 5.4307043 -9.75956122] 与 [ 5.85943906 -8.38192364] 距离为 1.44 计算 [ 5.4307043 -9.75956122] 与 [0.69523642 3.23270535] 距离为 13.83 --- 计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71 --- 最近距离:[ 5.69192445 -9.47641249] &amp; [ 5.4307043 -9.75956122] = 0.39, 合并后中心:[[ 5.56131437 -9.61798686]] ☞ 第 2 次迭代 --- 计算 [ 6.04774884 -10.30504657] 与 [2.90159483 5.42121526] 距离为 16.04 计算 [ 6.04774884 -10.30504657] 与 [4.1575017 3.89627276] 距离为 14.33 计算 [ 6.04774884 -10.30504657] 与 [1.53636249 5.11121453] 距离为 16.06 计算 [ 6.04774884 -10.30504657] 与 [ 3.88101257 -9.59334486] 距离为 2.28 计算 [ 6.04774884 -10.30504657] 与 [1.70789903 6.00435173] 距离为 16.88 计算 [ 6.04774884 -10.30504657] 与 [ 5.85943906 -8.38192364] 距离为 1.93 计算 [ 6.04774884 -10.30504657] 与 [0.69523642 3.23270535] 距离为 14.56 计算 [ 6.04774884 -10.30504657] 与 [ 5.56131437 -9.61798686] 距离为 0.84 --- 计算 [2.90159483 5.42121526] 与 [4.1575017 3.89627276] 距离为 1.98 计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4 计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05 计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33 计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12 计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11 计算 [2.90159483 5.42121526] 与 [ 5.56131437 -9.61798686] 距离为 15.27 --- 计算 [4.1575017 3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89 计算 [4.1575017 3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49 计算 [4.1575017 3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23 计算 [4.1575017 3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4 计算 [4.1575017 3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53 计算 [4.1575017 3.89627276] 与 [ 5.56131437 -9.61798686] 距离为 13.59 --- 计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89 计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91 计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17 计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06 计算 [1.53636249 5.11121453] 与 [ 5.56131437 -9.61798686] 距离为 15.27 --- 计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75 计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32 计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22 计算 [ 3.88101257 -9.59334486] 与 [ 5.56131437 -9.61798686] 距离为 1.68 --- 计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97 计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95 计算 [1.70789903 6.00435173] 与 [ 5.56131437 -9.61798686] 距离为 16.09 --- 计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71 计算 [ 5.85943906 -8.38192364] 与 [ 5.56131437 -9.61798686] 距离为 1.27 --- 计算 [0.69523642 3.23270535] 与 [ 5.56131437 -9.61798686] 距离为 13.74 --- 最近距离:[ 6.04774884 -10.30504657] &amp; [ 5.56131437 -9.61798686] = 0.84, 合并后中心:[[ 5.80453161 -9.96151671]] ☞ 第 3 次迭代 --- 计算 [2.90159483 5.42121526] 与 [4.1575017 3.89627276] 距离为 1.98 计算 [2.90159483 5.42121526] 与 [1.53636249 5.11121453] 距离为 1.4 计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05 计算 [2.90159483 5.42121526] 与 [1.70789903 6.00435173] 距离为 1.33 计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12 计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11 计算 [2.90159483 5.42121526] 与 [ 5.80453161 -9.96151671] 距离为 15.65 --- 计算 [4.1575017 3.89627276] 与 [1.53636249 5.11121453] 距离为 2.89 计算 [4.1575017 3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49 计算 [4.1575017 3.89627276] 与 [1.70789903 6.00435173] 距离为 3.23 计算 [4.1575017 3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4 计算 [4.1575017 3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53 计算 [4.1575017 3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96 --- 计算 [1.53636249 5.11121453] 与 [ 3.88101257 -9.59334486] 距离为 14.89 计算 [1.53636249 5.11121453] 与 [1.70789903 6.00435173] 距离为 0.91 计算 [1.53636249 5.11121453] 与 [ 5.85943906 -8.38192364] 距离为 14.17 计算 [1.53636249 5.11121453] 与 [0.69523642 3.23270535] 距离为 2.06 计算 [1.53636249 5.11121453] 与 [ 5.80453161 -9.96151671] 距离为 15.67 --- 计算 [ 3.88101257 -9.59334486] 与 [1.70789903 6.00435173] 距离为 15.75 计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32 计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22 计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96 --- 计算 [1.70789903 6.00435173] 与 [ 5.85943906 -8.38192364] 距离为 14.97 计算 [1.70789903 6.00435173] 与 [0.69523642 3.23270535] 距离为 2.95 计算 [1.70789903 6.00435173] 与 [ 5.80453161 -9.96151671] 距离为 16.48 --- 计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71 计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58 --- 计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15 --- 最近距离:[1.53636249 5.11121453] &amp; [1.70789903 6.00435173] = 0.91, 合并后中心:[[1.62213076 5.55778313]] ☞ 第 4 次迭代 --- 计算 [2.90159483 5.42121526] 与 [4.1575017 3.89627276] 距离为 1.98 计算 [2.90159483 5.42121526] 与 [ 3.88101257 -9.59334486] 距离为 15.05 计算 [2.90159483 5.42121526] 与 [ 5.85943906 -8.38192364] 距离为 14.12 计算 [2.90159483 5.42121526] 与 [0.69523642 3.23270535] 距离为 3.11 计算 [2.90159483 5.42121526] 与 [ 5.80453161 -9.96151671] 距离为 15.65 计算 [2.90159483 5.42121526] 与 [1.62213076 5.55778313] 距离为 1.29 --- 计算 [4.1575017 3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49 计算 [4.1575017 3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4 计算 [4.1575017 3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53 计算 [4.1575017 3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96 计算 [4.1575017 3.89627276] 与 [1.62213076 5.55778313] 距离为 3.03 --- 计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32 计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22 计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96 计算 [ 3.88101257 -9.59334486] 与 [1.62213076 5.55778313] 距离为 15.32 --- 计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71 计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58 计算 [ 5.85943906 -8.38192364] 与 [1.62213076 5.55778313] 距离为 14.57 --- 计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15 计算 [0.69523642 3.23270535] 与 [1.62213076 5.55778313] 距离为 2.5 --- 计算 [ 5.80453161 -9.96151671] 与 [1.62213076 5.55778313] 距离为 16.07 --- 最近距离:[2.90159483 5.42121526] &amp; [1.62213076 5.55778313] = 1.29, 合并后中心:[[2.26186279 5.4894992 ]] ☞ 第 5 次迭代 --- 计算 [4.1575017 3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49 计算 [4.1575017 3.89627276] 与 [ 5.85943906 -8.38192364] 距离为 12.4 计算 [4.1575017 3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53 计算 [4.1575017 3.89627276] 与 [ 5.80453161 -9.96151671] 距离为 13.96 计算 [4.1575017 3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48 --- 计算 [ 3.88101257 -9.59334486] 与 [ 5.85943906 -8.38192364] 距离为 2.32 计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22 计算 [ 3.88101257 -9.59334486] 与 [ 5.80453161 -9.96151671] 距离为 1.96 计算 [ 3.88101257 -9.59334486] 与 [2.26186279 5.4894992 ] 距离为 15.17 --- 计算 [ 5.85943906 -8.38192364] 与 [0.69523642 3.23270535] 距离为 12.71 计算 [ 5.85943906 -8.38192364] 与 [ 5.80453161 -9.96151671] 距离为 1.58 计算 [ 5.85943906 -8.38192364] 与 [2.26186279 5.4894992 ] 距离为 14.33 --- 计算 [0.69523642 3.23270535] 与 [ 5.80453161 -9.96151671] 距离为 14.15 计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75 --- 计算 [ 5.80453161 -9.96151671] 与 [2.26186279 5.4894992 ] 距离为 15.85 --- 最近距离:[ 5.85943906 -8.38192364] &amp; [ 5.80453161 -9.96151671] = 1.58, 合并后中心:[[ 5.83198533 -9.17172018]] ☞ 第 6 次迭代 --- 计算 [4.1575017 3.89627276] 与 [ 3.88101257 -9.59334486] 距离为 13.49 计算 [4.1575017 3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53 计算 [4.1575017 3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48 计算 [4.1575017 3.89627276] 与 [ 5.83198533 -9.17172018] 距离为 13.17 --- 计算 [ 3.88101257 -9.59334486] 与 [0.69523642 3.23270535] 距离为 13.22 计算 [ 3.88101257 -9.59334486] 与 [2.26186279 5.4894992 ] 距离为 15.17 计算 [ 3.88101257 -9.59334486] 与 [ 5.83198533 -9.17172018] 距离为 2.0 --- 计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75 计算 [0.69523642 3.23270535] 与 [ 5.83198533 -9.17172018] 距离为 13.43 --- 计算 [2.26186279 5.4894992 ] 与 [ 5.83198533 -9.17172018] 距离为 15.09 --- 最近距离:[ 3.88101257 -9.59334486] &amp; [ 5.83198533 -9.17172018] = 2.0, 合并后中心:[[ 4.85649895 -9.38253252]] ☞ 第 7 次迭代 --- 计算 [4.1575017 3.89627276] 与 [0.69523642 3.23270535] 距离为 3.53 计算 [4.1575017 3.89627276] 与 [2.26186279 5.4894992 ] 距离为 2.48 计算 [4.1575017 3.89627276] 与 [ 4.85649895 -9.38253252] 距离为 13.3 --- 计算 [0.69523642 3.23270535] 与 [2.26186279 5.4894992 ] 距离为 2.75 计算 [0.69523642 3.23270535] 与 [ 4.85649895 -9.38253252] 距离为 13.28 --- 计算 [2.26186279 5.4894992 ] 与 [ 4.85649895 -9.38253252] 距离为 15.1 --- 最近距离:[4.1575017 3.89627276] &amp; [2.26186279 5.4894992 ] = 2.48, 合并后中心:[[3.20968225 4.69288598]] ☞ 第 8 次迭代 --- 计算 [0.69523642 3.23270535] 与 [ 4.85649895 -9.38253252] 距离为 13.28 计算 [0.69523642 3.23270535] 与 [3.20968225 4.69288598] 距离为 2.91 --- 计算 [ 4.85649895 -9.38253252] 与 [3.20968225 4.69288598] 距离为 14.17 --- 最近距离:[0.69523642 3.23270535] &amp; [3.20968225 4.69288598] = 2.91, 合并后中心:[[1.95245933 3.96279567]] ☞ 第 9 次迭代 --- 计算 [ 4.85649895 -9.38253252] 与 [1.95245933 3.96279567] 距离为 13.66 --- 最近距离:[ 4.85649895 -9.38253252] &amp; [1.95245933 3.96279567] = 13.66, 合并后中心:[[ 3.40447914 -2.70986843]] array([[ 3.40447914, -2.70986843]]) 通过上面的计算过程，你应该能很清晰地看出 Agglomerative 聚类的完整过程了。我们将 data 数组的每行依次按 0-9 编号，并将计算过程绘制成层次聚类的二叉树结构如下： 建立好树形结构后，如果我们想取类别为 2，就从顶部画一条横线就可以了。然后，沿着网络延伸到叶节点就能找到各自对应的类别。如下图所示： 如果你对着 data 预设的类别，你会发现最终的聚类结果和 data[1] = [0, 1, 1, 1, 0, 1, 0, 0, 0, 1]完全一致。 至此，我们就完整实现了自底向上层次聚类法。 使用 scikit-learn 完成 Agglomerative 聚类scikit-learn 中也提供了 Agglomerative 聚类的类，相应的参数解释如下： 1sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=&lt;function mean&gt;) 其中： n_clusters: 表示最终要查找类别的数量，例如上面的 2 类。 affinity: 亲和力度量，有 euclidean（欧式距离）, l1（L1 范数）, l2（L2 范数）, manhattan（曼哈顿距离）等可选。 linkage: 连接方法：ward（单连接）, complete（全连接）, average（平均连接）可选。 实验同样使用上面的数据集完成模型构建并聚类： 1234from sklearn.cluster import AgglomerativeClusteringmodel = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='average')model.fit_predict(data[0]) array([1, 0, 0, 0, 1, 0, 1, 1, 1, 0]) 可以看到，最终的聚类结构和我们上面是一致的。（表示类别的 1, 0 相反没有影响） 自顶向下层次聚类法除了上面讲到的自底向上的层次聚类法，还有一类是自顶向下层次聚类法，这种方法的计算流程与前一种正好相反，但过程要复杂很多。 大致说来，我们首先将全部数据归为一类，然后逐步分割成小类，而这里的分割方法又有常见的 2 种形式： 利用 K-Means 算法进行分割首先，我们说一说利用 K-Means 算法分割方法： 把数据集 `D` 归为单个类别 `C` 作为顶层。 使用 K-Means 算法把 `C` 划分成 2 个子类别，构成子层； 可递归使用 K-Means 算法继续划分子层到终止条件。 同样，我们可以通过示意图来演示该聚类算法的流程。 首先，全部数据在一个类别中： 然后，通过 K-Means 算法把其聚成 2 类。 紧接着，将子类再分别使用 K-Means 算法聚成 2 类。 最终，直到所以的数据都各自为 1 个类别，即分割完成。 利用 K-Means 算法进行自顶向下层次聚类过程同样属于看起来简单，但计算量庞大的过程。 利用平均距离进行分割 把数据集 `D` 归为单个类别 `C` 作为顶层。 从类别 `C` 中取出点 `d`，使得 `d` 满足到 `C` 中其他点的平均距离最远，构成类别 `N`。 继续从类别 `C` 中取出点 `d’`, 使得 `d’` 满足到 `C` 中其他点的平均距离与到 `N` 中点的平均距离之间的差值最大，并将点放入 `N`。 重复步骤 3，直到差值为负数。 再从子类中重复步骤 2,3,4 直到全部点单独成类，即完成分割。 同样，我们可以通过示意图来演示该聚类算法的流程。 首先，全部数据在一个类别中： 然后，我们依次抽取 1 个数据点，并计算它与其他点的平均距离，且最终取平均距离最大的点单独成类。例如这里计算出结果为 5。 同样，从剩下的 4 个点再取出一个点，使该点到剩下点的平均距离与该点到点 5 的距离差值最大且不为负数。这里没有点满足条件，终止。 接下来，从剩下的 4 个点中再取出一个点，并计算它与其他 3 点的距离，取最大单独成类。 同样，从剩下的 3 个点中再取出一个点，使该点到剩下点的平均距离与该点到点 4 的距离差值最大且不为负数，合并为 1 类。点 3 明显满足： 重复步骤，继续计算形成子层。然后对子层中包含有对应元素的类重复上面的步骤聚类，直到全部点单独成类，即完成分割。 自顶向下层次聚类法在实施过程中常常遇到一个问题，那就是如果两个样本在上一步聚类中被划分成不同的类别，那么即使这两个点距离非常近，后面也不会被放到一类中。 所以在实际应用中，自顶向下层次聚类法没有自底而上的层次聚类法常用，这里也就不再进行实现了，了解其运行原理即可。 BIRCH 聚类算法除了上文提到的两种层次聚类方法，还有一种非常常用且高效的层次聚类法，叫做 BIRCH。 BIRCH 的全称为 Balanced Iterative Reducing and Clustering using Hierarchies，直译过来就是「使用层次方法的平衡迭代规约和聚类」。该算法由时任 IBM 工程师 Tian Zhang 于 1996 年发明，详见 论文。 BIRCH 最大的特点就是高效，可用于大型数据集的快速聚类。 `CF` 聚类特征BIRCH 的聚类过程主要是涉及到 `CF` 聚类特征和 `CF` Tree 聚类特征树的概念。所以，我们需要先了解什么是聚类特征。 一组样本的 `CF` 聚类特征定义为如下所示的三元组： CF = \\langle ( N, LS, SS ) \\rangle其中，`N` 表示该 `CF` 中拥有的样本点的数量； `LS` 表示该 `CF` 中拥有的样本点各特征维度的和向量；`SS` 表示该 `CF` 中拥有的样本点各特征维度的平方和。 例如，我们有 5 个样本，分别为：`(1,3), (2,5), (1,3), (7,9), (8,8)`，那么： `N = 5` \\(LS = (1+2+1+7+8, 3+5+3+9+8) =(19, 28)\\) \\(SS = (1^2+2^2+1^2+7^2+8^2+3^2+5^2+3^2+9^2+8^2) = (307)\\) 于是，对应的 `CF` 值就为： CF = \\langle 5, (19,28), (307) \\rangle`CF` 拥有可加性，例如当 \\(CF’= \\langle 3, (35, 36), 857 \\rangle\\) 时： CF' + CF = \\langle 5, (19,28), (307) \\rangle + \\langle 3, (12, 26), 87 \\rangle = \\langle 8, (31, 54), (394) \\rangle`CF` 聚类特征本质上是定义类别（簇）的信息，并有效地对数据进行压缩。 `CF` 聚类特征树接下来，我们介绍第二个概念 `CF` 聚类特征树。 `CF` 树由根节点（root node）、枝节点（branch node）和叶节点（leaf node）构成。另包含有三个参数，分别为：枝平衡因子 \\(\\beta\\)、叶平衡因子 \\(\\lambda\\) 和空间阈值 \\(\\tau\\)。而非叶节点（nonleaf node）中包含不多于 \\(\\beta\\) 个 \\([CF,child_{i}]\\) 的元项。 BIRCH 算法的核心就是基于训练样本建立了 `CF` 聚类特征树。`CF` 聚类特征树对应的输出就是若干个 `CF` 节点，每个节点里的样本点就是一个聚类的类别。 其实，关于 `CF` 聚类特征树的特点以及树的生成过程还有很多内容可以深入学习，不过这里面涉及到大量的数学理论和推导过程，不太好理解，这里就不再展开了。有兴趣的同学可以阅读 原论文。 最后，我们简单说一下 BIRCH 算法相比 Agglomerative 算法的优势，也就是总结学习 BIRCH 算法的必要性： BIRCH 算法在建立 CF 特征树时只存储原始数据的特征信息，并不需要存储原始数据信息，内存开销上更优，计算高效。 BIRCH 算法只需要遍历一遍原始数据，而 Agglomerative 算法在每次迭代都需要遍历一遍数据，再次突出 BIRCH 的高效性。 BIRCH 属于在线学习算法，并支持对流数据的聚类，开始聚类时并不需要知道所有的数据。 BIRCH 聚类实现上面说了这么多，总结就是 BIRCH 属于层次聚类算法中非常高效的那一种方法。下面，就来看一看如何调用 scikit-learn 提供的 BIRCH 类完成聚类任务。 我们先导入 DIGITS 数据集，并查看前 5 个手写字符。 123456789import numpy as npfrom sklearn import datasetsdigits = datasets.load_digits()# 查看前 5 个字符fig, axes = plt.subplots(1, 5, figsize=(12,4))for i, image in enumerate(digits.images[:5]): axes[i].imshow(image, cmap=plt.cm.gray_r) 我们都知道，一个手写字符的数据是由 8x8 的矩阵表示。 1digits.images[0] array([[ 0., 0., 5., 13., 9., 1., 0., 0.], [ 0., 0., 13., 15., 10., 15., 5., 0.], [ 0., 3., 15., 2., 0., 11., 8., 0.], [ 0., 4., 12., 0., 0., 8., 8., 0.], [ 0., 5., 8., 0., 0., 9., 8., 0.], [ 0., 4., 11., 0., 1., 12., 7., 0.], [ 0., 2., 14., 5., 10., 12., 0., 0.], [ 0., 0., 6., 13., 10., 0., 0., 0.]]) 如果我们针对该矩阵进行扁平化处理，就能变为 1x64 的向量。对于这样一个高维向量，虽然可以在聚类时直接计算距离，但却无法很好地在二维平面中表示相应的数据点。因为，二维平面中的点只由横坐标和纵坐标组成。 所以，为了尽可能还原聚类的过程，我们需要将 1x64 的行向量（64 维），处理成 1x2 的行向量（2 维），也就是降维的过程。 既然是降低维度，那么应该怎样做呢？是直接取前面两位数，或者随机取出两位？当然不是。这里学习一种新方法，叫 PCA 主成分分析。 PCA 主成分分析（降维）主成分分析是多元线性统计里面的概念，它的英文是：Principal Components Analysis，简称 PCA。主成分分析旨在降低数据的维数，通过保留数据集中的主要成分来简化数据集。 主成分分析的数学原理非常简单，通过对协方差矩阵进行特征分解，从而得出主成分（特征向量）与对应的权值（特征值）。然后剔除那些较小特征值（较小权值）对应的特征，从而达到降低数据维数的目的。 主成分分析通常有两个作用： 参考本文的目的，方便将数据用于低维空间可视化。聚类过程中的可视化是很有必要的。 高维度数据集往往就意味着计算资源的大量消耗。通过对数据进行降维，我们就能在不较大影响结果的同时，减少模型学习时间。 现在，假定我们需要将特征维度从 `n` 维降到 `m` 维，PCA 的计算流程如下： 1.对各维度特征进行标准化处理： x_j^{(i)}=\\frac{x_j^{(i)}-\\mu_j}{s_j}其中，\\(\\mu_j\\) 为特征 `j` 的均值，`s_j` 为特征 `j` 的标准差。 2.计算对应的协方差矩阵： \\Sigma = \\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)})(x^{(i)})^T = \\frac{1}{m} \\cdot X^TX3.对协方差矩阵进行奇异值分解（SVD），得到特征向量： (U,S,V^T) = SVD(\\Sigma)4.从 `U` 中取出前 `m` 个左奇异向量，构成一个约减矩阵 \\(U_{reduce}\\) ： U_{reduce} = (u^{(1)},u^{(2)},\\cdots,u^{(k)})5.计算新的特征向量 \\(z^{(i)}\\)： z^{(i)}=U_{reduce}^T \\cdot x^{(i)}6.最后根据新的特征向量执行特征还原： x_{new}=U_{reduce}z^{(i)}PCA 的过程听起来简单，执行起来还是比较麻烦的。所以，我们这里直接使用 scikit-learn 中 PCA 方法完成： 1sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto') 其中： n_components= 表示需要保留主成分（特征）的数量。 copy= 表示针对原始数据降维还是针对原始数据副本降维。当参数为 False 时，降维后的原始数据会发生改变，这里默认为 True。 whiten= 白化表示将特征之间的相关性降低，并使得每个特征具有相同的方差。 svd_solver= 表示奇异值分解 SVD 的方法。有 4 参数，分别是：auto, full, arpack, randomized。 在使用 PCA 降维时，我们也会使用到 PCA.fit() 方法。.fit() 是 scikit-learn 训练模型的通用方法，但是该方法本身返回的是模型的参数。所以，通常我们会使用 PCA.fit_transform() 方法直接返回降维后的数据结果。 下面，我们就针对 DIGITS 数据集进行特征降维。 123456from sklearn.decomposition import PCA# PCA 将数据降为 2 维pca = PCA(n_components=2)pca_data = pca.fit_transform(digits.data)pca_data array([[ -1.25946556, 21.27488973], [ 7.95761389, -20.7686883 ], [ 6.99192015, -9.95599952], ..., [ 10.80128328, -6.96025693], [ -4.87209994, 12.42397329], [ -0.34439245, 6.36553344]]) 可以看到，每一行的特征已经由先前的 64 个缩减为 2 个了。 接下来将降维后的数据绘制到二维平面中。 12plt.figure(figsize=(10, 8))plt.scatter(pca_data[:,0], pca_data[:,1]) 上图就是 DIGITS 数据集中 1797 个样本通过 PCA 降维后对应在二维平面的数据点。 现在，我们可以直接使用 BIRCH 对降维后的数据进行聚类。由于我们提前知道这是手写数字字符，所以选择聚为 10 类。当然，在聚类时，我们只是知道大致要聚集的类别数量，而并不知道数据对应的标签值。 BIRCH 在 scikit-learn 对应的主要类及参数如下： 1sklearn.cluster.Birch(threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True) 其中： threshold: 每个 CF 的空间阈值 \\(\\tau\\)。参数值越小，则 CF 特征树的规模会越大，学习时花费的时间和内存会越多。默认值是 0.5，但如果样本的方差较大，则一般需要增大这个默认值。 branching_factor: CF 树中所有节点的最大 CF 数。该参数默认为 50，如果样本量非常大，一般需要增大这个默认值。 n_clusters: 虽然层次聚类无需预先设定类别数量，但可以设定期望查询的类别数。 接下来，使用 BIRCH 算法得到 PCA 降维后数据的聚类结果： 12345from sklearn.cluster import Birchbirch = Birch(n_clusters=10)cluster_pca = birch.fit_predict(pca_data)cluster_pca array([3, 0, 0, ..., 0, 5, 9]) 利用得到的聚类结果对散点图进行着色。 12plt.figure(figsize=(10, 8))plt.scatter(pca_data[:,0], pca_data[:,1],c=cluster_pca) 12345678910111213141516# 计算聚类过程中的决策边界x_min, x_max = pca_data[:, 0].min() - 1, pca_data[:, 0].max() + 1y_min, y_max = pca_data[:, 1].min() - 1, pca_data[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, .4), np.arange(y_min, y_max, .4))temp_cluster = birch.predict(np.c_[xx.ravel(), yy.ravel()])# 将决策边界绘制出来temp_cluster = temp_cluster.reshape(xx.shape)plt.figure(figsize=(10, 8))plt.contourf(xx, yy, temp_cluster, cmap=plt.cm.bwr, alpha=.3)plt.scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_pca, s=15)# 图像参数设置plt.xlim(x_min, x_max)plt.ylim(y_min, y_max) 其实，我们可以利用预先知道的各字符对应的标签对散点图进行着色，对比上面的聚类结果。 12plt.figure(figsize=(10, 8))plt.scatter(pca_data[:,0], pca_data[:,1],c=digits.target) 对照两幅图片，你会发现对 PCA 降维数据的聚类结果大致符合原数据的分布趋势。这里色块的颜色不对应没有关系，因为原标签和聚类标签的顺序不对应，只需要关注数据块的分布规律即可。 不过，使用真实标签绘制出来的散点图明显凌乱很多，这其实是由于 PCA 降维造成的。 一般情况下，我们输入到聚类模型中的数据不一定要是降维后的数据。下面输入原数据重新聚类试一试。 12cluster_ori = birch.fit_predict(digits.data)cluster_ori array([7, 9, 4, ..., 4, 1, 4]) 12plt.figure(figsize=(10, 8))plt.scatter(pca_data[:,0], pca_data[:,1],c=cluster_ori) 现在你会发现，实验得到的聚类结果更加符合原数据集的分布规律了。再次强调，这里颜色不分离其实是由于 PCA 降维后在二维平面可视化的效果，不代表真实的聚类效果。 不过，最后我们再强调一下 PCA 的使用情形。一般情况下，我们不会拿到数据就进行 PCA 处理，只有当算法不尽如人意、训练时间太长、需要可视化等情形才考虑使用 PCA。其主要原因是，PCA 被看作是对数据的有损压缩，会造成数据集原始特征丢失。 本次我们了解了层次聚类方法，特别地学习了向上、向下以及 BIRCH 算法。其中，比较常用的是自底向上或 BIRCH 方法，且 BIRCH 拥有计算高效的特点。不过，BIRCH 也有一些弊端，例如对高维数据的聚类效果往往不太好，有时候我们也会使用 Mini Batch K-Means 进行替代。最后，通过表格对比本次实验的 3 种层次聚类法的优缺点： 拓展阅读： Hierarchical clustering - Wikipedia BIRCH - Wikipedia","link":"/2019/02/11/Hierarchical_clustering_method/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"sort","slug":"sort","link":"/tags/sort/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"linear regression","slug":"linear-regression","link":"/tags/linear-regression/"},{"name":"MAE","slug":"MAE","link":"/tags/MAE/"},{"name":"MSE","slug":"MSE","link":"/tags/MSE/"},{"name":"MAPE","slug":"MAPE","link":"/tags/MAPE/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"ECS","slug":"ECS","link":"/tags/ECS/"},{"name":"Seafile","slug":"Seafile","link":"/tags/Seafile/"},{"name":"k-means","slug":"k-means","link":"/tags/k-means/"},{"name":"jekyll","slug":"jekyll","link":"/tags/jekyll/"},{"name":"next","slug":"next","link":"/tags/next/"},{"name":"polynomial regression","slug":"polynomial-regression","link":"/tags/polynomial-regression/"},{"name":"k-nearest neighbors","slug":"k-nearest-neighbors","link":"/tags/k-nearest-neighbors/"},{"name":"Gradient Descent","slug":"Gradient-Descent","link":"/tags/Gradient-Descent/"},{"name":"MF","slug":"MF","link":"/tags/MF/"},{"name":"RS","slug":"RS","link":"/tags/RS/"},{"name":"naive bayes","slug":"naive-bayes","link":"/tags/naive-bayes/"},{"name":"bagging","slug":"bagging","link":"/tags/bagging/"},{"name":"boosting","slug":"boosting","link":"/tags/boosting/"},{"name":"support vector machine","slug":"support-vector-machine","link":"/tags/support-vector-machine/"},{"name":"decision tree","slug":"decision-tree","link":"/tags/decision-tree/"},{"name":"cluster","slug":"cluster","link":"/tags/cluster/"},{"name":"Deep learning","slug":"Deep-learning","link":"/tags/Deep-learning/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/tags/TensorFlow/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"perceptron","slug":"perceptron","link":"/tags/perceptron/"},{"name":"artificial neural network","slug":"artificial-neural-network","link":"/tags/artificial-neural-network/"},{"name":"HAC","slug":"HAC","link":"/tags/HAC/"}],"categories":[{"name":"深度学习","slug":"深度学习","link":"/categories/深度学习/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"博客","slug":"博客","link":"/categories/博客/"},{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/categories/Ubuntu/"},{"name":"排序算法","slug":"Python/排序算法","link":"/categories/Python/排序算法/"},{"name":"GitHub Pages","slug":"博客/GitHub-Pages","link":"/categories/博客/GitHub-Pages/"},{"name":"线性回归","slug":"机器学习/线性回归","link":"/categories/机器学习/线性回归/"},{"name":"ECS","slug":"Ubuntu/ECS","link":"/categories/Ubuntu/ECS/"},{"name":"K-Means","slug":"机器学习/K-Means","link":"/categories/机器学习/K-Means/"},{"name":"私人网盘","slug":"Ubuntu/ECS/私人网盘","link":"/categories/Ubuntu/ECS/私人网盘/"},{"name":"主题优化","slug":"博客/主题优化","link":"/categories/博客/主题优化/"},{"name":"多项式回归","slug":"机器学习/多项式回归","link":"/categories/机器学习/多项式回归/"},{"name":"K-近邻算法","slug":"机器学习/K-近邻算法","link":"/categories/机器学习/K-近邻算法/"},{"name":"梯度下降","slug":"机器学习/梯度下降","link":"/categories/机器学习/梯度下降/"},{"name":"矩阵分解","slug":"矩阵分解","link":"/categories/矩阵分解/"},{"name":"朴素贝叶斯","slug":"机器学习/朴素贝叶斯","link":"/categories/机器学习/朴素贝叶斯/"},{"name":"装袋","slug":"机器学习/装袋","link":"/categories/机器学习/装袋/"},{"name":"推荐系统","slug":"矩阵分解/推荐系统","link":"/categories/矩阵分解/推荐系统/"},{"name":"提升","slug":"机器学习/装袋/提升","link":"/categories/机器学习/装袋/提升/"},{"name":"支持向量机","slug":"机器学习/支持向量机","link":"/categories/机器学习/支持向量机/"},{"name":"决策树","slug":"机器学习/决策树","link":"/categories/机器学习/决策树/"},{"name":"划分聚类","slug":"机器学习/K-Means/划分聚类","link":"/categories/机器学习/K-Means/划分聚类/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","link":"/categories/深度学习/卷积神经网络/"},{"name":"计算机视觉","slug":"深度学习/卷积神经网络/计算机视觉","link":"/categories/深度学习/卷积神经网络/计算机视觉/"},{"name":"感知机","slug":"机器学习/感知机","link":"/categories/机器学习/感知机/"},{"name":"人工神经网络","slug":"机器学习/感知机/人工神经网络","link":"/categories/机器学习/感知机/人工神经网络/"},{"name":"层次聚类","slug":"机器学习/K-Means/层次聚类","link":"/categories/机器学习/K-Means/层次聚类/"}]}